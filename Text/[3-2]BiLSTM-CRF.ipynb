{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM-CRF \n",
    "\n",
    "References:  \n",
    "1. [ADVANCED: MAKING DYNAMIC DECISIONS AND THE BI-LSTM CRF](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)  \n",
    "2. [Implementing a linear-chain Conditional Random Field (CRF) in PyTorch](https://towardsdatascience.com/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Chain Conditional Random Field (CRF) \n",
    "\n",
    "The source sequence is $x = \\{x_1, x_2, \\dots, x_T \\}$, and the target sequence is $y = \\{y_1, y_2, \\dots, y_T \\}$.  \n",
    "If we ignore the dependence between elements in $y$, we can model as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y|x) &= \\prod_{t=1}^T \\frac{\\exp \\left( U(x_t, y_t) \\right)}{\\sum_{y'_t} \\exp \\left( U(x_t, y'_t) \\right)} \\\\\n",
    "&= \\prod_{t=1}^T \\frac{\\exp \\left( U(x_t, y_t) \\right)}{Z(x_t)} \\\\\n",
    "&= \\frac{\\exp \\left( \\sum_{t=1}^T U(x_t, y_t) \\right)}{\\prod_{t=1}^T Z(x_t)} \\\\\n",
    "&= \\frac{\\exp \\left( \\sum_{t=1}^T U(x_t, y_t) \\right)}{Z(x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $U(x_t, y_t)$ is *emissions* or *unary scores*, $Z(x_t)$ is *partition function* (a normalization factor).  \n",
    "In a *linear-chain CRF*, we add *transition scores* $T(y_t, y_{t+1})$ to the above equation:  \n",
    "$$\n",
    "P(y|x) = \\frac{\\exp \\left( \\sum_{t=1}^T U(x_t, y_t) + \\sum_{t=0}^T T(y_t, y_{t+1}) \\right)}{Z(x)}\n",
    "$$\n",
    "where $y_0$ and $y_{T+1}$ are the starting and stopping tags, respectively; their values are fixed.  \n",
    "The partition function should sum over all possible combinations over the label set at each timestep: \n",
    "$$\n",
    "Z(x) = \\sum_{y'_1} \\sum_{y'_2} \\dots \\sum_{y'_T} \\exp \\left( \\sum_{t=1}^T U(x_t, y'_t) + \\sum_{t=0}^T T(y'_t, y'_{t+1}) \\right)\n",
    "$$\n",
    "\n",
    "The *negative log-likelihood loss (NLL-Loss)* is: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "L &= -\\log \\left( P(y|x) \\right) \\\\\n",
    "&= \\log \\left( Z(x) \\right) - \\left( \\sum_{t=1}^T U(x_t, y_t) + \\sum_{t=0}^T T(y_t, y_{t+1}) \\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Algorithm: Dynamic Programing for Computing the Partition Function\n",
    "\n",
    "The time complexity of computing $Z(x)$ would be $O(\\vert y \\vert^T)$... but we can use *dynamic programing* to reduce it.  \n",
    "Specifically, we define the state:\n",
    "$$\n",
    "\\alpha_s (y_s) = \\sum_{y'_1} \\sum_{y'_2} \\dots \\sum_{y'_{s-1}} \\exp \\left( \\sum_{t=1}^{s-1} U(x_t, y'_t) + \\sum_{t=0}^{s-2} T(y'_t, y'_{t+1}) + T(y'_{s-1}, y_s) \\right)\n",
    "$$\n",
    "where $\\alpha_s (y_s)$ may be regarded as the sum of scores reaching $y_s$. Note:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha_1(y_1) &= \\exp \\left( T(y_0, y_1) \\right) \\\\\n",
    "\\alpha_{T+1}(y_{T+1}) &= Z(x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When computing $\\alpha_{s+1}(y_{s+1})$, we only require the information of $\\alpha_s(y'_s)$ for different $y'_s$, instead of the information before step $s$ (i.e., the paths reaching each $y'_s$). Hence, this is a dynamic programing problem. \n",
    "In the log-space, we have the *state transition equation*:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log( \\alpha_{s+1}(y_{s+1}) ) &= \\log \\left( \\sum_{y'_1} \\sum_{y'_2} \\dots \\sum_{y'_s} \\exp \\left( \\sum_{t=1}^s U(x_t, y'_t) + \\sum_{t=0}^{s-1} T(y'_t, y'_{t+1}) + T(y'_s, y_{s+1}) \\right) \\right) \\\\\n",
    "&= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_s, y'_s) + T(y'_s, y_{s+1}) \\right) \\sum_{y'_1} \\sum_{y'_2} \\dots \\sum_{y'_{s-1}} \\exp \\left( \\sum_{t=1}^{s-1} U(x_t, y'_t) + \\sum_{t=0}^{s-2} T(y'_t, y'_{t+1}) + T(y'_{s-1}, y'_s) \\right) \\right) \\\\\n",
    "&= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_s, y'_s) + T(y'_s, y_{s+1}) \\right) \\alpha_s(y'_s) \\right) \\\\\n",
    "&= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_s, y'_s) + T(y'_s, y_{s+1}) + \\log (\\alpha_s(y'_s)) \\right) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note** that it is equivalent to add the *unary scores* before or after `logsumexp`:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log( \\alpha_{s+1}(y_{s+1}) ) + U(x_{s+1}, y_{s+1}) &= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_s, y'_s) + T(y'_s, y_{s+1}) + \\log (\\alpha_s(y'_s)) \\right) \\right) + U(x_{s+1}, y_{s+1}) \\\\\n",
    "&= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_s, y'_s) + T(y'_s, y_{s+1}) + \\log (\\alpha_s(y'_s)) \\right) \\right) + \\log( \\exp ( U(x_{s+1}, y_{s+1}) )) \\\\\n",
    "&= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_{s+1}, y_{s+1}) + U(x_s, y'_s) + T(y'_s, y_{s+1}) + \\log (\\alpha_s(y'_s)) \\right) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Hence, for each step, it is equivalent to deal with $U(x_s, y_s)$ and $T(y_{s-1}, y_s)$ together, or deal with $U(x_s, y_s)$ and $T(y_s, y_{s+1})$ together.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm: Finding the Best Sequence of Labels \n",
    "\n",
    "Similarly, we use dynamic programing to find the best sequence of labels (i.e., decoding).  \n",
    "Specifically, we define the state: \n",
    "$$\n",
    "\\beta_s (y_s) = \\max_{y'_1, y'_2, \\dots, y'_{s-1}} \\sum_{t=1}^{s-1} U(x_t, y'_t) + \\sum_{t=0}^{s-2} T(y'_t, y'_{t+1}) + T(y'_{s-1}, y_s) \n",
    "$$\n",
    "where $\\beta_s (y_s)$ may be regarded as the max score reaching $y_s$. Note: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_1 (y_1) &= T(y_0, y_1) \\\\\n",
    "\\beta_{T+1}(y_{T+1}) &= \\max_{y'_1, y'_2, \\dots, y'_T} \\sum_{t=1}^T U(x_t, y'_t) + \\sum_{t=0}^{T-1} T(y'_t, y'_{t+1}) + T(y'_T, y_{T+1}) \n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "Aparently, this is again a dynamic programing problem. We have the *state transition equation*: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{s+1} (y_{s+1}) &= \\max_{y'_1, y'_2, \\dots, y'_s} \\sum_{t=1}^s U(x_t, y'_t) + \\sum_{t=0}^{s-1} T(y'_t, y'_{t+1}) + T(y'_s, y_{s+1}) \\\\\n",
    "&= \\max_{y'_s} U(x_s, y'_s) + T(y'_s, y_{s+1}) + \\max_{y'_1, y'_2, \\dots, y'_{s-1}} \\sum_{t=1}^{s-1} U(x_t, y'_t) + \\sum_{t=0}^{s-2} T(y'_t, y'_{t+1}) + T(y'_{s-1}, y'_s) \\\\\n",
    "&= \\max_{y'_s} U(x_s, y'_s) + T(y'_s, y_{s+1}) + \\beta_s (y'_s)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(['the',\n   'wall',\n   'street',\n   'journal',\n   'reported',\n   'today',\n   'that',\n   'apple',\n   'corporation',\n   'made',\n   'money'],\n  ['B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O']),\n (['georgia', 'tech', 'is', 'a', 'university', 'in', 'georgia'],\n  ['B', 'I', 'O', 'O', 'O', 'O', 'B'])]"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "training_data = [(\"the wall street journal reported today that apple corporation made money\".split(),\n",
    "                  \"B I I I O O O B I O O\".split()), \n",
    "                 (\"georgia tech is a university in georgia\".split(),\n",
    "                  \"B I O O O O B\".split())]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 5\n",
    "HID_DIM = 4\n",
    "VOC_DIM = len(word_to_ix)\n",
    "TAG_DIM = len(tag_to_ix)\n",
    "\n",
    "emb = nn.Embedding(VOC_DIM, EMB_DIM)\n",
    "rnn = nn.LSTM(EMB_DIM, HID_DIM//2, num_layers=1, bidirectional=True)\n",
    "hid2tag = nn.Linear(HID_DIM, TAG_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0],\n        [1],\n        [2],\n        [3],\n        [4]])\ntensor([[0],\n        [1],\n        [1],\n        [1],\n        [2]])\n"
    }
   ],
   "source": [
    "ex_idx = 0\n",
    "# Use the first several steps as illustration\n",
    "# sent/tags: (step, batch=1)\n",
    "sent = torch.tensor([word_to_ix[w] for w in training_data[ex_idx][0]], dtype=torch.long).unsqueeze(1)[:5]\n",
    "tags = torch.tensor([tag_to_ix[t] for t in training_data[ex_idx][1]], dtype=torch.long).unsqueeze(1)[:5]\n",
    "print(sent)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.1154,  0.1013, -0.2363, -0.1659, -0.3134]],\n\n        [[-0.1415,  0.1352, -0.2847, -0.2100, -0.2518]],\n\n        [[-0.0351,  0.0659, -0.1660, -0.1903, -0.2150]],\n\n        [[ 0.0703, -0.0114,  0.0622,  0.0429, -0.1247]],\n\n        [[-0.0787,  0.0378, -0.0107,  0.0106, -0.1368]]],\n       grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "embbed = emb(sent)\n",
    "rnn_outs, _ = rnn(embbed)\n",
    "\n",
    "# feats: (step, batch=1, tag_dim)\n",
    "feats = hid2tag(rnn_outs)\n",
    "print(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-1.1290e+00, -1.2998e+00, -1.6684e+00, -8.6616e-01, -1.0000e+04],\n        [-4.6060e-01,  4.6536e-01, -1.5174e+00, -1.5891e+00, -1.0000e+04],\n        [-1.2220e+00, -2.4080e-01, -8.2866e-01,  7.9692e-01, -1.0000e+04],\n        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04],\n        [-5.8353e-01, -6.6203e-01,  1.1687e-01,  1.1368e+00, -1.0000e+04]],\n       requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# transitions[i, j] is the score of transitioning *to* i *from* j.\n",
    "transitions = nn.Parameter(torch.randn(TAG_DIM, TAG_DIM))\n",
    "transitions.data[tag_to_ix[START_TAG], :] = -1e4\n",
    "transitions.data[:, tag_to_ix[STOP_TAG]] = -1e4\n",
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.2256], grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# The numerator: score\n",
    "# The original implementation\n",
    "def _score_sentence(feats, tags):\n",
    "    score = torch.zeros(1)\n",
    "    tags = torch.cat([torch.tensor([tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "    for i, feat in enumerate(feats):\n",
    "        score = score + transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "    score = score +transitions[tag_to_ix[STOP_TAG], tags[-1]]\n",
    "    return score\n",
    "\n",
    "_score_sentence(feats.squeeze(1), tags.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.2256], grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Vectorized implementation\n",
    "def _score_sentence_vec(feats, tags):\n",
    "    feat_scores = feats.gather(dim=-1, index=tags.unsqueeze(-1)).squeeze(-1)\n",
    "    # print(feat_scores.size())\n",
    "\n",
    "    from_tags = torch.cat([torch.full((1, tags.size(1)), fill_value=tag_to_ix[START_TAG], dtype=torch.long), tags], dim=0)\n",
    "    to_tags = torch.cat([tags, torch.full((1, tags.size(1)), fill_value=tag_to_ix[STOP_TAG], dtype=torch.long)], dim=0)\n",
    "    trans_scores = transitions[to_tags, from_tags]\n",
    "    # print(trans_scores.size())\n",
    "\n",
    "    return feat_scores.sum(dim=0) + trans_scores.sum(dim=0)\n",
    "\n",
    "_score_sentence_vec(feats, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(2.4575, grad_fn=<LogsumexpBackward>)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import itertools\n",
    "scores = []\n",
    "for potential_tags in itertools.product(*[range(TAG_DIM) for _ in range(feats.size(0))]):\n",
    "    potential_tags = torch.tensor(potential_tags, dtype=torch.long).unsqueeze(1)\n",
    "    potential_score = _score_sentence_vec(feats, potential_tags)\n",
    "    scores.append(potential_score)\n",
    "\n",
    "torch.logsumexp(torch.cat(scores), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(2.4575, grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# The denominator: partition function\n",
    "# The original implementation\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, torch.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def _forward_alg(feats):\n",
    "    # Do the forward algorithm to compute the partition function\n",
    "    init_alphas = torch.full((1, TAG_DIM), -1e4)\n",
    "    # START_TAG has all of the score.\n",
    "    init_alphas[0][tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "    # Wrap in a variable so that we will get automatic backprop\n",
    "    forward_var = init_alphas\n",
    "\n",
    "    # Iterate through the sentence\n",
    "    for feat in feats:\n",
    "        alphas_t = []  # The forward tensors at this timestep\n",
    "        for next_tag in range(TAG_DIM):\n",
    "            # broadcast the emission score: it is the same regardless of\n",
    "            # the previous tag\n",
    "            emit_score = feat[next_tag].view(1, -1).expand(1, TAG_DIM)\n",
    "            # the ith entry of trans_score is the score of transitioning to\n",
    "            # next_tag from i\n",
    "            trans_score = transitions[next_tag].view(1, -1)\n",
    "            # The ith entry of next_tag_var is the value for the\n",
    "            # edge (i -> next_tag) before we do log-sum-exp\n",
    "            next_tag_var = forward_var + trans_score + emit_score\n",
    "            # The forward variable for this tag is log-sum-exp of all the\n",
    "            # scores.\n",
    "            alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "        forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "    terminal_var = forward_var + transitions[tag_to_ix[STOP_TAG]]\n",
    "    alpha = log_sum_exp(terminal_var)\n",
    "    return alpha\n",
    "\n",
    "_forward_alg(feats.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-10000., -10000., -10000.,      0., -10000.]])\ntensor([2.4575], grad_fn=<LogsumexpBackward>)\n"
    }
   ],
   "source": [
    "# Vectorized implementation\n",
    "alphas = torch.full((feats.size(1), TAG_DIM), fill_value=-1e4)\n",
    "alphas[:, tag_to_ix[START_TAG]] = 0\n",
    "print(alphas)\n",
    "\n",
    "for t in range(feats.size(0)):\n",
    "    # alphas: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "    # feats[t]: (batch=1, tag_dim) -> (batch=1, tag_dim, 1)\n",
    "    alphas = torch.logsumexp(alphas.unsqueeze(1) + feats[t].unsqueeze(2) + transitions, dim=-1)\n",
    "\n",
    "alphas = alphas + transitions[tag_to_ix[STOP_TAG]]\n",
    "print(torch.logsumexp(alphas, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-8.6616e-01, -1.5891e+00,  7.9692e-01, -1.0000e+04,  1.1368e+00]],\n       grad_fn=<UnsqueezeBackward0>)\ntensor([2.4575], grad_fn=<SelectBackward>)\n"
    }
   ],
   "source": [
    "# Note: It is equivalent to add the unary scores before or after `logsumexp`. \n",
    "alphas = transitions[:, tag_to_ix[START_TAG]].unsqueeze(0)\n",
    "print(alphas)\n",
    "\n",
    "for t in range(feats.size(0)):\n",
    "    # alphas: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "    # feats[t]: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "    alphas = torch.logsumexp((alphas + feats[t]).unsqueeze(1) + transitions, dim=-1)\n",
    "\n",
    "print(alphas[:, tag_to_ix[STOP_TAG]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.8662, -1.5891,  0.7969]], grad_fn=<UnsqueezeBackward0>)\ntensor([1.9266], grad_fn=<LogsumexpBackward>)\n"
    }
   ],
   "source": [
    "# Remove the starting and stopping tags in computation\n",
    "alphas = transitions[:-2, tag_to_ix[START_TAG]].unsqueeze(0)\n",
    "print(alphas)\n",
    "\n",
    "for t in range(feats.size(0)-1):\n",
    "    alphas = torch.logsumexp((alphas + feats[t, :, -2]).unsqueeze(1) + transitions[:-2, :-2], dim=-1)\n",
    "\n",
    "# Only transitioning to the stopping tag\n",
    "alphas = torch.logsumexp(alphas + feats[-1, :, -2] + transitions[tag_to_ix[STOP_TAG], :-2], dim=-1)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.0290, grad_fn=<SelectBackward>),\n [tensor(2), tensor(1), tensor(1), tensor(1), tensor(2)])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Viterbi decoding\n",
    "# The original implementation\n",
    "def _viterbi_decode(feats):\n",
    "    backpointers = []\n",
    "\n",
    "    # Initialize the viterbi variables in log space\n",
    "    init_vvars = torch.full((1, TAG_DIM), -10000.)\n",
    "    init_vvars[0][tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "    # forward_var at step i holds the viterbi variables for step i-1\n",
    "    forward_var = init_vvars\n",
    "    for feat in feats:\n",
    "        bptrs_t = []  # holds the backpointers for this step\n",
    "        viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "        for next_tag in range(TAG_DIM):\n",
    "            # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "            # previous step, plus the score of transitioning\n",
    "            # from tag i to next_tag.\n",
    "            # We don't include the emission scores here because the max\n",
    "            # does not depend on them (we add them in below)\n",
    "            next_tag_var = forward_var + transitions[next_tag]\n",
    "            best_tag_id = torch.argmax(next_tag_var)\n",
    "            bptrs_t.append(best_tag_id)\n",
    "            viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "        # Now add in the emission scores, and assign forward_var to the set\n",
    "        # of viterbi variables we just computed\n",
    "        forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "        backpointers.append(bptrs_t)\n",
    "\n",
    "    # Transition to STOP_TAG\n",
    "    terminal_var = forward_var + transitions[tag_to_ix[STOP_TAG]]\n",
    "    best_tag_id = torch.argmax(terminal_var)\n",
    "    path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "    # Follow the back pointers to decode the best path.\n",
    "    best_path = [best_tag_id]\n",
    "    for bptrs_t in reversed(backpointers):\n",
    "        best_tag_id = bptrs_t[best_tag_id]\n",
    "        best_path.append(best_tag_id)\n",
    "    # Pop off the start tag (we dont want to return that to the caller)\n",
    "    start = best_path.pop()\n",
    "    assert start == tag_to_ix[START_TAG]  # Sanity check\n",
    "    best_path.reverse()\n",
    "    return path_score, best_path\n",
    "\n",
    "_viterbi_decode(feats.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-8.6616e-01, -1.5891e+00,  7.9692e-01, -1.0000e+04,  1.1368e+00]],\n       grad_fn=<UnsqueezeBackward0>)\ntensor([0.0290], grad_fn=<SelectBackward>)\ntensor([[2],\n        [1],\n        [1],\n        [1],\n        [2],\n        [4]])\n"
    }
   ],
   "source": [
    "# Vectorized implementation\n",
    "alphas = transitions[:, tag_to_ix[START_TAG]].unsqueeze(0)\n",
    "# best_paths: (step=1, batch, tag_dim)\n",
    "best_paths = torch.arange(TAG_DIM).repeat(feats.size(1), 1).unsqueeze(0)\n",
    "print(alphas)\n",
    "\n",
    "for t in range(feats.size(0)):\n",
    "    # alphas: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "    # feats[t]: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "    # indices: (batch=1, tag_dim)\n",
    "    # indices tell that given the current tag, which is the best tag for last step?\n",
    "    alphas, indices = torch.max((alphas + feats[t]).unsqueeze(1) + transitions, dim=-1)\n",
    "\n",
    "    # selected_paths: (step, batch=1, tag_dim) \n",
    "    # The paths selected according to indices\n",
    "    selected_paths = torch.cat([best_paths[:, i, indices[i]].unsqueeze(1) for i in range(feats.size(1))], dim=1)\n",
    "    this_step = torch.arange(TAG_DIM).repeat(feats.size(1), 1).unsqueeze(0)\n",
    "    best_paths = torch.cat([selected_paths, this_step], dim=0)\n",
    "\n",
    "print(alphas[:, tag_to_ix[STOP_TAG]])\n",
    "print(best_paths[:, :, tag_to_ix[STOP_TAG]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, voc_dim, emb_dim, hid_dim, tag_dim, start_idx, stop_idx):\n",
    "        super().__init__()\n",
    "        self.start_idx = start_idx\n",
    "        self.stop_idx = stop_idx\n",
    "\n",
    "        self.emb = nn.Embedding(voc_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim//2, num_layers=1, bidirectional=True)\n",
    "        self.hid2tag = nn.Linear(hid_dim, tag_dim)\n",
    "\n",
    "        # transitions[i, j] is the score of transitioning from j to i\n",
    "        self.transitions = nn.Parameter(torch.randn(tag_dim, tag_dim))\n",
    "        self.transitions.data[self.start_idx, :] = -1e4\n",
    "        self.transitions.data[:, self.stop_idx] = -1e4\n",
    "\n",
    "    def _get_features(self, src):\n",
    "        embbed = self.emb(src)\n",
    "        rnn_outs, _ = self.rnn(embbed)\n",
    "\n",
    "        # feats: (step, batch=1, tag_dim)\n",
    "        feats = self.hid2tag(rnn_outs)\n",
    "        return feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        feat_scores = feats.gather(dim=-1, index=tags.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        from_tags = torch.cat([torch.full((1, tags.size(1)), fill_value=self.start_idx, dtype=torch.long), tags], dim=0)\n",
    "        to_tags = torch.cat([tags, torch.full((1, tags.size(1)), fill_value=self.stop_idx, dtype=torch.long)], dim=0)\n",
    "        trans_scores = self.transitions[to_tags, from_tags]\n",
    "\n",
    "        return feat_scores.sum(dim=0) + trans_scores.sum(dim=0)\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        alphas = self.transitions[:, self.start_idx].unsqueeze(0)\n",
    "        \n",
    "        for t in range(feats.size(0)):\n",
    "            # alphas: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "            # feats[t]: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "            alphas = torch.logsumexp((alphas + feats[t]).unsqueeze(1) + self.transitions, dim=-1)\n",
    "\n",
    "        return alphas[:, self.stop_idx]\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        alphas = self.transitions[:, self.start_idx].unsqueeze(0)\n",
    "        # best_paths: (step=1, batch, tag_dim)\n",
    "        best_paths = torch.arange(feats.size(-1)).repeat(feats.size(1), 1).unsqueeze(0)\n",
    "\n",
    "        for t in range(feats.size(0)):\n",
    "            # alphas: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "            # feats[t]: (batch=1, tag_dim) -> (batch=1, 1, tag_dim)\n",
    "            # indices: (batch=1, tag_dim)\n",
    "            # indices tell that given the current tag, which is the best tag for last step?\n",
    "            alphas, indices = torch.max((alphas + feats[t]).unsqueeze(1) + self.transitions, dim=-1)\n",
    "\n",
    "            # selected_paths: (step, batch=1, tag_dim) \n",
    "            # The paths selected according to indices\n",
    "            selected_paths = torch.cat([best_paths[:, i, indices[i]].unsqueeze(1) for i in range(feats.size(1))], dim=1)\n",
    "            this_step = torch.arange(feats.size(-1)).repeat(feats.size(1), 1).unsqueeze(0)\n",
    "            best_paths = torch.cat([selected_paths, this_step], dim=0)\n",
    "\n",
    "        return alphas[:, self.stop_idx], best_paths[:, :, self.stop_idx]\n",
    "\n",
    "    def neg_log_likelihood(self, src, tags):\n",
    "        feats = self._get_features(src)\n",
    "        partitions = self._forward_alg(feats)\n",
    "        scores = self._score_sentence(feats, tags)\n",
    "        return partitions - scores\n",
    "\n",
    "    def forward(self, src):\n",
    "        feats = self._get_features(src)\n",
    "\n",
    "        scores, tags = self._viterbi_decode(feats)\n",
    "        return scores, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([-0.2256], grad_fn=<AddBackward0>)\ntensor([2.4575], grad_fn=<SelectBackward>)\n(tensor([0.0290], grad_fn=<SelectBackward>), tensor([[2],\n        [1],\n        [1],\n        [1],\n        [2],\n        [4]]))\ntensor([2.6831], grad_fn=<SubBackward0>)\n(tensor([0.0290], grad_fn=<SelectBackward>), tensor([[2],\n        [1],\n        [1],\n        [1],\n        [2],\n        [4]]))\n"
    }
   ],
   "source": [
    "model = BiLSTMCRF(VOC_DIM, EMB_DIM, HID_DIM, TAG_DIM, tag_to_ix[START_TAG], tag_to_ix[STOP_TAG])\n",
    "model.emb = emb\n",
    "model.rnn = rnn\n",
    "model.hid2tag = hid2tag\n",
    "model.transitions = transitions\n",
    "\n",
    "\n",
    "feats = model._get_features(sent)\n",
    "print(model._score_sentence(feats, tags))\n",
    "print(model._forward_alg(feats))\n",
    "print(model._viterbi_decode(feats))\n",
    "\n",
    "print(model.neg_log_likelihood(sent, tags))\n",
    "print(model(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0, 11],\n        [ 1, 12],\n        [ 2, 13],\n        [ 3, 14],\n        [ 4, 15]])\ntensor([[0, 0],\n        [1, 1],\n        [1, 2],\n        [1, 2],\n        [2, 2]])\n"
    }
   ],
   "source": [
    "src1 = torch.tensor([word_to_ix[w] for w in training_data[0][0]], dtype=torch.long).unsqueeze(1)[:5]\n",
    "src2 = torch.tensor([word_to_ix[w] for w in training_data[1][0]], dtype=torch.long).unsqueeze(1)[:5]\n",
    "batch_src = torch.cat([src1, src2], dim=1)\n",
    "print(batch_src)\n",
    "\n",
    "tags1 = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long).unsqueeze(1)[:5]\n",
    "tags2 = torch.tensor([tag_to_ix[t] for t in training_data[1][1]], dtype=torch.long).unsqueeze(1)[:5]\n",
    "batch_tags = torch.cat([tags1, tags2], dim=1)\n",
    "print(batch_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([2.6831, 5.0988], grad_fn=<SubBackward0>)\n(tensor([0.0290, 0.3021], grad_fn=<SelectBackward>), tensor([[2, 2],\n        [1, 1],\n        [1, 1],\n        [1, 1],\n        [2, 2],\n        [4, 4]]))\n"
    }
   ],
   "source": [
    "print(model.neg_log_likelihood(batch_src, batch_tags))\n",
    "print(model(batch_src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0.0290, 0.3021], grad_fn=<SelectBackward>)\ntensor([[2, 2],\n        [1, 1],\n        [1, 1],\n        [1, 1],\n        [2, 2],\n        [4, 4]])\ntensor([[0, 0],\n        [1, 1],\n        [1, 2],\n        [1, 2],\n        [2, 2]])\n"
    }
   ],
   "source": [
    "scores, decoded = model(batch_src)\n",
    "print(scores)\n",
    "print(decoded)\n",
    "print(batch_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 3.9643068313598633\n100 2.418396472930908\n150 1.6392817497253418\n200 1.1952505111694336\n250 0.9155898094177246\n300 0.7282357215881348\n"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(300): \n",
    "    # Forward pass & Calculate loss\n",
    "    batch_losses = model.neg_log_likelihood(batch_src, batch_tags)\n",
    "    loss = batch_losses.sum()\n",
    "    # Backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(epoch+1, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([6.4811, 6.9006], grad_fn=<SelectBackward>)\ntensor([[0, 0],\n        [1, 1],\n        [1, 2],\n        [1, 2],\n        [2, 2],\n        [4, 4]])\ntensor([[0, 0],\n        [1, 1],\n        [1, 2],\n        [1, 2],\n        [2, 2]])\n"
    }
   ],
   "source": [
    "scores, decoded = model(batch_src)\n",
    "print(scores)\n",
    "print(decoded)\n",
    "print(batch_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-1.5686e+00, -1.8517e+00, -2.1087e+00,  6.7935e-01, -9.9971e+03],\n        [ 6.5971e-01,  3.3785e-01, -2.2905e+00, -2.0786e+00, -9.9971e+03],\n        [-1.5681e+00,  7.6895e-01, -2.7771e-01, -2.5855e-01, -9.9971e+03],\n        [-9.9971e+03, -9.9971e+03, -9.9971e+03, -9.9971e+03, -9.9971e+03],\n        [-8.0492e-01, -1.2626e+00,  9.3918e-01,  1.1364e+00, -9.9971e+03]],\n       requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "model.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
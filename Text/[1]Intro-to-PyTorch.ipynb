{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensors and Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1., 2., 3.],\n        [6., 5., 4.]])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "m_data = [[1, 2, 3],\n",
    "          [6, 5, 4]]\n",
    "m = torch.tensor(m_data, dtype=torch.float)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-0.3902,  1.4256,  0.0491, -0.0559],\n         [-1.0172,  0.6198,  0.3173, -1.8878],\n         [-0.3775,  1.6218, -0.5878,  0.0452]],\n\n        [[-0.5670,  1.6442,  1.6313, -1.2841],\n         [ 0.0240,  0.4192,  1.3738,  0.6919],\n         [-0.7332, -0.2310,  0.1162, -0.6269]]])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "x = torch.randn((2, 3, 4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-1.1112, -0.3307,  1.0290, -0.5752],\n        [ 0.2071, -0.6596,  1.4697,  0.1795]])\ntensor([[ 0.3530, -0.2241, -0.1479,  0.6436],\n        [ 1.1626, -0.2370,  0.1363,  0.1794],\n        [ 0.9533, -1.3683, -1.6694,  0.3587]])\ntensor([[-1.1112, -0.3307,  1.0290, -0.5752],\n        [ 0.2071, -0.6596,  1.4697,  0.1795],\n        [ 0.3530, -0.2241, -0.1479,  0.6436],\n        [ 1.1626, -0.2370,  0.1363,  0.1794],\n        [ 0.9533, -1.3683, -1.6694,  0.3587]])\n"
    }
   ],
   "source": [
    "x1 = torch.randn((2, 4))\n",
    "y1 = torch.randn((3, 4))\n",
    "print(x1)\n",
    "print(y1)\n",
    "# By default, concatenate along first axis (dim)\n",
    "# like np.concatenate...\n",
    "z1 = torch.cat([x1, y1], dim=0)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[-8.4383e-01, -1.4043e+00, -1.6591e-03, -4.5854e-01],\n         [ 4.7313e-03,  4.8140e-01,  3.2641e+00, -2.0551e+00],\n         [ 8.1119e-01, -1.0239e+00,  1.2149e-01, -1.3790e-01]],\n\n        [[-1.3063e+00,  1.4654e-01, -7.1071e-02,  3.5700e-01],\n         [-2.8966e-01, -6.3497e-01, -1.2692e+00, -1.5048e+00],\n         [-1.4057e+00, -8.9566e-01, -8.9407e-01, -3.6626e-01]]])\ntensor([[-8.4383e-01, -1.4043e+00, -1.6591e-03, -4.5854e-01,  4.7313e-03,\n          4.8140e-01,  3.2641e+00, -2.0551e+00,  8.1119e-01, -1.0239e+00,\n          1.2149e-01, -1.3790e-01],\n        [-1.3063e+00,  1.4654e-01, -7.1071e-02,  3.5700e-01, -2.8966e-01,\n         -6.3497e-01, -1.2692e+00, -1.5048e+00, -1.4057e+00, -8.9566e-01,\n         -8.9407e-01, -3.6626e-01]])\ntensor([[-8.4383e-01, -1.4043e+00, -1.6591e-03, -4.5854e-01,  4.7313e-03,\n          4.8140e-01,  3.2641e+00, -2.0551e+00,  8.1119e-01, -1.0239e+00,\n          1.2149e-01, -1.3790e-01],\n        [-1.3063e+00,  1.4654e-01, -7.1071e-02,  3.5700e-01, -2.8966e-01,\n         -6.3497e-01, -1.2692e+00, -1.5048e+00, -1.4057e+00, -8.9566e-01,\n         -8.9407e-01, -3.6626e-01]])\n"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "# like np.reshape...\n",
    "print(x.view(2, 12))\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1., 2., 3.])\ntensor([5., 7., 9.])\nNone\n"
    }
   ],
   "source": [
    "# The Tensors have requires_grad=False, NOT tracking computation history, by default. \n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(x)\n",
    "\n",
    "y = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1., 2., 3.], requires_grad=True)\ntensor([5., 7., 9.], grad_fn=<AddBackward0>)\n<AddBackward0 object at 0x7f7349936fd0>\n"
    }
   ],
   "source": [
    "# The Tensors have requires_grad=True, tracking computation history. \n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = torch.tensor([4, 5, 6], dtype=torch.float32, requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(21., grad_fn=<SumBackward0>)\n<SumBackward0 object at 0x7f73494f1d60>\ntensor([1., 1., 1.])\n"
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)\n",
    "\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n"
    }
   ],
   "source": [
    "# Detach from computation tracking. \n",
    "new_z = z.detach()\n",
    "print(new_z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Layers and Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[ 0.2485,  0.2113, -0.1850,  0.0898, -0.1738],\n         [-0.4004,  0.0753, -0.1953, -0.1229, -0.4364],\n         [ 0.3894,  0.1676,  0.2391,  0.3415,  0.0402]], requires_grad=True),\n Parameter containing:\n tensor([0.3819, 0.1956, 0.2790], requires_grad=True)]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Linear layer mapping from dim=5 to dim=3\n",
    "# The layer includes parameters W, b\n",
    "lin = nn.Linear(5, 3)\n",
    "list(lin.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-1.7785, -1.7638, -1.0277, -1.1721,  1.0654],\n        [-0.1361, -0.4240,  0.5884, -1.1480, -1.3654]])\ntensor([[-0.5331,  0.6546, -1.3124],\n        [ 0.2838,  0.8401, -0.1514]], grad_fn=<AddmmBackward>)\ntensor([[-0.5331,  0.6546, -1.3124],\n        [ 0.2838,  0.8401, -0.1514]], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "x = torch.randn(2, 5)\n",
    "print(x)\n",
    "\n",
    "print(lin(x))\n",
    "print(x.mm(lin.weight.T) + lin.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.2580, -0.3184],\n        [-0.3047,  0.3732]])\ntensor([[0.0000, 0.0000],\n        [0.0000, 0.3732]])\n"
    }
   ],
   "source": [
    "# Most people default to tanh or ReLU as non-linearity\n",
    "x = torch.randn(2, 2)\n",
    "print(x)\n",
    "print(F.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.7600,  0.0070, -2.0061,  0.3675, -0.5086],\n        [-1.1502,  0.3516, -1.2420,  2.0638,  0.0822]])\ntensor([[0.4015, 0.1891, 0.0253, 0.2712, 0.1129],\n        [0.0288, 0.1293, 0.0263, 0.7168, 0.0988]])\ntensor([1.0000, 1.0000])\ntensor([[-0.9125, -1.6654, -3.6786, -1.3050, -2.1810],\n        [-3.5471, -2.0453, -3.6388, -0.3330, -2.3147]])\n"
    }
   ],
   "source": [
    "# Softmax & Probability\n",
    "x = torch.randn(2, 5)\n",
    "print(x)\n",
    "\n",
    "# dim=-1 -> apply to the most inner axis\n",
    "print(F.softmax(x, dim=-1))\n",
    "print(F.softmax(x, dim=-1).sum(dim=1))\n",
    "print(F.log_softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
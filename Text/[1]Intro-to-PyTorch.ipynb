{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensors and Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1., 2., 3.],\n        [6., 5., 4.]])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "m_data = [[1, 2, 3],\n",
    "          [6, 5, 4]]\n",
    "m = torch.tensor(m_data, dtype=torch.float)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0.8621,  1.3001,  1.6746, -0.9454],\n         [-1.2412,  2.0782, -0.6528, -0.5414],\n         [ 0.0771, -0.9474,  2.1355,  0.0284]],\n\n        [[-0.6245,  0.9582,  1.1625,  0.3573],\n         [-0.2623,  1.5938,  0.4975,  0.3255],\n         [-0.9988, -0.2040,  1.6705,  0.4629]]])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "x = torch.randn((2, 3, 4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.4317, -0.1789, -1.4176,  2.7956],\n        [ 2.4455,  0.5367, -1.8635, -0.0413]])\ntensor([[-0.3488, -0.3780, -0.3134, -2.0054],\n        [ 0.2738, -0.8761,  2.0643, -1.2928],\n        [-0.1407,  0.6571,  0.0211,  0.5552]])\ntensor([[ 0.4317, -0.1789, -1.4176,  2.7956],\n        [ 2.4455,  0.5367, -1.8635, -0.0413],\n        [-0.3488, -0.3780, -0.3134, -2.0054],\n        [ 0.2738, -0.8761,  2.0643, -1.2928],\n        [-0.1407,  0.6571,  0.0211,  0.5552]])\n"
    }
   ],
   "source": [
    "x1 = torch.randn((2, 4))\n",
    "y1 = torch.randn((3, 4))\n",
    "print(x1)\n",
    "print(y1)\n",
    "# By default, concatenate along first axis (dim)\n",
    "# like np.concatenate...\n",
    "z1 = torch.cat([x1, y1], dim=0)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[-0.4134, -2.3437,  0.3276,  0.4520],\n         [ 2.2598, -1.0423, -0.4690, -0.3379],\n         [-0.0252,  0.0714, -1.1776,  0.7061]],\n\n        [[ 0.7615, -1.3326,  2.4765,  1.3274],\n         [-0.0181, -2.7809, -0.6896, -0.7834],\n         [ 0.2362,  1.2167,  1.4762,  1.2964]]])\ntensor([[-0.4134, -2.3437,  0.3276,  0.4520,  2.2598, -1.0423, -0.4690, -0.3379,\n         -0.0252,  0.0714, -1.1776,  0.7061],\n        [ 0.7615, -1.3326,  2.4765,  1.3274, -0.0181, -2.7809, -0.6896, -0.7834,\n          0.2362,  1.2167,  1.4762,  1.2964]])\ntensor([[-0.4134, -2.3437,  0.3276,  0.4520,  2.2598, -1.0423, -0.4690, -0.3379,\n         -0.0252,  0.0714, -1.1776,  0.7061],\n        [ 0.7615, -1.3326,  2.4765,  1.3274, -0.0181, -2.7809, -0.6896, -0.7834,\n          0.2362,  1.2167,  1.4762,  1.2964]])\n"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "# like np.reshape...\n",
    "print(x.view(2, 12))\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1., 2., 3.])\ntensor([5., 7., 9.])\nNone\n"
    }
   ],
   "source": [
    "# The Tensors have requires_grad=False, NOT tracking computation history, by default. \n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(x)\n",
    "\n",
    "y = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1., 2., 3.], requires_grad=True)\ntensor([5., 7., 9.], grad_fn=<AddBackward0>)\n<AddBackward0 object at 0x0000023E3F3E0FC8>\n"
    }
   ],
   "source": [
    "# The Tensors have requires_grad=True, tracking computation history. \n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = torch.tensor([4, 5, 6], dtype=torch.float32, requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(21., grad_fn=<SumBackward0>)\n<SumBackward0 object at 0x0000023E3F3E6C08>\ntensor([1., 1., 1.])\n"
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)\n",
    "\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n"
    }
   ],
   "source": [
    "# Detach from computation tracking. \n",
    "new_z = z.detach()\n",
    "print(new_z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Layers and Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-0.0579,  0.3437,  0.3479,  0.3967, -0.3022],\n         [-0.4053, -0.0244, -0.1431,  0.3994, -0.4026],\n         [-0.0929,  0.3741, -0.2829,  0.1329, -0.3502]], requires_grad=True),\n Parameter containing:\n tensor([0.1193, 0.1004, 0.3738], requires_grad=True)]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Linear layer mapping from dim=5 to dim=3\n",
    "# The layer includes parameters W, b\n",
    "lin = nn.Linear(5, 3)\n",
    "list(lin.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.3871,  1.1387,  0.6009, -0.4370, -1.4277],\n        [-1.0157,  1.2764,  0.7053, -1.0976, -0.6602]])\ntensor([[0.9554, 0.2300, 1.0358],\n        [0.6262, 0.2075, 0.8315]], grad_fn=<AddmmBackward>)\ntensor([[0.9554, 0.2300, 1.0358],\n        [0.6262, 0.2075, 0.8315]], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "x = torch.randn(2, 5)\n",
    "print(x)\n",
    "\n",
    "print(lin(x))\n",
    "print(x.mm(lin.weight.T) + lin.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-1.9328,  1.3682],\n        [-1.8452,  1.3094]])\ntensor([[0.0000, 1.3682],\n        [0.0000, 1.3094]])\n"
    }
   ],
   "source": [
    "# Most people default to tanh or ReLU as non-linearity\n",
    "x = torch.randn(2, 2)\n",
    "print(x)\n",
    "print(F.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.1544,  1.5411,  0.2937,  1.4034, -0.0031],\n        [ 1.3801, -1.1911, -1.6563,  0.0643, -0.9061]])\ntensor([[0.0953, 0.3814, 0.1096, 0.3323, 0.0814],\n        [0.6692, 0.0512, 0.0321, 0.1795, 0.0680]])\ntensor([1.0000, 1.0000])\ntensor([[-2.3507, -0.9639, -2.2114, -1.1016, -2.5082],\n        [-0.4017, -2.9729, -3.4382, -1.7175, -2.6879]])\n"
    }
   ],
   "source": [
    "# Softmax & Probability\n",
    "x = torch.randn(2, 5)\n",
    "print(x)\n",
    "\n",
    "# dim=-1 -> apply to the most inner axis\n",
    "print(F.softmax(x, dim=-1))\n",
    "print(F.softmax(x, dim=-1).sum(dim=1))\n",
    "print(F.log_softmax(x, dim=-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
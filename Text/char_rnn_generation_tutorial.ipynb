{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNLP From Scratch: Generating Names with a Character-Level RNN\n*************************************************************\n**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n\nThis is our second of three tutorials on \"NLP From Scratch\".\nIn the `first tutorial </intermediate/char_rnn_classification_tutorial>`\nwe used a RNN to classify names into their language of origin. This time\nwe'll turn around and generate names from languages.\n\n::\n\n    > python sample.py Russian RUS\n    Rovakov\n    Uantov\n    Shavakov\n\n    > python sample.py German GER\n    Gerren\n    Ereng\n    Rosher\n\n    > python sample.py Spanish SPA\n    Salla\n    Parer\n    Allan\n\n    > python sample.py Chinese CHI\n    Chan\n    Hang\n    Iun\n\nWe are still hand-crafting a small RNN with a few linear layers. The big\ndifference is instead of predicting a category after reading in all the\nletters of a name, we input a category and output one letter at a time.\nRecurrently predicting characters to form language (this could also be\ndone with words or other higher order constructs) is often referred to\nas a \"language model\".\n\n**Recommended Reading:**\n\nI assume you have at least installed PyTorch, know Python, and\nunderstand Tensors:\n\n-  https://pytorch.org/ For installation instructions\n-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n\nIt would also be useful to know about RNNs and how they work:\n\n-  `The Unreasonable Effectiveness of Recurrent Neural\n   Networks <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n   shows a bunch of real life examples\n-  `Understanding LSTM\n   Networks <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n   is about LSTMs specifically but also informative about RNNs in\n   general\n\nI also suggest the previous tutorial, :doc:`/intermediate/char_rnn_classification_tutorial`\n\n\nPreparing the Data\n==================\n\n.. Note::\n   Download the data from\n   `here <https://download.pytorch.org/tutorial/data.zip>`_\n   and extract it to the current directory.\n\nSee the last tutorial for more detail of this process. In short, there\nare a bunch of plain text files ``data/names/[Language].txt`` with a\nname per line. We split lines into an array, convert Unicode to ASCII,\nand end up with a dictionary ``{language: [names ...]}``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "# categories: 18 ['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\nO'Neal\n"
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'-\"\nn_letters = len(all_letters) + 1 # Plus EOS marker\n\ndef findFiles(path): return glob.glob(path)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\n# Build the category_lines dictionary, a list of lines per category\ncategory_lines = {}\nall_categories = []\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)\n\nif n_categories == 0:\n    raise RuntimeError('Data not found. Make sure that you downloaded data '\n        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n        'the current directory.')\n\nprint('# categories:', n_categories, all_categories)\nprint(unicodeToAscii(\"O'Néàl\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the Network\n====================\n\nThis network extends `the last tutorial's RNN <#Creating-the-Network>`__\nwith an extra argument for the category tensor, which is concatenated\nalong with the others. The category tensor is a one-hot vector just like\nthe letter input.\n\nWe will interpret the output as the probability of the next letter. When\nsampling, the most likely output letter is used as the next input\nletter.\n\nI added a second linear layer ``o2o`` (after combining hidden and\noutput) to give it more muscle to work with. There's also a dropout\nlayer, which `randomly zeros parts of its\ninput <https://arxiv.org/abs/1207.0580>`__ with a given probability\n(here 0.1) and is usually used to fuzz inputs to prevent overfitting.\nHere we're using it towards the end of the network to purposely add some\nchaos and increase sampling variety.\n\n.. figure:: https://i.imgur.com/jzVrf7f.png\n   :alt:\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n        self.dropout = nn.Dropout(0.1)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, category, input, hidden):\n        input_combined = torch.cat((category, input, hidden), 1)\n        hidden = self.i2h(input_combined)\n        output = self.i2o(input_combined)\n        output_combined = torch.cat((hidden, output), 1)\n        output = self.o2o(output_combined)\n        output = self.dropout(output)\n        output = self.softmax(output)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n=========\nPreparing for Training\n----------------------\n\nFirst of all, helper functions to get random pairs of (category, line):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\n\n# Random item from a list\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\n# Get a random category and random line from that category\ndef randomTrainingPair():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n    return category, line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each timestep (that is, for each letter in a training word) the\ninputs of the network will be\n``(category, current letter, hidden state)`` and the outputs will be\n``(next letter, next hidden state)``. So for each training set, we'll\nneed the category, a set of input letters, and a set of output/target\nletters.\n\nSince we are predicting the next letter from the current letter for each\ntimestep, the letter pairs are groups of consecutive letters from the\nline - e.g. for ``\"ABCD<EOS>\"`` we would create (\"A\", \"B\"), (\"B\", \"C\"),\n(\"C\", \"D\"), (\"D\", \"EOS\").\n\n.. figure:: https://i.imgur.com/JH58tXY.png\n   :alt:\n\nThe category tensor is a `one-hot\ntensor <https://en.wikipedia.org/wiki/One-hot>`__ of size\n``<1 x n_categories>``. When training we feed it to the network at every\ntimestep - this is a design choice, it could have been included as part\nof initial hidden state or some other strategy.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# One-hot vector for category\ndef categoryTensor(category):\n    li = all_categories.index(category)\n    tensor = torch.zeros(1, n_categories)\n    tensor[0][li] = 1\n    return tensor\n\n# One-hot matrix of first to last letters (not including EOS) for input\ndef inputTensor(line):\n    tensor = torch.zeros(len(line), 1, n_letters)\n    for li in range(len(line)):\n        letter = line[li]\n        tensor[li][0][all_letters.find(letter)] = 1\n    return tensor\n\n# LongTensor of second letter to end (EOS) for target\ndef targetTensor(line):\n    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n    letter_indexes.append(n_letters - 1) # EOS\n    return torch.LongTensor(letter_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For convenience during training we'll make a ``randomTrainingExample``\nfunction that fetches a random (category, line) pair and turns them into\nthe required (category, input, target) tensors.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make category, input, and target tensors from a random category, line pair\ndef randomTrainingExample():\n    category, line = randomTrainingPair()\n    category_tensor = categoryTensor(category)\n    input_line_tensor = inputTensor(line)\n    target_line_tensor = targetTensor(line)\n    return category_tensor, input_line_tensor, target_line_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the Network\n--------------------\n\nIn contrast to classification, where only the last output is used, we\nare making a prediction at every step, so we are calculating loss at\nevery step.\n\nThe magic of autograd allows you to simply sum these losses at each step\nand call backward at the end.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()\n\nlearning_rate = 0.0005\n\ndef train(category_tensor, input_line_tensor, target_line_tensor):\n    target_line_tensor.unsqueeze_(-1)\n    hidden = rnn.initHidden()\n\n    rnn.zero_grad()\n\n    loss = 0\n\n    for i in range(input_line_tensor.size(0)):\n        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n        l = criterion(output, target_line_tensor[i])\n        loss += l\n\n    loss.backward()\n\n    for p in rnn.parameters():\n        p.data.add_(p.grad.data, alpha=-learning_rate)\n\n    return output, loss.item() / input_line_tensor.size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To keep track of how long training takes I am adding a\n``timeSince(timestamp)`` function which returns a human readable string:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport math\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training is business as usual - call train a bunch of times and wait a\nfew minutes, printing the current time and loss every ``print_every``\nexamples, and keeping store of an average loss per ``plot_every`` examples\nin ``all_losses`` for plotting later.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0m 18s (5000 5%) 2.5216\n0m 36s (10000 10%) 3.2339\n0m 54s (15000 15%) 2.9643\n1m 12s (20000 20%) 2.2843\n1m 31s (25000 25%) 2.3275\n1m 49s (30000 30%) 2.5861\n2m 8s (35000 35%) 3.0170\n2m 26s (40000 40%) 2.2301\n2m 44s (45000 45%) 3.4245\n3m 3s (50000 50%) 2.1832\n3m 21s (55000 55%) 3.7772\n3m 40s (60000 60%) 1.9668\n3m 58s (65000 65%) 2.8422\n4m 16s (70000 70%) 1.9457\n4m 35s (75000 75%) 3.8061\n4m 53s (80000 80%) 2.1763\n5m 12s (85000 85%) 1.9842\n5m 30s (90000 90%) 2.8182\n5m 49s (95000 95%) 2.1280\n6m 7s (100000 100%) 2.3256\n"
        }
      ],
      "source": [
        "rnn = RNN(n_letters, 128, n_letters)\n\nn_iters = 100000\nprint_every = 5000\nplot_every = 500\nall_losses = []\ntotal_loss = 0 # Reset every plot_every iters\n\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    output, loss = train(*randomTrainingExample())\n    total_loss += loss\n\n    if iter % print_every == 0:\n        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n\n    if iter % plot_every == 0:\n        all_losses.append(total_loss / plot_every)\n        total_loss = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the Losses\n-------------------\n\nPlotting the historical loss from all\\_losses shows the network\nlearning:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[<matplotlib.lines.Line2D at 0x1e6638b9048>]"
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m6a389d8181\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"89.920445\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 25 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(83.557945 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.157082\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(121.794582 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.39372\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 75 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(160.03122 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.630358\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(195.086608 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"242.866995\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 125 -->\r\n      <g transform=\"translate(233.323245 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"281.103633\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(271.559883 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"319.340271\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 175 -->\r\n      <g transform=\"translate(309.796521 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"357.576909\" xlink:href=\"#m6a389d8181\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(348.033159 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m5a353fe10d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a353fe10d\" y=\"218.317193\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 2.25 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 222.116411)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a353fe10d\" y=\"189.128278\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 2.50 -->\r\n      <g transform=\"translate(7.2 192.927497)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a353fe10d\" y=\"159.939364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 2.75 -->\r\n      <g transform=\"translate(7.2 163.738582)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a353fe10d\" y=\"130.750449\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 3.00 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 134.549668)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a353fe10d\" y=\"101.561535\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 3.25 -->\r\n      <g transform=\"translate(7.2 105.360753)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a353fe10d\" y=\"72.37262\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 3.50 -->\r\n      <g transform=\"translate(7.2 76.171839)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a353fe10d\" y=\"43.183706\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 3.75 -->\r\n      <g transform=\"translate(7.2 46.982924)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a353fe10d\" y=\"13.994791\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 4.00 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 17.79401)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#pd51a5ebf7d)\" d=\"M 51.683807 17.083636 \r\nL 53.213272 49.592369 \r\nL 54.742738 100.263292 \r\nL 56.272203 111.95057 \r\nL 57.801669 118.863066 \r\nL 59.331134 124.084947 \r\nL 60.8606 125.717857 \r\nL 62.390065 134.613292 \r\nL 63.919531 134.916567 \r\nL 65.448996 138.956886 \r\nL 66.978462 135.632616 \r\nL 70.037393 145.126553 \r\nL 71.566858 144.630618 \r\nL 73.096324 148.207587 \r\nL 74.625789 147.370181 \r\nL 76.155255 148.319573 \r\nL 77.68472 152.683583 \r\nL 79.214186 152.844167 \r\nL 80.743651 154.004739 \r\nL 82.273117 154.977046 \r\nL 85.332048 159.079975 \r\nL 86.861514 159.105538 \r\nL 88.390979 160.418614 \r\nL 89.920445 163.194365 \r\nL 91.44991 159.60544 \r\nL 92.979376 168.407882 \r\nL 94.508841 168.98242 \r\nL 96.038307 162.746714 \r\nL 97.567772 166.867231 \r\nL 99.097238 165.767319 \r\nL 100.626703 168.556262 \r\nL 102.156169 174.231992 \r\nL 103.685634 169.745013 \r\nL 105.2151 172.034618 \r\nL 106.744565 172.486066 \r\nL 108.274031 171.203116 \r\nL 109.803496 176.66587 \r\nL 111.332962 175.534489 \r\nL 112.862427 171.977545 \r\nL 114.391893 174.077031 \r\nL 115.921358 175.646615 \r\nL 117.450824 175.368094 \r\nL 118.980289 172.897329 \r\nL 122.03922 179.720773 \r\nL 123.568686 177.818546 \r\nL 125.098151 178.135928 \r\nL 126.627617 181.541116 \r\nL 128.157082 177.744173 \r\nL 129.686548 178.917368 \r\nL 131.216013 181.730176 \r\nL 132.745479 183.639096 \r\nL 134.274944 185.0633 \r\nL 135.80441 183.912485 \r\nL 137.333875 186.591559 \r\nL 138.863341 185.145167 \r\nL 140.392806 185.641197 \r\nL 141.922272 185.44507 \r\nL 143.451737 182.485825 \r\nL 144.981203 186.786368 \r\nL 146.510668 190.224952 \r\nL 148.040134 190.169013 \r\nL 149.569599 189.593723 \r\nL 151.099065 188.608435 \r\nL 152.62853 192.410084 \r\nL 154.157996 187.35071 \r\nL 155.687461 187.451419 \r\nL 157.216927 193.445315 \r\nL 158.746392 190.778364 \r\nL 160.275858 193.636816 \r\nL 161.805323 188.467942 \r\nL 163.334789 188.013028 \r\nL 164.864255 189.354025 \r\nL 166.39372 189.767316 \r\nL 167.923186 188.817917 \r\nL 169.452651 195.357496 \r\nL 170.982117 193.694387 \r\nL 172.511582 187.453777 \r\nL 174.041048 193.578808 \r\nL 175.570513 193.852247 \r\nL 177.099979 191.40845 \r\nL 178.629444 192.759966 \r\nL 180.15891 192.484372 \r\nL 181.688375 191.528846 \r\nL 183.217841 196.203449 \r\nL 184.747306 189.070227 \r\nL 186.276772 193.125179 \r\nL 187.806237 196.572893 \r\nL 189.335703 199.035245 \r\nL 190.865168 198.012556 \r\nL 192.394634 195.216584 \r\nL 193.924099 193.773179 \r\nL 195.453565 200.229401 \r\nL 196.98303 194.261454 \r\nL 200.041961 201.08718 \r\nL 201.571427 199.817314 \r\nL 203.100892 196.026461 \r\nL 204.630358 198.02206 \r\nL 206.159823 202.796262 \r\nL 207.689289 200.412613 \r\nL 209.218754 191.526389 \r\nL 210.74822 201.186391 \r\nL 212.277685 203.45318 \r\nL 216.866082 200.321233 \r\nL 218.395547 197.936045 \r\nL 219.925013 201.773645 \r\nL 221.454478 201.453563 \r\nL 222.983944 201.47812 \r\nL 224.513409 199.838279 \r\nL 226.042875 199.30071 \r\nL 227.57234 204.429359 \r\nL 229.101806 201.917701 \r\nL 230.631271 206.744296 \r\nL 232.160737 201.298935 \r\nL 233.690202 205.19203 \r\nL 235.219668 205.504575 \r\nL 236.749133 196.176823 \r\nL 238.278599 201.262581 \r\nL 239.808064 202.548477 \r\nL 241.33753 200.353218 \r\nL 242.866995 203.284714 \r\nL 244.396461 205.34431 \r\nL 245.925927 204.454377 \r\nL 247.455392 197.302308 \r\nL 248.984858 202.51551 \r\nL 250.514323 202.00929 \r\nL 252.043789 202.276366 \r\nL 253.573254 210.511247 \r\nL 255.10272 201.457571 \r\nL 256.632185 202.275117 \r\nL 258.161651 202.432825 \r\nL 259.691116 204.162697 \r\nL 261.220582 208.344622 \r\nL 262.750047 203.657727 \r\nL 264.279513 206.536592 \r\nL 265.808978 201.537194 \r\nL 267.338444 206.387414 \r\nL 268.867909 203.25036 \r\nL 270.397375 209.066315 \r\nL 271.92684 203.467072 \r\nL 273.456306 201.548278 \r\nL 274.985771 201.828778 \r\nL 276.515237 203.258377 \r\nL 278.044702 200.193099 \r\nL 279.574168 209.057484 \r\nL 281.103633 204.145944 \r\nL 282.633099 208.476439 \r\nL 284.162564 208.279765 \r\nL 285.69203 205.376772 \r\nL 287.221495 209.199911 \r\nL 288.750961 207.222716 \r\nL 290.280426 203.424207 \r\nL 291.809892 204.326969 \r\nL 293.339357 210.005282 \r\nL 294.868823 210.172011 \r\nL 296.398288 206.351831 \r\nL 297.927754 211.503761 \r\nL 299.457219 209.965142 \r\nL 300.986685 201.57561 \r\nL 302.51615 207.967135 \r\nL 304.045616 208.95503 \r\nL 305.575081 206.512627 \r\nL 307.104547 207.964109 \r\nL 308.634012 205.957324 \r\nL 310.163478 208.279733 \r\nL 311.692943 204.007808 \r\nL 313.222409 208.539925 \r\nL 314.751874 213.635154 \r\nL 316.28134 205.387978 \r\nL 317.810805 210.935722 \r\nL 319.340271 205.676997 \r\nL 320.869736 208.38646 \r\nL 322.399202 208.750527 \r\nL 323.928667 210.605178 \r\nL 325.458133 212.826308 \r\nL 328.517064 206.723525 \r\nL 330.04653 211.125836 \r\nL 331.575995 207.591957 \r\nL 333.105461 210.363722 \r\nL 334.634926 209.126102 \r\nL 336.164392 203.068583 \r\nL 337.693857 214.413428 \r\nL 339.223323 209.515768 \r\nL 340.752788 214.610066 \r\nL 342.282254 213.042944 \r\nL 343.811719 205.772523 \r\nL 345.341185 212.716489 \r\nL 346.87065 213.877272 \r\nL 348.400116 208.775633 \r\nL 349.929581 212.740917 \r\nL 351.459047 208.936286 \r\nL 352.988512 207.728621 \r\nL 354.517978 214.756364 \r\nL 356.047443 212.444968 \r\nL 356.047443 212.444968 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd51a5ebf7d\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sampling the Network\n====================\n\nTo sample we give the network a letter and ask what the next one is,\nfeed that in as the next letter, and repeat until the EOS token.\n\n-  Create tensors for input category, starting letter, and empty hidden\n   state\n-  Create a string ``output_name`` with the starting letter\n-  Up to a maximum output length,\n\n   -  Feed the current letter to the network\n   -  Get the next letter from highest output, and next hidden state\n   -  If the letter is EOS, stop here\n   -  If a regular letter, add to ``output_name`` and continue\n\n-  Return the final name\n\n.. Note::\n   Rather than having to give it a starting letter, another\n   strategy would have been to include a \"start of string\" token in\n   training and have the network choose its own starting letter.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Rovellov\nUanghov\nShavanov\nGerren\nEren\nRonger\nSanteran\nParer\nAllan\nChan\nHan\nIun\n"
        }
      ],
      "source": [
        "max_length = 20\n\n# Sample from a category and starting letter\ndef sample(category, start_letter='A'):\n    with torch.no_grad():  # no need to track history in sampling\n        category_tensor = categoryTensor(category)\n        input = inputTensor(start_letter)\n        hidden = rnn.initHidden()\n\n        output_name = start_letter\n\n        for i in range(max_length):\n            output, hidden = rnn(category_tensor, input[0], hidden)\n            topv, topi = output.topk(1)\n            topi = topi[0][0]\n            if topi == n_letters - 1:\n                break\n            else:\n                letter = all_letters[topi]\n                output_name += letter\n            input = inputTensor(letter)\n\n        return output_name\n\n# Get multiple samples from one category and multiple starting letters\ndef samples(category, start_letters='ABC'):\n    for start_letter in start_letters:\n        print(sample(category, start_letter))\n\nsamples('Russian', 'RUS')\n\nsamples('German', 'GER')\n\nsamples('Spanish', 'SPA')\n\nsamples('Chinese', 'CHI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exercises\n=========\n\n-  Try with a different dataset of category -> line, for example:\n\n   -  Fictional series -> Character name\n   -  Part of speech -> Word\n   -  Country -> City\n\n-  Use a \"start of sentence\" token so that sampling can be done without\n   choosing a start letter\n-  Get better results with a bigger and/or better shaped network\n\n   -  Try the nn.LSTM and nn.GRU layers\n   -  Combine multiple of these RNNs as a higher level network\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
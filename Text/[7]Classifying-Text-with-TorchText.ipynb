{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "120000lines [00:08, 14440.58lines/s]\n120000lines [00:15, 7844.34lines/s]\n7600lines [00:00, 8029.35lines/s]\n"
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "NGrams = 2\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=\"../assets/data\", ngrams=NGrams, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2,\n tensor([    572,     564,       2,    2326,   49106,     150,      88,       3,\n            1143,      14,      32,      15,      32,      16,  443749,       4,\n             572,     499,      17,      10,  741769,       7,  468770,       4,\n              52,    7019,    1050,     442,       2,   14341,     673,  141447,\n          326092,   55044,    7887,     411,    9870,  628642,      43,      44,\n             144,     145,  299709,  443750,   51274,     703,   14312,      23,\n         1111134,  741770,  411508,  468771,    3779,   86384,  135944,  371666,\n            4052]))"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, voc_size, emb_dim, n_class):\n",
    "        super(TextSentiment, self).__init__()\n",
    "        # TODO: Why sparse?\n",
    "        self.emb_sum = nn.EmbeddingBag(voc_size, emb_dim, sparse=True)\n",
    "        self.fc = nn.Linear(emb_dim, n_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        # Use `Tensor.data` to NOT track computation history. \n",
    "        self.emb_sum.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.emb_sum(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "VOC_SIZE = len(train_dataset.get_vocab())\n",
    "EMB_DIM = 32\n",
    "N_CLASS = len(train_dataset.get_labels())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextSentiment(VOC_SIZE, EMB_DIM, N_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    \"\"\"\n",
    "    Transform a batch from dataset to a concatenated tensor with offsets. \n",
    "    \"\"\"\n",
    "    labels = torch.tensor([entry[0] for entry in batch])\n",
    "    texts = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(t) for t in texts[:-1]]\n",
    "    offsets = torch.tensor(offsets).cumsum(dim=-1)\n",
    "    texts = torch.cat(texts)\n",
    "    return texts, offsets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 1 | time in 0.15 minutes, 9 seconds\n\tLoss: 0.0261 (train) | Acc: 84.7% (train)\n\tLoss: 0.0037 (valid) | Acc: 90.3% (valid)\nEpoch: 2 | time in 0.15 minutes, 9 seconds\n\tLoss: 0.0119 (train) | Acc: 93.7% (train)\n\tLoss: 0.0037 (valid) | Acc: 90.4% (valid)\nEpoch: 3 | time in 0.15 minutes, 9 seconds\n\tLoss: 0.0070 (train) | Acc: 96.3% (train)\n\tLoss: 0.0044 (valid) | Acc: 89.9% (valid)\nEpoch: 4 | time in 0.15 minutes, 9 seconds\n\tLoss: 0.0038 (train) | Acc: 98.1% (train)\n\tLoss: 0.0048 (valid) | Acc: 89.8% (valid)\nEpoch: 5 | time in 0.15 minutes, 9 seconds\n\tLoss: 0.0022 (train) | Acc: 99.0% (train)\n\tLoss: 0.0055 (valid) | Acc: 89.7% (valid)\n"
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = np.inf\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=4.0)\n",
    "# Switch `lr = lr * gamma` every `step_size` times that `scheduler.step()` is called. \n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_dataset, sub_valid_dataset = random_split(train_dataset, [train_len, len(train_dataset)-train_len])\n",
    "sub_train_loader = DataLoader(sub_train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "sub_valid_loader = DataLoader(sub_valid_dataset, batch_size=BATCH_SIZE*5, shuffle=False, collate_fn=generate_batch)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for i, (texts, offsets, labels) in enumerate(sub_train_loader):\n",
    "        texts, offsets, labels = texts.to(device), offsets.to(device), labels.to(device)\n",
    "        # Forward pass\n",
    "        outs = model(texts, offsets)\n",
    "        # Calculate loss\n",
    "        loss = loss_func(outs, labels)\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outs.argmax(dim=-1) == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(sub_train_dataset)\n",
    "    train_acc /= len(sub_train_dataset)\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluating\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    for i, (texts, offsets, labels) in enumerate(sub_valid_loader):\n",
    "        texts, offsets, labels = texts.to(device), offsets.to(device), labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outs = model(texts, offsets)\n",
    "            loss = loss_func(outs, labels)\n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += (outs.argmax(dim=-1) == labels).sum().item()\n",
    "\n",
    "    valid_loss /= len(sub_valid_dataset)\n",
    "    valid_acc /= len(sub_valid_dataset)\n",
    "    model.train()\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1} | time in {mins} minutes, {secs} seconds\")\n",
    "    print(f\"\\tLoss: {train_loss:.4f} (train) | Acc: {train_acc * 100:.1f}% (train)\")\n",
    "    print(f\"\\tLoss: {valid_loss:.4f} (valid) | Acc: {valid_acc * 100:.1f}% (valid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loss: 0.0046 (test) | Acc: 90.8% (test)\n"
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*5, shuffle=False, collate_fn=generate_batch)\n",
    "for i, (texts, offsets, labels) in enumerate(test_loader):\n",
    "    texts, offsets, labels = texts.to(device), offsets.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outs = model(texts, offsets)\n",
    "        loss = loss_func(outs, labels)\n",
    "        test_loss += loss.item()\n",
    "        test_acc += (outs.argmax(dim=-1) == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_dataset)\n",
    "test_acc /= len(test_dataset)\n",
    "model.train()\n",
    "\n",
    "print(f\"\\tLoss: {test_loss:.4f} (test) | Acc: {test_acc * 100:.1f}% (test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-2.8763,  6.8487, -2.4136, -1.6231]], device='cuda:0')\nSports\n"
    }
   ],
   "source": [
    "import re\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "\n",
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = train_dataset.get_vocab()\n",
    "text = torch.tensor([vocab[token] for token in ngrams_iterator(tokenizer(ex_text_str), NGrams)], device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outs = model(text, torch.tensor([0], device=device))\n",
    "\n",
    "print(outs)\n",
    "print(ag_news_label[outs.argmax().item() + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
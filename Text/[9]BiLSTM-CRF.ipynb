{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM-CRF \n",
    "[ADVANCED: MAKING DYNAMIC DECISIONS AND THE BI-LSTM CRF](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)  \n",
    "[Implementing a linear-chain Conditional Random Field (CRF) in PyTorch](https://towardsdatascience.com/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Chain Conditional Random Field (CRF) \n",
    "\n",
    "The source sequence is $x = \\{x_1, x_2, \\dots, x_T \\}$, and the target sequence is $y = \\{y_1, y_2, \\dots, y_T \\}$.  \n",
    "If we ignore the dependence between elements in $y$, we can model as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y|x) &= \\prod_{t=1}^T \\frac{\\exp \\left( U(x_t, y_t) \\right)}{\\sum_{y'_t} \\exp \\left( U(x_t, y'_t) \\right)} \\\\\n",
    "&= \\prod_{t=1}^T \\frac{\\exp \\left( U(x_t, y_t) \\right)}{Z(x_t)} \\\\\n",
    "&= \\frac{\\exp \\left( \\sum_{t=1}^T U(x_t, y_t) \\right)}{\\prod_{t=1}^T Z(x_t)} \\\\\n",
    "&= \\frac{\\exp \\left( \\sum_{t=1}^T U(x_t, y_t) \\right)}{Z(x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $U(x_t, y_t)$ is *emissions* or *unary scores*, $Z(x_t)$ is *partition function* (a normalization factor).  \n",
    "In a *linear-chain CRF*, we add *transition scores* $T(y_t, y_{t+1})$ to the above equation:  \n",
    "$$\n",
    "P(y|x) = \\frac{\\exp \\left( \\sum_{t=1}^T U(x_t, y_t) + \\sum_{t=0}^T T(y_t, y_{t+1}) \\right)}{Z(x)}\n",
    "$$\n",
    "where $y_0$ and $y_{T+1}$ are the starting and stopping tags, respectively; their values are fixed.  \n",
    "The partition function should sum over all possible combinations over the label set at each timestep: \n",
    "$$\n",
    "Z(x) = \\sum_{y'_1} \\sum_{y'_2} \\dots \\sum_{y'_T} \\exp \\left( \\sum_{t=1}^T U(x_t, y'_t) + \\sum_{t=0}^T T(y'_t, y'_{t+1}) \\right)\n",
    "$$\n",
    "\n",
    "The *negative log-likelihood loss (NLL-Loss)* is: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "L &= -\\log \\left( P(y|x) \\right) \\\\\n",
    "&= \\log \\left( Z(x) \\right) - \\left( \\sum_{t=1}^T U(x_t, y_t) + \\sum_{t=0}^T T(y_t, y_{t+1}) \\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Algorithm: Dynamic Programing for Computing the Partition Function\n",
    "\n",
    "The time complexity of computing $Z(x)$ would be $O(\\vert y \\vert^T)$... but we can use *dynamic programing* to reduce it.  \n",
    "Specifically, we define the state:\n",
    "$$\n",
    "\\alpha_s (y_s) = \\sum_{y'_1} \\sum_{y'_2} \\dots \\sum_{y'_{s-1}} \\exp \\left( \\sum_{t=1}^{s-1} U(x_t, y'_t) + \\sum_{t=0}^{s-2} T(y'_t, y'_{t+1}) + T(y'_{s-1}, y_s) \\right)\n",
    "$$\n",
    "where $\\alpha_s (y_s)$ may be regarded as the sum of scores reaching $y_s$. Note:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha_1(y_1) &= \\exp \\left( T(y_0, y_1) \\right) \\\\\n",
    "\\alpha_{T+1}(y_{T+1}) &= Z(x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "When computing $\\alpha_{s+1}(y_{s+1})$, we only require the information of $\\alpha_s(y'_s)$ for different $y'_s$, instead of the information before step $s$ (i.e., the paths reaching each $y'_s$). Hence, this is a dynamic programing problem. \n",
    "In the log-space, we have the *state transition equation*:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log( \\alpha_{s+1}(y_{s+1}) ) &= \\log \\left( \\sum_{y'_1} \\sum_{y'_2} \\dots \\sum_{y'_s} \\exp \\left( \\sum_{t=1}^s U(x_t, y'_t) + \\sum_{t=0}^{s-1} T(y'_t, y'_{t+1}) + T(y'_s, y_{s+1}) \\right) \\right) \\\\\n",
    "&= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_s, y'_s) + T(y'_s, y_{s+1}) \\right) \\sum_{y'_1} \\sum_{y'_2} \\dots \\sum_{y'_{s-1}} \\exp \\left( \\sum_{t=1}^{s-1} U(x_t, y'_t) + \\sum_{t=0}^{s-2} T(y'_t, y'_{t+1}) + T(y'_{s-1}, y'_s) \\right) \\right) \\\\\n",
    "&= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_s, y'_s) + T(y'_s, y_{s+1}) \\right) \\alpha_s(y'_s) \\right) \\\\\n",
    "&= \\log \\left( \\sum_{y'_s} \\exp \\left( U(x_s, y'_s) + T(y'_s, y_{s+1}) + \\log (\\alpha_s(y'_s)) \\right) \\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm: Finding the Best Sequence of Labels \n",
    "\n",
    "Similarly, we use dynamic programing to find the best sequence of labels (i.e., decoding).  \n",
    "Specifically, we define the state: \n",
    "$$\n",
    "\\beta_s (y_s) = \\max_{y'_1, y'_2, \\dots, y'_{s-1}} \\exp \\left( \\sum_{t=1}^{s-1} U(x_t, y'_t) + \\sum_{t=0}^{s-2} T(y'_t, y'_{t+1}) + T(y'_{s-1}, y_s) \\right)\n",
    "$$\n",
    "where $\\beta_s (y_s)$ may be regarded as the max score reaching $y_s$. Note: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_1 (y_1) &= \\exp \\left( T(y_0, y_1) \\right) \\\\\n",
    "\\beta_{T+1}(y_{T+1}) &= \\max_{y'_1, y'_2, \\dots, y'_T} \\exp \\left( \\sum_{t=1}^T U(x_t, y'_t) + \\sum_{t=0}^{T-1} T(y'_t, y'_{t+1}) + T(y'_T, y_{T+1}) \\right)\n",
    "\\end{aligned} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(['the',\n   'wall',\n   'street',\n   'journal',\n   'reported',\n   'today',\n   'that',\n   'apple',\n   'corporation',\n   'made',\n   'money'],\n  ['B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O']),\n (['georgia', 'tech', 'is', 'a', 'university', 'in', 'georgia'],\n  ['B', 'I', 'O', 'O', 'O', 'O', 'B'])]"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "training_data = [(\"the wall street journal reported today that apple corporation made money\".split(),\n",
    "                  \"B I I I O O O B I O O\".split()), \n",
    "                 (\"georgia tech is a university in georgia\".split(),\n",
    "                  \"B I O O O O B\".split())]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 5\n",
    "HID_DIM = 4\n",
    "VOC_DIM = len(word_to_ix)\n",
    "TAG_DIM = len(tag_to_ix)\n",
    "\n",
    "emb = nn.Embedding(VOC_DIM, EMB_DIM)\n",
    "rnn = nn.LSTM(EMB_DIM, HID_DIM//2, num_layers=1, bidirectional=True)\n",
    "hid2tag = nn.Linear(HID_DIM, TAG_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0],\n        [ 1],\n        [ 2],\n        [ 3],\n        [ 4],\n        [ 5],\n        [ 6],\n        [ 7],\n        [ 8],\n        [ 9],\n        [10]])\ntensor([[0],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [0],\n        [1],\n        [2],\n        [2]])\n"
    }
   ],
   "source": [
    "ex_idx = 0\n",
    "# sent/tags: (step, batch=1)\n",
    "sent = torch.tensor([word_to_ix[w] for w in training_data[ex_idx][0]], dtype=torch.long).unsqueeze(1)\n",
    "tags = torch.tensor([tag_to_ix[t] for t in training_data[ex_idx][1]], dtype=torch.long).unsqueeze(1)\n",
    "print(sent)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.1153,  0.1015, -0.2363, -0.1656, -0.3136]],\n\n        [[-0.1415,  0.1355, -0.2845, -0.2094, -0.2525]],\n\n        [[-0.0358,  0.0669, -0.1656, -0.1879, -0.2165]],\n\n        [[ 0.0696, -0.0096,  0.0631,  0.0470, -0.1283]],\n\n        [[-0.0778,  0.0396, -0.0088,  0.0143, -0.1432]],\n\n        [[ 0.0399,  0.1006, -0.1594, -0.1141, -0.3252]],\n\n        [[-0.1022,  0.0704, -0.1275, -0.0914, -0.1637]],\n\n        [[-0.0796,  0.0148, -0.0523, -0.1221, -0.1086]],\n\n        [[-0.1715,  0.0859, -0.0614,  0.0529, -0.1484]],\n\n        [[-0.2149,  0.1116, -0.0498,  0.1023, -0.1867]],\n\n        [[-0.0229, -0.0485,  0.1840,  0.1056, -0.0520]]],\n       grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "embbed = emb(sent)\n",
    "rnn_outs, _ = rnn(embbed)\n",
    "\n",
    "# feats: (step, batch=1, tag_dim)\n",
    "feats = hid2tag(rnn_outs)\n",
    "print(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-1.1290e+00, -1.2998e+00, -1.6684e+00, -8.6616e-01, -1.0000e+04],\n        [-4.6060e-01,  4.6536e-01, -1.5174e+00, -1.5891e+00, -1.0000e+04],\n        [-1.2220e+00, -2.4080e-01, -8.2866e-01,  7.9692e-01, -1.0000e+04],\n        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04],\n        [-5.8353e-01, -6.6203e-01,  1.1687e-01,  1.1368e+00, -1.0000e+04]],\n       requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# transitions[i, j] is the score of transitioning *to* i *from* j.\n",
    "transitions = nn.Parameter(torch.randn(TAG_DIM, TAG_DIM))\n",
    "transitions.data[tag_to_ix[START_TAG], :] = -1e4\n",
    "transitions.data[:, tag_to_ix[STOP_TAG]] = -1e4\n",
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-5.2230], grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# The numerator: score\n",
    "# The original implementation\n",
    "def _score_sentence(feats, tags):\n",
    "    score = torch.zeros(1)\n",
    "    tags = torch.cat([torch.tensor([tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "    for i, feat in enumerate(feats):\n",
    "        score = score + transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "    score = score +transitions[tag_to_ix[STOP_TAG], tags[-1]]\n",
    "    return score\n",
    "\n",
    "_score_sentence(feats.squeeze(1), tags.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([11, 1])\ntorch.Size([12, 1])\ntensor([-5.2230], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "# Vectorized implementation\n",
    "feat_scores = feats.gather(dim=-1, index=tags.unsqueeze(-1)).squeeze(-1)\n",
    "print(feat_scores.size())\n",
    "\n",
    "from_tags = torch.cat([torch.full((1, tags.size(1)), fill_value=tag_to_ix[START_TAG], dtype=torch.long), tags], dim=0)\n",
    "to_tags = torch.cat([tags, torch.full((1, tags.size(1)), fill_value=tag_to_ix[STOP_TAG], dtype=torch.long)], dim=0)\n",
    "trans_scores = transitions[to_tags, from_tags]\n",
    "print(trans_scores.size())\n",
    "\n",
    "print(feat_scores.sum(dim=0) + trans_scores.sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(6.5684, grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# The denominator: partition function\n",
    "# The original implementation\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, torch.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def _forward_alg(feats):\n",
    "    # Do the forward algorithm to compute the partition function\n",
    "    init_alphas = torch.full((1, TAG_DIM), -10000.)\n",
    "    # START_TAG has all of the score.\n",
    "    init_alphas[0][tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "    # Wrap in a variable so that we will get automatic backprop\n",
    "    forward_var = init_alphas\n",
    "\n",
    "    # Iterate through the sentence\n",
    "    for feat in feats:\n",
    "        alphas_t = []  # The forward tensors at this timestep\n",
    "        for next_tag in range(TAG_DIM):\n",
    "            # broadcast the emission score: it is the same regardless of\n",
    "            # the previous tag\n",
    "            emit_score = feat[next_tag].view(1, -1).expand(1, TAG_DIM)\n",
    "            # the ith entry of trans_score is the score of transitioning to\n",
    "            # next_tag from i\n",
    "            trans_score = transitions[next_tag].view(1, -1)\n",
    "            # The ith entry of next_tag_var is the value for the\n",
    "            # edge (i -> next_tag) before we do log-sum-exp\n",
    "            next_tag_var = forward_var + trans_score + emit_score\n",
    "            # The forward variable for this tag is log-sum-exp of all the\n",
    "            # scores.\n",
    "            alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "        forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "    terminal_var = forward_var + transitions[tag_to_ix[STOP_TAG]]\n",
    "    alpha = log_sum_exp(terminal_var)\n",
    "    return alpha\n",
    "\n",
    "_forward_alg(feats.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-10000., -10000., -10000.,      0., -10000.]])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([6.5684], grad_fn=<LogsumexpBackward>)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Vectorized implementation\n",
    "alphas = torch.full((feats.size(1), TAG_DIM), fill_value=-1e4)\n",
    "alphas[:, tag_to_ix[START_TAG]] = 0\n",
    "print(alphas)\n",
    "\n",
    "for t in range(feats.size(0)):\n",
    "    # alphas: (batch=1, tag_dim)\n",
    "    # feats[t]: (batch=1, tag_dim)\n",
    "    alphas = torch.logsumexp(alphas.unsqueeze(1) + feats[t].unsqueeze(2) + transitions, dim=-1)\n",
    "\n",
    "alphas = alphas + transitions[tag_to_ix[STOP_TAG]]\n",
    "torch.logsumexp(alphas, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
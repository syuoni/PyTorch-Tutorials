{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Install `spacy` and download the raw data for the English and German Spacy tokenizers.  \n",
    "NOTE: Administrator permission required. \n",
    "```bash\n",
    "$ pip install spacy\n",
    "$ python -m spacy download en\n",
    "$ python -m spacy download de\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG), \n",
    "                                                    root='data/')\n",
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['<unk>', '<pad>', '<sos>', '<eos>', '.']\n[0, 1, 2, 3, 4]\n['<unk>', '<pad>', '<sos>', '<eos>', '.', 'ein', 'einem', 'in']\n['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the']\n"
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# A dict mapping word to index\n",
    "print(list(SRC.vocab.stoi.keys())[:5])\n",
    "print(list(SRC.vocab.stoi.values())[:5])\n",
    "# A list mapping index to word\n",
    "print(SRC.vocab.itos[:8])\n",
    "print(TRG.vocab.itos[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BucketIterator`: Iterate over the Datasets of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[<torchtext.data.example.Example object at 0x000002788AF3FD88>, <torchtext.data.example.Example object at 0x000002788AF3FF48>, <torchtext.data.example.Example object at 0x000002788B3F5D08>, <torchtext.data.example.Example object at 0x000002788AF35188>]\n"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 4\n",
    "# The default collate function checks if the batch contains tensors, numpy-arrays, ...\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([19, 4])\ntorch.Size([20, 4])\n"
    }
   ],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# `BucketIterator` automatically transforms word sequences to tensors with paddings. \n",
    "train_iterator = BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.src.size())\n",
    "    print(batch.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[   2,    2,    2,    2],\n        [   5,    5,    8,    5],\n        [  49,  227,   36,   13],\n        [  31, 3787,   22,   11],\n        [  21,   26,   80, 3077],\n        [   6,    9,  280,   10],\n        [ 107,  217,   68,   14],\n        [   9,   19,   20,    0],\n        [  37, 1192,   88,   12],\n        [   5,   12,   27,   14],\n        [2387,   14,   14,  546],\n        [   7,  923,    0,   34],\n        [  15,    7,    5,    4],\n        [  81,    6,    0,    3],\n        [  10, 1288,    4,    1],\n        [1160,    9,    3,    1],\n        [   4,  119,    1,    1],\n        [   3,    4,    1,    1],\n        [   1,    3,    1,    1]])"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# `BucketIterator` automatically adds <sos>, <eos>, <pad>, <unk> to Tensors. \n",
    "batch.src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[   2,    2,    2,    2],\n        [   4,    4,    4,    4],\n        [  55,  262,   38,    9],\n        [  15,  667,   12,    8],\n        [  32,   34,   24,    4],\n        [  20,   10,   19,  101],\n        [   4,  230,  389,   39],\n        [  94,    4,    4,    6],\n        [  15,  350, 1661,    4],\n        [ 140,    8,  134, 1135],\n        [   4,    4,    6,  197],\n        [1293, 1000,   43,   11],\n        [   6,    6,   12,    0],\n        [  44,    4,    4,  630],\n        [ 181, 1019, 1616,    5],\n        [  11,    5,  443,    3],\n        [  10,    3,   57,    1],\n        [ 883,    1,    5,    1],\n        [   5,    1,    3,    1],\n        [   3,    1,    1,    1]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# `BucketIterator` automatically adds <sos>, <eos>, <pad>, <unk> to Tensors. \n",
    "batch.trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIM = len(SRC.vocab)\n",
    "OUT_DIM = len(TRG.vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,  in_dim: int,  emb_dim: int, \n",
    "                 enc_hid_dim: int,  dec_hid_dim: int,  dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, enc_ins: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        # enc_ins: (batch_size, src_len)\n",
    "        embedded = self.dropout(self.emb(enc_ins))\n",
    "        # outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # hidden: (2, batch_size, enc_hid_dim)\n",
    "        outs, hidden = self.rnn(embedded)\n",
    "        # Concatenate the last hidden states in two directions. \n",
    "        # hidden: (batch_size, enc_hid_dim*2)\n",
    "        hidden = torch.cat([hidden[0], hidden[1]], dim=-1)\n",
    "        # hidden: (batch_size, dec_hid_dim)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 19, 128])\ntorch.Size([4, 64])\n"
    }
   ],
   "source": [
    "encoder = Encoder(IN_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "# No initial hidden state provided, default to be zeros. \n",
    "enc_outs, dec_hidden = encoder(batch.src.T)\n",
    "print(enc_outs.size())\n",
    "print(dec_hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_in = enc_hid_dim*2 + dec_hid_dim\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self, dec_hidden: torch.Tensor, enc_outs: torch.Tensor) -> torch.Tensor:\n",
    "        src_len = enc_outs.size(1)\n",
    "        # repeated_dec_hidden: (batch_size, src_len, dec_hid_dim)\n",
    "        repeated_dec_hidden = dec_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # enc_outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # energy: (batch_size, src_len, attn_dim)\n",
    "        energy = torch.tanh(self.attn(torch.cat([repeated_dec_hidden, enc_outs], dim=-1)))\n",
    "        # attn: (batch_size, src_len)\n",
    "        attn = energy.sum(dim=-1)\n",
    "        return F.softmax(attn, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 19])\n"
    }
   ],
   "source": [
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "attn = attention(dec_hidden, enc_outs)\n",
    "print(attn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dim: int, emb_dim: int, enc_hid_dim: int,  dec_hid_dim: int,  \n",
    "                 dropout: float, attention: Attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.emb = nn.Embedding(out_dim, emb_dim)\n",
    "        # Single-directional\n",
    "        self.rnn = nn.GRU(enc_hid_dim*2 + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim*2 + dec_hid_dim + emb_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, dec_ins: torch.Tensor, dec_hidden: torch.Tensor, \n",
    "                enc_outs: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        One-step forward. \n",
    "        \"\"\"\n",
    "        # dec_ins: (batch_size, 1)\n",
    "        # embedded: (batch_size, 1, dec_emb_dim)\n",
    "        embedded = self.dropout(self.emb(dec_ins))\n",
    "        \n",
    "        # attn: (batch_size, src_len)\n",
    "        attn = self.attention(dec_hidden, enc_outs)\n",
    "        # enc_outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # wtd_enc_rep: (batch_size, 1, enc_hid_dim*2)\n",
    "        wtd_enc_rep = attn.unsqueeze(1).bmm(enc_outs)\n",
    "        # rnn_ins: (batch_size, 1, enc_hid_dim*2 + dec_emb_dim)\n",
    "        rnn_ins = torch.cat([embedded, wtd_enc_rep], dim=-1)\n",
    "        # outs: (batch_size, 1, dec_hid_dim)\n",
    "        outs, dec_hidden = self.rnn(rnn_ins, dec_hidden.unsqueeze(0))\n",
    "        # outs: (batch_size, 1, trg_voc_size)\n",
    "        outs = self.fc(torch.cat([outs, wtd_enc_rep, embedded], dim=-1))\n",
    "        return outs, dec_hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 64])\ntorch.Size([4, 64])\ntorch.Size([4, 1, 5893])\n"
    }
   ],
   "source": [
    "decoder = Decoder(OUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention)\n",
    "\n",
    "print(dec_hidden.size())\n",
    "\n",
    "dec_ins_0 = batch.trg[0].unsqueeze(1)\n",
    "dec_outs_0, dec_hidden = decoder(dec_ins_0, dec_hidden, enc_outs)\n",
    "print(dec_hidden.size())\n",
    "print(dec_outs_0.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, enc_ins: torch.Tensor, dec_ins: torch.Tensor, \n",
    "                teacher_forcing_ratio: float=0.5) -> torch.Tensor:\n",
    "        # enc_ins: (batch_size, src_len)\n",
    "        # dec_ins: (batch_size, trg_len)\n",
    "        # No initial hidden state provided, default to be zeros. \n",
    "        enc_outs, dec_hidden = self.encoder(enc_ins)\n",
    "\n",
    "        dec_outs = []\n",
    "        # The first input to the decoder is the <sos> token. \n",
    "        # dec_ins_t: (batch_size, 1)\n",
    "        dec_ins_t = dec_ins[:, 0].unsqueeze(1)\n",
    "        for t in range(1, dec_ins.size(1)):\n",
    "            # dec_outs_t: (batch_size, 1, trg_voc_size)\n",
    "            dec_outs_t, dec_hidden = decoder(dec_ins_t, dec_hidden, enc_outs)\n",
    "            top1 = dec_outs_t.max(dim=-1)[1]\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                dec_ins_t = dec_ins[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                dec_ins_t = top1\n",
    "            dec_outs.append(dec_outs_t)\n",
    "        return torch.cat(dec_outs, dim=1)\n",
    "\n",
    "    def translate(self, enc_ins: torch.Tensor, \n",
    "                  sos: int, eos: int, max_len: int=20) -> torch.Tensor:\n",
    "        enc_outs, dec_hidden = self.encoder(enc_ins)\n",
    "        top1s = []\n",
    "\n",
    "        # The first input to the decoder is the <sos> token. \n",
    "        # dec_ins_t: (batch_size=1, 1)\n",
    "        dec_ins_t = torch.ones(enc_ins.size(0), 1, dtype=torch.long) * sos\n",
    "        for t in range(max_len):\n",
    "            # dec_outs_t: (batch_size=1, 1, trg_voc_size)\n",
    "            dec_outs_t, dec_hidden = decoder(dec_ins_t, dec_hidden, enc_outs)\n",
    "            top1 = dec_outs_t.max(dim=-1)[1]\n",
    "            dec_ins_t = top1\n",
    "            top1s.append(top1)\n",
    "            if dec_ins_t.item() == eos:\n",
    "                break\n",
    "        return torch.cat(top1s, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([4, 19, 5893])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "dec_outs = model(batch.src.T, batch.trg.T)\n",
    "dec_outs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]],\n\n        [[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]]])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Check if data are mixed across different samples in a batch.\n",
    "model.eval()\n",
    "dec_outs_012 = model(batch.src.T[:3], batch.trg.T[:3], 1)\n",
    "dec_outs_123 = model(batch.src.T[1:], batch.trg.T[1:], 1)\n",
    "dec_outs_012[1:] == dec_outs_123[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[2835, 4841,  545, 2087, 3754, 1951,  311, 5354, 1201, 2783, 4106, 4593,\n         1528, 5818, 2057, 4030, 3182, 3798, 3611, 5134]])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "TRG_SOS_IDX = TRG.vocab.stoi['<sos>']\n",
    "TRG_EOS_IDX = TRG.vocab.stoi['<eos>']\n",
    "\n",
    "model.translate(batch.src.T[:1], TRG_SOS_IDX, TRG_EOS_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 1,856,685 trainable parameters\n"
    }
   ],
   "source": [
    "encoder = Encoder(IN_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "decoder = Decoder(OUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes of Padding\n",
    "What should be noticed when using a mini-batch with sequences of different lengths? -> Padding\n",
    "* For both input and output sequences, if the RNNs are bidirectional, the initial hidden states from the backward direction is incorrect, as the hidden states have passed some padding positions. \n",
    "    * Use `pack_padded_sequence` and `pad_packed_sequence`.\n",
    "* For input sequence, some attention weights may be applied to the padding positions. \n",
    "* For input sequence, the pooling operation along the sequence may include the padding positions. \n",
    "* For output sequence, the loss calculation may include the padding positions. \n",
    "    * Use `ignore_index` parameter when creating the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Padding Index: 1\n"
    }
   ],
   "source": [
    "# NOTE: When scoring the model performance, tell the `nn.CrossEntropyLoss` function to ignore the indices where the target is padding. \n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "print(\"Padding Index: %d\" % PAD_IDX)\n",
    "\n",
    "# ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient.\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=PAD_IDX, reduction='mean')\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "#optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([27, 128])\ntorch.Size([26, 128])\n"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# The iterators work like `DataLoader`.\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE*2, BATCH_SIZE*2), \n",
    "    device=device)\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.src.size())\n",
    "    print(batch.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Time: 3m 27s\n\tTrain Loss: 5.687 | Train PPL: 294.946\n\t Val. Loss: 5.050 |  Val. PPL: 156.088\nEpoch: 02 | Time: 3m 21s\n\tTrain Loss: 5.023 | Train PPL: 151.845\n\t Val. Loss: 4.754 |  Val. PPL: 116.033\nEpoch: 03 | Time: 3m 24s\n\tTrain Loss: 4.747 | Train PPL: 115.195\n\t Val. Loss: 4.497 |  Val. PPL:  89.757\nEpoch: 04 | Time: 3m 19s\n\tTrain Loss: 4.561 | Train PPL:  95.724\n\t Val. Loss: 4.433 |  Val. PPL:  84.177\nEpoch: 05 | Time: 3m 12s\n\tTrain Loss: 4.414 | Train PPL:  82.581\n\t Val. Loss: 4.193 |  Val. PPL:  66.214\nEpoch: 06 | Time: 3m 10s\n\tTrain Loss: 4.328 | Train PPL:  75.782\n\t Val. Loss: 4.065 |  Val. PPL:  58.272\nEpoch: 07 | Time: 3m 11s\n\tTrain Loss: 4.234 | Train PPL:  68.960\n\t Val. Loss: 4.198 |  Val. PPL:  66.535\nEpoch: 08 | Time: 3m 13s\n\tTrain Loss: 4.138 | Train PPL:  62.683\n\t Val. Loss: 3.940 |  Val. PPL:  51.443\nEpoch: 09 | Time: 3m 12s\n\tTrain Loss: 4.032 | Train PPL:  56.349\n\t Val. Loss: 3.868 |  Val. PPL:  47.836\nEpoch: 10 | Time: 3m 12s\n\tTrain Loss: 3.942 | Train PPL:  51.527\n\t Val. Loss: 3.817 |  Val. PPL:  45.457\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    # Training\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        # Forward pass\n",
    "        dec_outs = model(batch.src.T, batch.trg.T)\n",
    "        dec_outs_flattened = dec_outs.view(-1, dec_outs.size(-1))\n",
    "        trg_flattened = batch.trg.T[:, 1:].flatten()\n",
    "        # Calculate loss\n",
    "        loss = loss_func(dec_outs_flattened, trg_flattened)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    train_loss = epoch_loss / len(train_iterator)\n",
    "\n",
    "    # Validating\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(valid_iterator):\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            dec_outs = model(batch.src.T, batch.trg.T)\n",
    "            dec_outs_flattened = dec_outs.view(-1, dec_outs.size(-1))\n",
    "            trg_flattened = batch.trg.T[:, 1:].flatten()\n",
    "            # Calculate loss\n",
    "            loss = loss_func(dec_outs_flattened, trg_flattened)\n",
    "            epoch_loss += loss.item()\n",
    "    valid_loss = epoch_loss / len(valid_iterator)\n",
    "    model.train()\n",
    "\n",
    "    epoch_secs = time.time() - start_time\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "| Test Loss: 3.775 | Test PPL:  43.593 |\n"
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "epoch_loss = 0\n",
    "for i, batch in enumerate(test_iterator):\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        dec_outs = model(batch.src.T, batch.trg.T)\n",
    "        dec_outs_flattened = dec_outs.view(-1, dec_outs.size(-1))\n",
    "        trg_flattened = batch.trg.T[:, 1:].flatten()\n",
    "        # Calculate loss\n",
    "        loss = loss_func(dec_outs_flattened, trg_flattened)\n",
    "        epoch_loss += loss.item()\n",
    "test_loss = epoch_loss / len(test_iterator)\n",
    "model.train()\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "De: geschäftiges asiatisches einkaufszentrum mit papierlaternen und einkäufern .\nEn (Trans): a <unk> <unk> <unk> <unk> in a <unk> in a <unk> . <eos>\nEn (Real): busy asian mall with paper lanterns and shoppers .\n==================================================\nDe: sechs männer sitzen auf einem acker mit holzkisten .\nEn (Trans): three men are sitting on a a in a a in a . . <eos>\nEn (Real): six men sit in a field of crops containing wooden crates .\n==================================================\nDe: der weiße hund läuft im flachen wasser .\nEn (Trans): a dog dog dog in a a a a a . <eos>\nEn (Real): the white dog is running in the shallow water .\n==================================================\nDe: eine gruppe stammesangehöriger füllt in der wüste wasserbehälter .\nEn (Trans): a group of people are in a a in a a . . <eos>\nEn (Real): a tribal group filling water jugs in the desert .\n==================================================\nDe: vier personen spielen fußball auf einem strand .\nEn (Trans): three people are in a a in a a in a . . <eos>\nEn (Real): four people are playing soccer on a beach .\n==================================================\nDe: der mann im weißen t-shirt macht sich daran , auf einen fels zu klettern .\nEn (Trans): a man in a black shirt is a a a a a . <eos>\nEn (Real): the man in the white t - shirt is starting to climb a rock .\n==================================================\nDe: ein mann ohne hemd geht zu einem gelben kajak .\nEn (Trans): a man in a black shirt is a a a a a . <eos>\nEn (Real): a shirtless man walks toward a yellow kayak .\n==================================================\nDe: ein glatzköpfiger mann geht auf einem gehsteig und telefoniert mit seinem handy .\nEn (Trans): a man in a man is sitting on a man in a man in a man . <eos>\nEn (Real): a bald man walking down a city sidewalk while talking on his cellphone .\n==================================================\nDe: ein junge in einem orangefarbenen shirt schüttet legosteine aus einer tasche .\nEn (Trans): a young boy in a blue shirt is a a a a a a a a . <eos>\nEn (Real): a boy in an orange shirt is pouring legos from a bag .\n==================================================\nDe: eine frau sieht zu , während ein mann mit verschränkten armen spricht .\nEn (Trans): a woman is a a a a a a a a a a a a a . <eos>\nEn (Real): a woman looks on as a man with folded arms is talking .\n==================================================\n"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "SRC_SOS_IDX = SRC.vocab.stoi['<sos>']\n",
    "SRC_EOS_IDX = SRC.vocab.stoi['<eos>']\n",
    "SRC_UNK_IDX = SRC.vocab.stoi['<unk>']\n",
    "\n",
    "for i in range(10):\n",
    "    IDX = np.random.randint(0, len(test_data))\n",
    "    print(\"De:\", \" \".join(test_data[IDX].src))\n",
    "\n",
    "    src = [SRC_SOS_IDX] + [SRC.vocab.stoi.get(w, SRC_UNK_IDX) for w in test_data[IDX].src] + [SRC_EOS_IDX]\n",
    "    src = torch.tensor(src, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    trans = model.translate(src, TRG_SOS_IDX, TRG_EOS_IDX).squeeze(0)\n",
    "    trans = \" \".join([TRG.vocab.itos[i.item()] for i in trans])\n",
    "    print(\"En (Trans):\", trans)\n",
    "    print(\"En (Real):\", \" \".join(test_data[IDX].trg))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Install `spacy` and download the raw data for the English and German Spacy tokenizers.  \n",
    "NOTE: Administrator permission required. \n",
    "```bash\n",
    "$ pip install spacy\n",
    "$ python -m spacy download en\n",
    "$ python -m spacy download de\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG), \n",
    "                                                    root='data/')\n",
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['<unk>', '<pad>', '<sos>', '<eos>', '.']\n[0, 1, 2, 3, 4]\n['<unk>', '<pad>', '<sos>', '<eos>', '.', 'ein', 'einem', 'in']\n['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the']\n"
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# A dict mapping word to index\n",
    "print(list(SRC.vocab.stoi.keys())[:5])\n",
    "print(list(SRC.vocab.stoi.values())[:5])\n",
    "# A list mapping index to word\n",
    "print(SRC.vocab.itos[:8])\n",
    "print(TRG.vocab.itos[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BucketIterator`: Iterate over the Datasets of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[<torchtext.data.example.Example object at 0x000002AE89F12B08>, <torchtext.data.example.Example object at 0x000002AE89F12C88>, <torchtext.data.example.Example object at 0x000002AE88EF56C8>, <torchtext.data.example.Example object at 0x000002AE88EF5708>]\n"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 4\n",
    "# The default collate function checks if the batch contains tensors, numpy-arrays, ...\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([22, 4])\ntorch.Size([26, 4])\n"
    }
   ],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# The `BucketIterator` automatically transforms word sequences to tensors with paddings. \n",
    "train_iterator = BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.src.size())\n",
    "    print(batch.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIM = len(SRC.vocab)\n",
    "OUT_DIM = len(TRG.vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,  in_dim: int,  emb_dim: int, \n",
    "                 enc_hid_dim: int,  dec_hid_dim: int,  dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, enc_ins: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        # enc_ins: (batch_size, src_len)\n",
    "        embedded = self.dropout(self.emb(enc_ins))\n",
    "        # outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # hidden: (2, batch_size, enc_hid_dim)\n",
    "        outs, hidden = self.rnn(embedded)\n",
    "        # Concatenate the last hidden states in two directions. \n",
    "        # hidden: (batch_size, enc_hid_dim*2)\n",
    "        hidden = torch.cat([hidden[0], hidden[1]], dim=-1)\n",
    "        # hidden: (batch_size, dec_hid_dim)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 22, 128])\ntorch.Size([4, 64])\n"
    }
   ],
   "source": [
    "encoder = Encoder(IN_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "# No initial hidden state provided, default to be zeros. \n",
    "enc_outs, dec_hidden = encoder(batch.src.T)\n",
    "print(enc_outs.size())\n",
    "print(dec_hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_in = enc_hid_dim*2 + dec_hid_dim\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self, dec_hidden: torch.Tensor, enc_outs: torch.Tensor) -> torch.Tensor:\n",
    "        src_len = enc_outs.size(1)\n",
    "        # repeated_dec_hidden: (batch_size, src_len, dec_hid_dim)\n",
    "        repeated_dec_hidden = dec_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # enc_outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # energy: (batch_size, src_len, enc_hid_dim*2 + dec_hid_dim)\n",
    "        energy = torch.tanh(torch.cat([repeated_dec_hidden, enc_outs], dim=-1))\n",
    "        # attn: (batch_size, src_len)\n",
    "        attn = energy.sum(dim=-1)\n",
    "        return F.softmax(attn, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 22])\n"
    }
   ],
   "source": [
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "attn = attention(dec_hidden, enc_outs)\n",
    "print(attn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dim: int, emb_dim: int, enc_hid_dim: int,  dec_hid_dim: int,  \n",
    "                 dropout: float, attention: Attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.emb = nn.Embedding(out_dim, emb_dim)\n",
    "        # Single-directional\n",
    "        self.rnn = nn.GRU(enc_hid_dim*2 + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim*2 + dec_hid_dim + emb_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, dec_ins: torch.Tensor, dec_hidden: torch.Tensor, \n",
    "                enc_outs: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        One-step forward. \n",
    "        \"\"\"\n",
    "        # dec_ins: (batch_size, 1)\n",
    "        # embedded: (batch_size, 1, dec_emb_dim)\n",
    "        embedded = self.dropout(self.emb(dec_ins))\n",
    "        \n",
    "        # attn: (batch_size, src_len)\n",
    "        attn = self.attention(dec_hidden, enc_outs)\n",
    "        # enc_outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # wtd_enc_rep: (batch_size, 1, enc_hid_dim*2)\n",
    "        wtd_enc_rep = attn.unsqueeze(1).bmm(enc_outs)\n",
    "        # rnn_ins: (batch_size, 1, enc_hid_dim*2 + dec_emb_dim)\n",
    "        rnn_ins = torch.cat([embedded, wtd_enc_rep], dim=-1)\n",
    "        # outs: (batch_size, 1, dec_hid_dim)\n",
    "        outs, dec_hidden = self.rnn(rnn_ins, dec_hidden.unsqueeze(0))\n",
    "        # outs: (batch_size, 1, trg_voc_size)\n",
    "        outs = self.fc(torch.cat([outs, wtd_enc_rep, embedded], dim=-1))\n",
    "        return outs, dec_hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 64])\ntorch.Size([4, 64])\ntorch.Size([4, 1, 5893])\n"
    }
   ],
   "source": [
    "decoder = Decoder(OUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention)\n",
    "\n",
    "print(dec_hidden.size())\n",
    "\n",
    "dec_ins_0 = batch.trg[0].unsqueeze(1)\n",
    "dec_outs_0, dec_hidden = decoder(dec_ins_0, dec_hidden, enc_outs)\n",
    "print(dec_hidden.size())\n",
    "print(dec_outs_0.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, enc_ins: torch.Tensor, dec_ins: torch.Tensor, \n",
    "                teacher_forcing_ratio: float=0.5) -> torch.Tensor:\n",
    "        # enc_ins: (batch_size, src_len)\n",
    "        # dec_ins: (batch_size, trg_len)\n",
    "        # No initial hidden state provided, default to be zeros. \n",
    "        enc_outs, dec_hidden = self.encoder(enc_ins)\n",
    "\n",
    "        dec_outs = []\n",
    "        # The first input to the decoder is the <sos> token. \n",
    "        # dec_ins_t: (batch_size, 1)\n",
    "        dec_ins_t = dec_ins[:, 0].unsqueeze(1)\n",
    "        for t in range(1, dec_ins.size(1)):\n",
    "            # dec_outs_t: (batch_size, 1, trg_voc_size)\n",
    "            dec_outs_t, dec_hidden = decoder(dec_ins_t, dec_hidden, enc_outs)\n",
    "            top1 = dec_outs_t.max(dim=-1)[1]\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                dec_ins_t = dec_ins[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                dec_ins_t = top1\n",
    "            dec_outs.append(dec_outs_t)\n",
    "        return torch.cat(dec_outs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([4, 25, 5893])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "dec_outs = model(batch.src.T, batch.trg.T, 0.5)\n",
    "dec_outs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 1,856,685 trainable parameters\n"
    }
   ],
   "source": [
    "encoder = Encoder(IN_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "decoder = Decoder(OUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: when scoring the performance of a language translation model in particular, we have to tell the `nn.CrossEntropyLoss` function to ignore the indices where the target is simply padding. \n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([22, 4])\ntorch.Size([26, 4])\n"
    }
   ],
   "source": [
    "# The iterators work like `DataLoader`.\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE*5, BATCH_SIZE*5), \n",
    "    device=device)\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.src.size())\n",
    "    print(batch.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
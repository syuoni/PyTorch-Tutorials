{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Install `spacy` and download the raw data for the English and German Spacy tokenizers.  \n",
    "NOTE: Administrator permission required. \n",
    "```bash\n",
    "$ pip install spacy\n",
    "$ python -m spacy download en\n",
    "$ python -m spacy download de\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG), \n",
    "                                                    root='data/')\n",
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['<unk>', '<pad>', '<sos>', '<eos>', '.']\n[0, 1, 2, 3, 4]\n['<unk>', '<pad>', '<sos>', '<eos>', '.', 'ein', 'einem', 'in']\n['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the']\n"
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# A dict mapping word to index\n",
    "print(list(SRC.vocab.stoi.keys())[:5])\n",
    "print(list(SRC.vocab.stoi.values())[:5])\n",
    "# A list mapping index to word\n",
    "print(SRC.vocab.itos[:8])\n",
    "print(TRG.vocab.itos[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BucketIterator`: Iterate over the Datasets of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[<torchtext.data.example.Example object at 0x7fd7eac40610>, <torchtext.data.example.Example object at 0x7fd7eac406d0>, <torchtext.data.example.Example object at 0x7fd7eac40810>, <torchtext.data.example.Example object at 0x7fd7f27c6d50>]\n"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 4\n",
    "# The default collate function checks if the batch contains tensors, numpy-arrays, ...\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([18, 4])\ntorch.Size([18, 4])\n"
    }
   ],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# `BucketIterator` automatically transforms word sequences to tensors with paddings. \n",
    "train_iterator = BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.src.size())\n",
    "    print(batch.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[   2,    2,    2,    2],\n        [   5,    5,   18,    8],\n        [  13,   13,   45, 3294],\n        [   7,   29,  839,   31],\n        [   6,  326,  498,  133],\n        [  51,   12,    4,   10],\n        [  79,   15,    3, 3135],\n        [ 212,   34,    1,   21],\n        [  27,   10,    1,   75],\n        [   6,  339,    1,    4],\n        [  51,   11,    1,    3],\n        [ 320,   77,    1,    1],\n        [  10,  277,    1,    1],\n        [   6,    4,    1,    1],\n        [  78,    3,    1,    1],\n        [ 104,    1,    1,    1],\n        [   4,    1,    1,    1],\n        [   3,    1,    1,    1]], device='cuda:0')"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# `BucketIterator` automatically adds <sos>, <eos>, <pad>, <unk> to Tensors. \n",
    "batch.src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[   2,    2,    2,    2],\n        [   9,    4,   16,    4],\n        [   6,    9,  666,   38],\n        [  25,   10,   17,   12],\n        [  23,   36, 2008,   63],\n        [ 436,    6,  167,  150],\n        [   6,    7,   72,  129],\n        [  43,  168,    5,   11],\n        [  12,   12,    3,  245],\n        [  25,    4,    1,    8],\n        [ 268,  142,    1,  553],\n        [  11,   28,    1,   11],\n        [  59,  119,    1, 1626],\n        [  77,    8,    1,    5],\n        [   5,   27,    1,    3],\n        [   3,  286,    1,    1],\n        [   1,    5,    1,    1],\n        [   1,    3,    1,    1]], device='cuda:0')"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# `BucketIterator` automatically adds <sos>, <eos>, <pad>, <unk> to Tensors. \n",
    "batch.trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIM = len(SRC.vocab)\n",
    "OUT_DIM = len(TRG.vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,  in_dim: int,  emb_dim: int, \n",
    "                 enc_hid_dim: int,  dec_hid_dim: int,  dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, enc_ins: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        # enc_ins: (batch_size, src_len)\n",
    "        embedded = self.dropout(self.emb(enc_ins))\n",
    "        # outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # hidden: (2, batch_size, enc_hid_dim)\n",
    "        outs, hidden = self.rnn(embedded)\n",
    "        # Concatenate the last hidden states in two directions. \n",
    "        # hidden: (batch_size, enc_hid_dim*2)\n",
    "        hidden = torch.cat([hidden[0], hidden[1]], dim=-1)\n",
    "        # hidden: (batch_size, dec_hid_dim)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 18, 128])\ntorch.Size([4, 64])\n"
    }
   ],
   "source": [
    "encoder = Encoder(IN_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT).to(device)\n",
    "# No initial hidden state provided, default to be zeros. \n",
    "enc_outs, dec_hidden = encoder(batch.src.T)\n",
    "print(enc_outs.size())\n",
    "print(dec_hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_in = enc_hid_dim*2 + dec_hid_dim\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self, dec_hidden: torch.Tensor, enc_outs: torch.Tensor) -> torch.Tensor:\n",
    "        src_len = enc_outs.size(1)\n",
    "        # repeated_dec_hidden: (batch_size, src_len, dec_hid_dim)\n",
    "        repeated_dec_hidden = dec_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # enc_outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # energy: (batch_size, src_len, attn_dim)\n",
    "        energy = torch.tanh(self.attn(torch.cat([repeated_dec_hidden, enc_outs], dim=-1)))\n",
    "        # attn: (batch_size, src_len)\n",
    "        attn = energy.sum(dim=-1)\n",
    "        return F.softmax(attn, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 18])\n"
    }
   ],
   "source": [
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM).to(device)\n",
    "attn = attention(dec_hidden, enc_outs)\n",
    "print(attn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dim: int, emb_dim: int, enc_hid_dim: int,  dec_hid_dim: int,  \n",
    "                 dropout: float, attention: Attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.emb = nn.Embedding(out_dim, emb_dim)\n",
    "        # Single-directional\n",
    "        self.rnn = nn.GRU(enc_hid_dim*2 + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim*2 + dec_hid_dim + emb_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, dec_ins: torch.Tensor, dec_hidden: torch.Tensor, \n",
    "                enc_outs: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        One-step forward. \n",
    "        \"\"\"\n",
    "        # dec_ins: (batch_size, 1)\n",
    "        # embedded: (batch_size, 1, dec_emb_dim)\n",
    "        embedded = self.dropout(self.emb(dec_ins))\n",
    "        \n",
    "        # attn: (batch_size, src_len)\n",
    "        attn = self.attention(dec_hidden, enc_outs)\n",
    "        # enc_outs: (batch_size, src_len, enc_hid_dim*2)\n",
    "        # wtd_enc_rep: (batch_size, 1, enc_hid_dim*2)\n",
    "        wtd_enc_rep = attn.unsqueeze(1).bmm(enc_outs)\n",
    "        # rnn_ins: (batch_size, 1, enc_hid_dim*2 + dec_emb_dim)\n",
    "        rnn_ins = torch.cat([embedded, wtd_enc_rep], dim=-1)\n",
    "        # outs: (batch_size, 1, dec_hid_dim)\n",
    "        outs, dec_hidden = self.rnn(rnn_ins, dec_hidden.unsqueeze(0))\n",
    "        # outs: (batch_size, 1, trg_voc_size)\n",
    "        outs = self.fc(torch.cat([outs, wtd_enc_rep, embedded], dim=-1))\n",
    "        return outs, dec_hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 64])\ntorch.Size([4, 64])\ntorch.Size([4, 1, 5893])\n"
    }
   ],
   "source": [
    "decoder = Decoder(OUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention).to(device)\n",
    "\n",
    "print(dec_hidden.size())\n",
    "\n",
    "dec_ins_0 = batch.trg[0].unsqueeze(1)\n",
    "dec_outs_0, dec_hidden = decoder(dec_ins_0, dec_hidden, enc_outs)\n",
    "print(dec_hidden.size())\n",
    "print(dec_outs_0.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, enc_ins: torch.Tensor, dec_ins: torch.Tensor, \n",
    "                teacher_forcing_ratio: float=0.5) -> torch.Tensor:\n",
    "        # enc_ins: (batch_size, src_len)\n",
    "        # dec_ins: (batch_size, trg_len)\n",
    "        # No initial hidden state provided, default to be zeros. \n",
    "        enc_outs, dec_hidden = self.encoder(enc_ins)\n",
    "\n",
    "        dec_outs = []\n",
    "        # The first input to the decoder is the <sos> token. \n",
    "        # dec_ins_t: (batch_size, 1)\n",
    "        dec_ins_t = dec_ins[:, 0].unsqueeze(1)\n",
    "        for t in range(1, dec_ins.size(1)):\n",
    "            # dec_outs_t: (batch_size, 1, trg_voc_size)\n",
    "            dec_outs_t, dec_hidden = self.decoder(dec_ins_t, dec_hidden, enc_outs)\n",
    "            top1 = dec_outs_t.max(dim=-1)[1]\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                dec_ins_t = dec_ins[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                dec_ins_t = top1\n",
    "            dec_outs.append(dec_outs_t)\n",
    "        return torch.cat(dec_outs, dim=1)\n",
    "\n",
    "    def translate(self, enc_ins: torch.Tensor, \n",
    "                  sos: int, eos: int, max_len: int=20) -> torch.Tensor:\n",
    "        enc_outs, dec_hidden = self.encoder(enc_ins)\n",
    "        top1s = []\n",
    "\n",
    "        # The first input to the decoder is the <sos> token. \n",
    "        # dec_ins_t: (batch_size=1, 1)\n",
    "        dec_ins_t = torch.ones(enc_ins.size(0), 1, dtype=torch.long, device=self.device) * sos\n",
    "        for t in range(max_len):\n",
    "            # dec_outs_t: (batch_size=1, 1, trg_voc_size)\n",
    "            dec_outs_t, dec_hidden = self.decoder(dec_ins_t, dec_hidden, enc_outs)\n",
    "            top1 = dec_outs_t.max(dim=-1)[1]\n",
    "            dec_ins_t = top1\n",
    "            top1s.append(top1)\n",
    "            if dec_ins_t.item() == eos:\n",
    "                break\n",
    "        return torch.cat(top1s, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([4, 17, 5893])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "dec_outs = model(batch.src.T, batch.trg.T)\n",
    "dec_outs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]],\n\n        [[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]]], device='cuda:0')"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Check if data are mixed across different samples in a batch.\n",
    "model.eval()\n",
    "dec_outs_012 = model(batch.src.T[:3], batch.trg.T[:3], 1)\n",
    "dec_outs_123 = model(batch.src.T[1:], batch.trg.T[1:], 1)\n",
    "dec_outs_012[1:] == dec_outs_123[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[2801,  532, 4413, 2061, 2615,  804, 5777, 4392, 4163,  748, 4600,  712,\n         3128, 2249, 3905, 5049, 2866,  470, 1802, 5247]], device='cuda:0')"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "TRG_SOS_IDX = TRG.vocab.stoi['<sos>']\n",
    "TRG_EOS_IDX = TRG.vocab.stoi['<eos>']\n",
    "\n",
    "model.translate(batch.src.T[:1], TRG_SOS_IDX, TRG_EOS_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 1,856,685 trainable parameters\n"
    }
   ],
   "source": [
    "encoder = Encoder(IN_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "decoder = Decoder(OUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes of Padding\n",
    "What should be noticed when using a mini-batch with sequences of different lengths? -> Padding\n",
    "* For both input and output sequences, if the RNNs are bidirectional, the initial hidden states from the backward direction is incorrect, as the hidden states have passed some padding positions. \n",
    "    * Use `pack_padded_sequence` and `pad_packed_sequence`.\n",
    "* For input sequence, some attention weights may be applied to the padding positions. \n",
    "* For input sequence, the pooling operation along the sequence may include the padding positions. \n",
    "* For output sequence, the loss calculation may include the padding positions. \n",
    "    * Use `ignore_index` parameter when creating the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Padding Index: 1\n"
    }
   ],
   "source": [
    "# NOTE: When scoring the model performance, tell the `nn.CrossEntropyLoss` function to ignore the indices where the target is padding. \n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "print(\"Padding Index: %d\" % PAD_IDX)\n",
    "\n",
    "# ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient.\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=PAD_IDX, reduction='mean')\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "#optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([29, 128])\n"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# The iterators work like `DataLoader`.\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE*2, BATCH_SIZE*2), \n",
    "    device=device)\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.src.size())\n",
    "    print(batch.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Time: 0m 22s\n\tTrain Loss: 5.675 | Train PPL: 291.471\n\t Val. Loss: 5.229 |  Val. PPL: 186.566\nEpoch: 02 | Time: 0m 22s\n\tTrain Loss: 4.979 | Train PPL: 145.352\n\t Val. Loss: 5.118 |  Val. PPL: 166.937\nEpoch: 03 | Time: 0m 22s\n\tTrain Loss: 4.749 | Train PPL: 115.487\n\t Val. Loss: 4.978 |  Val. PPL: 145.185\nEpoch: 04 | Time: 0m 22s\n\tTrain Loss: 4.600 | Train PPL:  99.510\n\t Val. Loss: 4.874 |  Val. PPL: 130.843\nEpoch: 05 | Time: 0m 22s\n\tTrain Loss: 4.453 | Train PPL:  85.881\n\t Val. Loss: 4.861 |  Val. PPL: 129.190\nEpoch: 06 | Time: 0m 22s\n\tTrain Loss: 4.351 | Train PPL:  77.579\n\t Val. Loss: 4.748 |  Val. PPL: 115.300\nEpoch: 07 | Time: 0m 22s\n\tTrain Loss: 4.261 | Train PPL:  70.870\n\t Val. Loss: 4.690 |  Val. PPL: 108.815\nEpoch: 08 | Time: 0m 22s\n\tTrain Loss: 4.160 | Train PPL:  64.068\n\t Val. Loss: 4.658 |  Val. PPL: 105.404\nEpoch: 09 | Time: 0m 22s\n\tTrain Loss: 4.072 | Train PPL:  58.647\n\t Val. Loss: 4.594 |  Val. PPL:  98.916\nEpoch: 10 | Time: 0m 22s\n\tTrain Loss: 3.989 | Train PPL:  54.008\n\t Val. Loss: 4.402 |  Val. PPL:  81.574\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    # Training\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        # Forward pass\n",
    "        dec_outs = model(batch.src.T, batch.trg.T)\n",
    "        dec_outs_flattened = dec_outs.view(-1, dec_outs.size(-1))\n",
    "        trg_flattened = batch.trg.T[:, 1:].flatten()\n",
    "        # Calculate loss\n",
    "        loss = loss_func(dec_outs_flattened, trg_flattened)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    train_loss = epoch_loss / len(train_iterator)\n",
    "\n",
    "    # Validating\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(valid_iterator):\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            dec_outs = model(batch.src.T, batch.trg.T, teacher_forcing_ratio=0)  #turn off teacher forcing\n",
    "            dec_outs_flattened = dec_outs.view(-1, dec_outs.size(-1))\n",
    "            trg_flattened = batch.trg.T[:, 1:].flatten()\n",
    "            # Calculate loss\n",
    "            loss = loss_func(dec_outs_flattened, trg_flattened)\n",
    "            epoch_loss += loss.item()\n",
    "    valid_loss = epoch_loss / len(valid_iterator)\n",
    "    model.train()\n",
    "\n",
    "    epoch_secs = time.time() - start_time\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "| Test Loss: 4.416 | Test PPL:  82.727 |\n"
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "epoch_loss = 0\n",
    "for i, batch in enumerate(test_iterator):\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        dec_outs = model(batch.src.T, batch.trg.T, teacher_forcing_ratio=0)  #turn off teacher forcing\n",
    "        dec_outs_flattened = dec_outs.view(-1, dec_outs.size(-1))\n",
    "        trg_flattened = batch.trg.T[:, 1:].flatten()\n",
    "        # Calculate loss\n",
    "        loss = loss_func(dec_outs_flattened, trg_flattened)\n",
    "        epoch_loss += loss.item()\n",
    "test_loss = epoch_loss / len(test_iterator)\n",
    "model.train()\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "De: ein kleines kind in grünen stiefeln spielt in einer schlammpfütze .\nEn (Trans): a little boy in a a a a a a a a a . <eos>\nEn (Real): a young child wearing green boots playing in a mud puddle\n==================================================\nDe: menschen fahren bei nacht mit mopeds die straße hinunter .\nEn (Trans): people are on a <unk> of a <unk> . <eos>\nEn (Real): people are driving scooters down the street at night .\n==================================================\nDe: eine gruppe asiatischer kinder in weißen hemden und kappen gibt eine vorstellung vor einer menge .\nEn (Trans): a group of people in a and and a and a and a and a and a . <eos>\nEn (Real): group of asian children dressed in white shirts and hats performing with a crowd looking on .\n==================================================\nDe: ein skateboarder fährt eine betonwand hoch und fällt beinahe beim versuch , einen trick zu machen .\nEn (Trans): a boy is a a a a a a a a the ball . <eos>\nEn (Real): a skateboarder rides up a concrete wall , nearly falling off as he tries a trick .\n==================================================\nDe: das ist eine gruppe von leuten , die auf einem event herumstehen .\nEn (Trans): a group of people are standing on a a of a a of a . . <eos>\nEn (Real): this is a group of people standing around at some sort of event .\n==================================================\nDe: ein mann schneidet holz mit einer kettensäge .\nEn (Trans): a man is a a a a a a a a . . <eos>\nEn (Real): a man using a chainsaw to cut lumber .\n==================================================\nDe: ein großer schwarzer pudel läuft auf dem gras mit einem spielzeug im maul .\nEn (Trans): a young in a dog and a dog in a dog and a dog in a dog . <eos>\nEn (Real): a big black poodle running on the grass with a toy in its mouth .\n==================================================\nDe: ein brauner hund watet in einen see , um einen stock zu holen .\nEn (Trans): a dog dog dog in a dog in a a ball in a dog . <eos>\nEn (Real): a brown dog wades into a lake to retrieve a stick .\n==================================================\nDe: ein kleines kind läuft neben roten stühlen .\nEn (Trans): a young boy in a a a a a a a a . <eos>\nEn (Real): a little kid is walking next to red banners .\n==================================================\nDe: ein kind sitzt an einem restauranttisch und hält eine papiermaske vors gesicht .\nEn (Trans): a young man is a a a a a a a a a a a a . . <eos>\nEn (Real): a child sitting at a restaurant table holding a paper mask against his face .\n==================================================\n"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "SRC_SOS_IDX = SRC.vocab.stoi['<sos>']\n",
    "SRC_EOS_IDX = SRC.vocab.stoi['<eos>']\n",
    "SRC_UNK_IDX = SRC.vocab.stoi['<unk>']\n",
    "\n",
    "for i in range(10):\n",
    "    IDX = np.random.randint(0, len(test_data))\n",
    "    print(\"De:\", \" \".join(test_data[IDX].src))\n",
    "\n",
    "    src = [SRC_SOS_IDX] + [SRC.vocab.stoi.get(w, SRC_UNK_IDX) for w in test_data[IDX].src] + [SRC_EOS_IDX]\n",
    "    src = torch.tensor(src, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    trans = model.translate(src, TRG_SOS_IDX, TRG_EOS_IDX).squeeze(0)\n",
    "    trans = \" \".join([TRG.vocab.itos[i.item()] for i in trans])\n",
    "    print(\"En (Trans):\", trans)\n",
    "    print(\"En (Real):\", \" \".join(test_data[IDX].trg))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
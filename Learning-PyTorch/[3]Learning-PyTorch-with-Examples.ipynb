{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-up: `numpy`\n",
    "Implement a network using `numpy`. \n",
    "\n",
    "The chain rule for matrix gradients.  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "z = f(Y), Y = AX + B \\rightarrow \\frac{\\partial z}{\\partial X} = A^T \\frac{\\partial z}{\\partial Y} \\\\\n",
    "z = f(Y), Y = XA + B \\rightarrow \\frac{\\partial z}{\\partial X} = \\frac{\\partial z}{\\partial Y} A^T \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 17779.934445622137\n100 1272.2658109026818\n150 197.74240228243002\n200 37.54745337570657\n250 7.718365241910927\n300 1.6499618175849955\n350 0.3603497852045676\n400 0.07973804354714127\n450 0.017789189447301703\n500 0.003991372345085601\n"
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    # h: (N, H)\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    # y_pred: (N, D_out)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = ((y_pred - y) ** 2).sum()\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss)\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    # China rule \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # (4) Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 11755.205078125\n100 310.06280517578125\n150 13.410354614257812\n200 0.683499276638031\n250 0.037457942962646484\n300 0.002346301218494773\n350 0.00029403201187960804\n400 7.899075717432424e-05\n450 3.370572812855244e-05\n500 1.897680886031594e-05\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    # h: (N, H)\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    # y_pred: (N, D_out)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = ((y_pred - y) ** 2).sum()\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    # China rule \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.T)\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.mm(grad_h)\n",
    "\n",
    "    # (4) Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 12863.048828125\n100 437.13348388671875\n150 23.839937210083008\n200 1.5797489881515503\n250 0.1170041561126709\n300 0.009401821531355381\n350 0.001031187130138278\n400 0.00022921152412891388\n450 8.458275988232344e-05\n500 4.188505045021884e-05\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = ((y_pred - y) ** 2).sum()\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        Receive: A Tensor containing the input\n",
    "        Return: A Tensor containing the output\n",
    "        ctx is a context object that can be used to stash information for backward \n",
    "        computation. You can cache arbitrary objects for use in the backward pass \n",
    "        using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Receive: A Tensor containing the gradient of the loss w.r.t the output\n",
    "        Return: A Tensor containing the gradient of the loss w.r.t the input\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 15177.96484375\n100 493.72174072265625\n150 27.658872604370117\n200 1.9257608652114868\n250 0.149516761302948\n300 0.01241301279515028\n350 0.001337741850875318\n400 0.0002710820408537984\n450 9.37502336455509e-05\n500 4.5355332986218855e-05\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "relu = MyReLU.apply\n",
    "\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = ((y_pred - y) ** 2).sum()\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: `nn` module\n",
    "Tensors and Autograd are relatively low-level for large neural networks.  \n",
    "The `nn` module deals with relatively high-level work, defining a set of Modules roughly equivalent to neural network layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 31.37506103515625\n100 2.320072650909424\n150 0.3068027198314667\n200 0.054306309670209885\n250 0.012054412625730038\n300 0.0030662568751722574\n350 0.0008425252162851393\n400 0.00024181004846468568\n450 7.122044917196035e-05\n500 2.1321982785593718e-05\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the network with nn module\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# Define the loss with nn module\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: `optim` module\n",
    "The `optim` module abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 203.43124389648438\n100 50.17519760131836\n150 8.437718391418457\n200 1.1158555746078491\n250 0.11447425931692123\n300 0.010102280415594578\n350 0.0008775893948040903\n400 6.843827577540651e-05\n450 4.252479357091943e-06\n500 1.991742948348474e-07\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the network with nn module\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# Define the loss with nn module\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Custom `nn` Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.fc1(x))\n",
    "        return self.fc2(h_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 174.67047119140625\n100 34.094120025634766\n150 3.6951241493225098\n200 0.27947697043418884\n250 0.023027854040265083\n300 0.002166505204513669\n350 0.00018708972493186593\n400 1.2912669262732379e-05\n450 6.738098363712197e-07\n500 2.577219149202392e-08\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the network with nn module\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Define the loss with nn module\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Control Flow + Weight Sharing\n",
    "As an example of dynamic graphs and weight sharing, we implement a very strange model: a fully-connected ReLU network that on each forward pass chooses a random number between 1 and 4 and uses that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers.\n",
    "\n",
    "For this model we can use normal Python flow control to implement the loop, and we can implement weight sharing among the innermost layers by simply reusing the same Module multiple times when defining the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fch = nn.Linear(H, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.fc1(x))\n",
    "        for _ in range(np.random.randint(1, 5)):\n",
    "            h_relu = F.relu(self.fch(h_relu))\n",
    "        return self.fc2(h_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 587.51953125\n100 527.52197265625\n150 492.1304016113281\n200 256.559814453125\n250 223.74461364746094\n300 48.736873626708984\n350 31.47231674194336\n400 32.43431091308594\n450 35.45283508300781\n500 34.850074768066406\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the network with nn module\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Define the loss with nn module\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
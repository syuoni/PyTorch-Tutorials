{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-up: numpy\n",
    "Implement a network using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{aligned}\n",
    "z = f(Y), Y = AX + B \\rightarrow \\frac{\\partial z}{\\partial X} = A^T \\frac{\\partial z}{\\partial Y} \\\\\n",
    "z = f(Y), Y = XA + B \\rightarrow \\frac{\\partial z}{\\partial X} = \\frac{\\partial z}{\\partial Y} A^T \n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 17771.563001788007\n100 824.3449816524\n150 63.10522474756195\n200 5.874290871420839\n250 0.6119854133533504\n300 0.06949911230053367\n350 0.008514900118536343\n400 0.0011171972723749862\n450 0.0001556328372400992\n500 2.2789354646465557e-05\n"
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    # h: (N, H)\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    # y_pred: (N, D_out)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = ((y_pred - y) ** 2).sum()\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss)\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    # China rule \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # (4) Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 14957.1787109375\n100 520.1260375976562\n150 28.716014862060547\n200 1.8750921487808228\n250 0.13260208070278168\n300 0.009954098612070084\n350 0.0010043686488643289\n400 0.0002048783644568175\n450 7.241935963975266e-05\n500 3.6382560210768133e-05\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    # h: (N, H)\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    # y_pred: (N, D_out)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = ((y_pred - y) ** 2).sum()\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    # China rule \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.T)\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.mm(grad_h)\n",
    "\n",
    "    # (4) Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 11510.7138671875\n100 541.0838623046875\n150 43.1258544921875\n200 4.282898426055908\n250 0.4824184775352478\n300 0.058715153485536575\n350 0.007716975640505552\n400 0.0012684206012636423\n450 0.000316200457746163\n500 0.00011679269664455205\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = ((y_pred - y) ** 2).sum()\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        Receive: A Tensor containing the input\n",
    "        Return: A Tensor containing the output\n",
    "        ctx is a context object that can be used to stash information for backward \n",
    "        computation. You can cache arbitrary objects for use in the backward pass \n",
    "        using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Receive: A Tensor containing the gradient of the loss w.r.t the output\n",
    "        Return: A Tensor containing the gradient of the loss w.r.t the input\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 20575.8984375\n100 1150.6260986328125\n150 99.48345184326172\n200 10.097822189331055\n250 1.1162739992141724\n300 0.1297990083694458\n350 0.01577002741396427\n400 0.002236555563285947\n450 0.0004819182795472443\n500 0.00015847866598051041\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "relu = MyReLU.apply\n",
    "\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = ((y_pred - y) ** 2).sum()\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: nn module\n",
    "Tensors and Autograd are relatively low-level for large neural networks.  \n",
    "\n",
    "The nn module deals with relatively high-level work, defining a set of Modules roughly equivalent to neural network layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 29.64038848876953\n100 2.567214250564575\n150 0.3545261025428772\n200 0.05738692358136177\n250 0.01023820973932743\n300 0.0019655346404761076\n350 0.00040638132486492395\n400 9.055338887264952e-05\n450 2.1870999262318946e-05\n500 5.810691163787851e-06\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the network with nn module\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# Define the loss with nn module\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: optim module\n",
    "The optim module  abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 166.0666961669922\n100 36.015262603759766\n150 4.174113750457764\n200 0.2797119915485382\n250 0.014573981054127216\n300 0.0006866778712719679\n350 2.770915125438478e-05\n400 8.530561217412469e-07\n450 1.8590624151215707e-08\n500 4.704743261640942e-10\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the network with nn module\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# Define the loss with nn module\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Custom nn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.fc1(x))\n",
    "        return self.fc2(h_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 184.05323791503906\n100 44.91203689575195\n150 6.023375034332275\n200 0.5573031902313232\n250 0.04588571563363075\n300 0.003909107763320208\n350 0.0002986715699080378\n400 1.7970227418118156e-05\n450 8.331199978783843e-07\n500 3.285331828806193e-08\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the network with nn module\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Define the loss with nn module\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Control Flow + Weight Sharing\n",
    "As an example of dynamic graphs and weight sharing, we implement a very strange model: a fully-connected ReLU network that on each forward pass chooses a random number between 1 and 4 and uses that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers.\n",
    "\n",
    "For this model we can use normal Python flow control to implement the loop, and we can implement weight sharing among the innermost layers by simply reusing the same Module multiple times when defining the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fch = nn.Linear(H, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.fc1(x))\n",
    "        for _ in range(np.random.randint(1, 5)):\n",
    "            h_relu = F.relu(self.fch(h_relu))\n",
    "        return self.fc2(h_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50 508.156005859375\n100 461.31329345703125\n150 223.98587036132812\n200 244.3576202392578\n250 273.8345031738281\n300 189.361572265625\n350 108.08002471923828\n400 69.9246826171875\n450 34.45793151855469\n500 41.158103942871094\n"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: outpur dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the network with nn module\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Define the loss with nn module\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # (1) Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # (2) Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(t+1, loss.item())\n",
    "\n",
    "    # (3) Backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (4) Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
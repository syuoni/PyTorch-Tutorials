{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# CUDA test\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1ea693e47a80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m             raise RuntimeError(\n\u001b[0;32m    148\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             raise AssertionError(\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1.0])\n",
    "xx = x.cuda()\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'xx' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4bd01785096b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# CUDNN test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_acceptable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'xx' is not defined"
     ]
    }
   ],
   "source": [
    "# CUDNN test\n",
    "from torch.backends import cudnn\n",
    "cudnn.is_acceptable(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation\n",
    "## Tensor: Size & Dimension\n",
    "\n",
    "For a N-dimensional Tensor x, its dimensions are numbered (0, 1, 2, ..., N-1). \n",
    "* The 0 dimension is the most outer dimension.\n",
    "* The N-1 dimension (or -1 dimension) is the most inner dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2],\n         [ 3,  4,  5],\n         [ 6,  7,  8],\n         [ 9, 10, 11]],\n\n        [[12, 13, 14],\n         [15, 16, 17],\n         [18, 19, 20],\n         [21, 22, 23]]])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x = torch.arange(0, 24).view(2, 4, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 4, 3])\ntorch.Size([2, 4, 3])\n3\n"
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)\n",
    "print(x.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n4\n3\n"
    }
   ],
   "source": [
    "print(x.size(0))\n",
    "print(x.size(1))\n",
    "print(x.size(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1, 2, 3])\ntorch.int64\n"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1.2000, 2.0000, 3.0000])\ntorch.float32\n"
    }
   ],
   "source": [
    "x = torch.tensor([1.2, 2, 3])\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Permute & Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2],\n         [ 3,  4,  5],\n         [ 6,  7,  8],\n         [ 9, 10, 11]],\n\n        [[12, 13, 14],\n         [15, 16, 17],\n         [18, 19, 20],\n         [21, 22, 23]]])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "x = torch.arange(0, 24).view(2, 4, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0,  3,  6,  9],\n         [12, 15, 18, 21]],\n\n        [[ 1,  4,  7, 10],\n         [13, 16, 19, 22]],\n\n        [[ 2,  5,  8, 11],\n         [14, 17, 20, 23]]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# (2, 0, 1) are the ORIGINAL dimensions\n",
    "# The original dimension 2 changes to the dimension 0 now. (The first group elements are 0, 1, 2)\n",
    "# The original dimension 0 changes to the dimension 1 now. (The first group elements are 0, 12)\n",
    "# The original dimension 1 changes to the dimension 2 now. (The first group elements are 0, 3, 6, 9)\n",
    "x.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0, 12],\n         [ 3, 15],\n         [ 6, 18],\n         [ 9, 21]],\n\n        [[ 1, 13],\n         [ 4, 16],\n         [ 7, 19],\n         [10, 22]],\n\n        [[ 2, 14],\n         [ 5, 17],\n         [ 8, 20],\n         [11, 23]]])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Equivalent to x.permute(n-1, n-2, ..., 0), if x is n-dimensional. \n",
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0,  3,  6,  9],\n        [ 1,  4,  7, 10],\n        [ 2,  5,  8, 11]])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# torch.t() only applies to 2D Tensor. \n",
    "x[0].t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Squeeze & Unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[[[ 0.7703,  1.6178]],\n\n          [[ 1.0111, -1.2006]]],\n\n\n         [[[ 0.2087,  1.7830]],\n\n          [[ 2.0622,  0.2360]]]]])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x = torch.randn(1, 2, 2, 1, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0.7703,  1.6178],\n         [ 1.0111, -1.2006]],\n\n        [[ 0.2087,  1.7830],\n         [ 2.0622,  0.2360]]])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Squeeze: Remove dimensions. \n",
    "x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[[ 0.7703,  1.6178]],\n\n         [[ 1.0111, -1.2006]]],\n\n\n        [[[ 0.2087,  1.7830]],\n\n         [[ 2.0622,  0.2360]]]])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1, 2, 3]])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Unsqueeze: Insert dimensions. \n",
    "x = torch.tensor([1, 2, 3])\n",
    "x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1],\n        [2],\n        [3]])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1],\n        [2],\n        [3]])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "x.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.9538,  0.3839, -0.2705],\n        [ 0.4907, -0.0826, -1.0331],\n        [-0.1848, -1.0522,  2.2946],\n        [-1.3760,  0.5522, -0.2061]])"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 3)\n",
    "# dim=0 means concatenating along the most outer dimension. (This dimension changes size.)\n",
    "# In this 2D case, it is equivalent to concatenating along rows. \n",
    "torch.cat([x, y], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.9538,  0.3839, -0.2705, -0.1848, -1.0522,  2.2946],\n        [ 0.4907, -0.0826, -1.0331, -1.3760,  0.5522, -0.2061]])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# dim=1 means concatenating along the dimension 1. (This dimension changes size.)\n",
    "# In this 2D case, it is equivalent to concatenating along columns. \n",
    "# In this 2D case, it is equivalent to dim=-1. \n",
    "torch.cat([x, y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.9537738 ,  0.38392898, -0.27051544],\n       [ 0.49068993, -0.08256959, -1.033057  ],\n       [-0.18484667, -1.0521578 ,  2.294553  ],\n       [-1.3759791 ,  0.5522005 , -0.20614688]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# axis=0 means concatenating along the most outer dimension.\n",
    "np.concatenate([x.numpy(), y.numpy()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0.9537738   0.38392898 -0.27051544 -0.18484667 -1.0521578   2.294553  ]\n [ 0.49068993 -0.08256959 -1.033057   -1.3759791   0.5522005  -0.20614688]]\n"
    }
   ],
   "source": [
    "# axis=1 means concatenating along the dimension 1.\n",
    "print(np.concatenate([x.numpy(), y.numpy()], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Repeat & Tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0, 1, 2],\n        [3, 4, 5]])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "x = torch.arange(6).view(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5],\n         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5],\n         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]],\n\n        [[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5],\n         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5],\n         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# torch.repeat behaves similar to numpy.tile, instead of numpy.repeat\n",
    "# Repeat along the dimension 0 for 2 times. \n",
    "# Repeat along the dimension 1 for 3 times. (Overlap the original dimension 0.)\n",
    "# Repeat along the dimension 2 for 4 times. (Overlap the original dimension 1.)\n",
    "x.repeat(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0., 1., 2.])\ntensor([0., 1., 2.])\ntensor(5.)\ntensor(5.)\n"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float)\n",
    "y = torch.arange(3, dtype=torch.float)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.dot: Vector-Vector inner production\n",
    "print(x.dot(y))\n",
    "# torch.matmal: Vector-Vector -> Scalar\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., 1., 2.],\n        [3., 4., 5.]])\ntensor([[0., 1.],\n        [2., 3.],\n        [4., 5.]])\ntensor([[10., 13.],\n        [28., 40.]])\ntensor([[10., 13.],\n        [28., 40.]])\n"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype=torch.float).view(2, 3)\n",
    "y = torch.arange(6, dtype=torch.float).view(3, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.mm: Matrix-Matrix production\n",
    "print(x.mm(y))\n",
    "# torch.matmal: Matrix-Matrix -> Matrix\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.]],\n\n        [[ 6.,  7.,  8.],\n         [ 9., 10., 11.]]])\ntensor([[[ 0.,  1.],\n         [ 2.,  3.],\n         [ 4.,  5.]],\n\n        [[ 6.,  7.],\n         [ 8.,  9.],\n         [10., 11.]]])\ntensor([[[ 10.,  13.],\n         [ 28.,  40.]],\n\n        [[172., 193.],\n         [244., 274.]]])\ntensor([[[ 10.,  13.],\n         [ 28.,  40.]],\n\n        [[172., 193.],\n         [244., 274.]]])\n"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float).view(2, 2, 3)\n",
    "y = torch.arange(12, dtype=torch.float).view(2, 3, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.bmm: Batched Matrix-Matrix production\n",
    "print(x.bmm(y))\n",
    "# torch.matmal: Batched Matrix-Batched Matrix -> Batched Matrix\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.]],\n\n        [[ 6.,  7.,  8.],\n         [ 9., 10., 11.]]])\ntensor([[[0., 1.],\n         [2., 3.],\n         [4., 5.]]])\ntensor([[[10., 13.],\n         [28., 40.]],\n\n        [[46., 67.],\n         [64., 94.]]])\n"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float).view(2, 2, 3)\n",
    "y = torch.arange(6, dtype=torch.float).view(1, 3, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.matmal: The non-matrix (i.e. batch) dimensions are broadcasted. \n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., 1., 2.],\n        [3., 4., 5.]])\ntensor([0., 1., 2.])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([ 5., 14.])"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "x = torch.arange(6, dtype=torch.float).view(2, 3)\n",
    "y = torch.arange(3, dtype=torch.float)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.matmal: Matrix-Vector -> Vector\n",
    "x.matmul(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "## Softmax\n",
    "\n",
    "$$\n",
    "f_i(x) = \\frac{e^{x_i}}{\\sum_j{e^{x_j}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.5000, 0.2689, 0.1192],\n        [0.5000, 0.7311, 0.8808]])\ntensor([[0.3333, 0.3333, 0.3333],\n        [0.0900, 0.2447, 0.6652]])\n"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3]], dtype=torch.float)\n",
    "\n",
    "# Specify the most outer dimension\n",
    "print(F.softmax(x, dim=0))\n",
    "# Specify the most inner dimension\n",
    "print(F.softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0.3333, 0.3333, 0.3333],\n         [0.0900, 0.2447, 0.6652]],\n\n        [[0.3333, 0.3333, 0.3333],\n         [0.6652, 0.2447, 0.0900]]])"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "x = torch.tensor([[[1, 1, 1], \n",
    "                   [1, 2, 3]], \n",
    "                  [[1, 1, 1], \n",
    "                   [3, 2, 1]]], dtype=torch.float)\n",
    "\n",
    "# Specify the most inner dimension\n",
    "F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[1., 1., 1.],\n         [1., 2., 3.]],\n\n        [[1., 1., 1.],\n         [3., 2., 1.]]])\ntensor([[[0.3333, 0.3333, 0.3333],\n         [0.0900, 0.2447, 0.6652]],\n\n        [[0.3333, 0.3333, 0.3333],\n         [0.6652, 0.2447, 0.0900]]])\n"
    }
   ],
   "source": [
    "# F.log_softmax cannot handle input over 3-rank, leading to wrong result (Depreciated)\n",
    "x = torch.tensor([[[1, 1, 1], [1, 2, 3]], \n",
    "                  [[1, 1, 1], [3, 2, 1]]], dtype=torch.float32)\n",
    "print(x)\n",
    "print(F.softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Softmax\n",
    "$$ \n",
    "f_i(x) = \\log \\left( \\frac{e^{x_i}}{\\sum_j{e^{x_j}}} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.6931, -1.3133, -2.1269],\n        [-0.6931, -0.3133, -0.1269]])\ntensor([[-1.0986, -1.0986, -1.0986],\n        [-2.4076, -1.4076, -0.4076]])\ntensor([[-1.0986, -1.0986, -1.0986],\n        [-2.4076, -1.4076, -0.4076]])\n"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3]], dtype=torch.float)\n",
    "\n",
    "# Specify the most outer dimension\n",
    "print(F.log_softmax(x, dim=0))\n",
    "# Specify the most inner dimension\n",
    "print(F.log_softmax(x, dim=-1))\n",
    "\n",
    "# Check...\n",
    "print(F.softmax(x, dim=-1).log())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.7311, 0.8808, 0.9526])"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "\n",
    "F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "$$\n",
    "f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.7616, 0.9640, 0.9951])"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "\n",
    "F.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\max (0, x)  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([-3., -2., -1.,  0.,  1.,  2.,  3.])\ntensor([0., 0., 0., 0., 1., 2., 3.])\n"
    }
   ],
   "source": [
    "x = torch.arange(-3, 4, dtype=torch.float)\n",
    "\n",
    "print(x)\n",
    "print(F.relu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "## MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(1.5000)\ntensor(1.5000)\n"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([1, 1, 2, 2], dtype=torch.float32)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "print(loss_func(x, y))\n",
    "print(((x - y)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(6.)\ntensor(6.)\n"
    }
   ],
   "source": [
    "# SSE Loss\n",
    "loss_func = nn.MSELoss(reduction='sum')\n",
    "print(loss_func(x, y))\n",
    "print(((x - y)**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth L1 Loss\n",
    "$$\n",
    "\\mathrm{SmoothL1Loss}(x, y) = \n",
    "\\begin{cases}\n",
    "    0.5 * (x - y)^2, & \\text{if } |x - y| < 1 \\\\\n",
    "    |x - y| - 0.5,   & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "It is less sensitive to outliers than the `MSELoss`. Also known as the Huber loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.6250)"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float)\n",
    "y = torch.tensor([1, 1, 2, 2], dtype=torch.float)\n",
    "\n",
    "loss_func = nn.SmoothL1Loss()\n",
    "loss_func(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLLoss (Negative Log-Likelihood Loss) \n",
    "$$\n",
    "\\mathrm{NLLLoss}(x, class) = -x[class] \\\\\n",
    "$$\n",
    "where x is a vector of log-likelihood of each class. \n",
    "\n",
    "The negative log likelihood loss. It is useful to train a classification task with n classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-1.0986, -1.0986, -1.0986],\n        [-2.4076, -1.4076, -0.4076]])"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3]], dtype=torch.float)\n",
    "y = torch.tensor([1, 0])\n",
    "\n",
    "# Use log_softmax to calculate log-likelihood \n",
    "x_ll = F.log_softmax(x, dim=-1)\n",
    "x_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(3.5062)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "loss_func = nn.NLLLoss(reduction='sum')\n",
    "loss_func(x_ll, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{CrossEntropyLoss}(x, class) &= -\\mathrm{LogSoftmax}(x) [class] \\\\\n",
    "                                    &= -\\log \\left( \\frac{e^{x[class]}}{\\sum_j e^{x[j]}} \\right) \\\\\n",
    "                                    &= -x[class] + \\log \\left( \\sum_j e^{x[j]} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\mathrm{Softmax}(x)$ is a vector of likelihood of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(3.5062)"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3]], dtype=torch.float)\n",
    "y = torch.tensor([1, 0])\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "# NO need to calculate log_likelihood \n",
    "# Get the same result with log_softmax -> NLLLoss\n",
    "loss_func(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "## SGD\n",
    "Args:  \n",
    "params (iterable): iterable of parameters to optimize or dicts defining parameter groups  \n",
    "lr (float): learning rate  \n",
    "momentum (float, optional): momentum factor (default: 0)  \n",
    "weight_decay (float, optional): weight decay (L2 penalty) (default: 0)  \n",
    "dampening (float, optional): dampening for momentum (default: 0)  \n",
    "nesterov (bool, optional): enables Nesterov momentum (default: False)  \n",
    "\n",
    "## Adadelta\n",
    "Arguments:  \n",
    "params (iterable): iterable of parameters to optimize or dicts defining parameter groups  \n",
    "rho (float, optional): coefficient used for computing a running average of squared gradients (default: 0.9)  \n",
    "eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-6)  \n",
    "lr (float, optional): coefficient that scale delta before it is applied to the parameters (default: 1.0)  \n",
    "weight_decay (float, optional): weight decay (L2 penalty) (default: 0)  \n",
    "\n",
    "## Adam\n",
    "Arguments:\n",
    "params (iterable): iterable of parameters to optimize or dicts defining parameter groups  \n",
    "lr (float, optional): learning rate (default: 1e-3)  \n",
    "betas (Tuple [float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))  \n",
    "eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)  \n",
    "weight_decay (float, optional): weight decay (L2 penalty) (default: 0)  \n",
    "\n",
    "## L-BFGS\n",
    "Arguments:\n",
    "lr (float): learning rate (default: 1)  \n",
    "max_iter (int): maximal number of iterations per optimization step (default: 20)  \n",
    "max_eval (int): maximal number of function evaluations per optimization step (default: max_iter * 1.25).  \n",
    "tolerance_grad (float): termination tolerance on first order optimality (default: 1e-5).  \n",
    "tolerance_change (float): termination tolerance on function value/parameter changes (default: 1e-9).  \n",
    "history_size (int): update history size (default: 100).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.9825298, 0.9661818, 0.9434212, 0.9811514, 1.0246055],\n      dtype=float32)"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "M = 500\n",
    "N = 5\n",
    "\n",
    "np.random.seed(515)\n",
    "x = np.random.randn(M, N).astype(np.float32)\n",
    "y = x.sum(axis=1) + np.random.randn(M).astype(np.float32)\n",
    "\n",
    "# Estimate the model with standard OLS. \n",
    "ols = LinearRegression()\n",
    "ols.fit(x, y)\n",
    "ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map data to Tensors\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[0.9825, 0.9662, 0.9434, 0.9811, 1.0246]], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# Test for SGD\n",
    "# Network\n",
    "fc = nn.Linear(5, 1)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(fc.parameters(), lr=0.01)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = fc(x)\n",
    "    # MUST put the predicted varibale at the first argument\n",
    "    loss = loss_func(y_pred, y.view(-1, 1))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for epoch in range(10000):\n",
    "    optimizer.step(closure=closure)\n",
    "\n",
    "fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[0.9825, 0.9662, 0.9434, 0.9812, 1.0246]], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# Test for L-BFGS\n",
    "# Network\n",
    "fc = nn.Linear(5, 1)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.LBFGS(fc.parameters())\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = fc(x)\n",
    "    # MUST put the predicted varibale at the first argument\n",
    "    loss = loss_func(y_pred, y.view(-1, 1))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# Performs a single optimization step\n",
    "optimizer.step(closure=closure)\n",
    "\n",
    "fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[0.9825, 0.9662, 0.9434, 0.9812, 1.0246]], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# Test for Adam\n",
    "# Network\n",
    "fc = nn.Linear(5, 1)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(fc.parameters())\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = fc(x)\n",
    "    # MUST put the predicted varibale at the first argument\n",
    "    loss = loss_func(y_pred, y.view(-1, 1))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for epoch in range(10000):\n",
    "    optimizer.step(closure=closure)\n",
    "\n",
    "fc.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\ntensor([ 0.,  0.,  0.,  6.,  8.,  0.,  0., 14., 16., 18.])\n"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.arange(0, 10, dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "dropout_layer = nn.Dropout(p=0.5)\n",
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([ 0.0000,  1.2500,  2.5000,  0.0000,  5.0000,  6.2500,  7.5000,  0.0000,\n         0.0000, 11.2500])\n"
    }
   ],
   "source": [
    "dropout_layer = nn.Dropout(p=0.2)\n",
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
    }
   ],
   "source": [
    "dropout_layer.eval()\n",
    "print(dropout_layer.training)\n",
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\ntensor([ 0.0000,  1.2500,  2.5000,  3.7500,  5.0000,  6.2500,  0.0000,  8.7500,\n         0.0000, 11.2500])\n"
    }
   ],
   "source": [
    "dropout_layer.train()\n",
    "print(dropout_layer.training)\n",
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
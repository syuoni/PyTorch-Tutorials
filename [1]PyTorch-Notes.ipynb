{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# CUDA test\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# CUDNN test\n",
    "from torch.backends import cudnn\n",
    "x = torch.Tensor([1.0])\n",
    "xx = x.cuda()\n",
    "\n",
    "cudnn.is_acceptable(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 Tesla V100-PCIE-32GB\n1 Tesla V100-PCIE-32GB\n2 Tesla V100-PCIE-32GB\n3 Tesla V100-PCIE-32GB\n"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.0940, -0.1102, -0.5684,  1.0773, -0.6414],\n        [ 0.6987,  0.3423, -1.4622, -0.5966, -0.6303]])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Random from a standard normal distribution N(0, 1). \n",
    "torch.randn(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\ntensor([[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]])\n"
    }
   ],
   "source": [
    "# Default dtype is torch.float\n",
    "print(torch.ones(2, 5))\n",
    "print(torch.ones(2, 5, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]])\n"
    }
   ],
   "source": [
    "# Default dtype is torch.float\n",
    "print(torch.zeros(2, 5))\n",
    "print(torch.zeros(2, 5, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0, 1, 2, 3, 4])\ntensor([0., 1., 2., 3., 4.])\n"
    }
   ],
   "source": [
    "# Default dtype is torch.long\n",
    "print(torch.arange(5))\n",
    "print(torch.arange(5, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1, 2, 3])\ntorch.int64\n"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1.2000, 2.0000, 3.0000])\ntorch.float32\n"
    }
   ],
   "source": [
    "x = torch.tensor([1.2, 2, 3])\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([100.0000,   0.6549,  -0.9260,  -0.3167,   0.5770],\n       grad_fn=<CopySlices>)\ntensor([100.0000,   0.6549,  -0.9260,  -0.3167,   0.5770],\n       grad_fn=<CopySlices>)\n"
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x\n",
    "\n",
    "# x and y are the same object. \n",
    "y[0] = 100\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([-2.0646,  1.0507, -0.5851,  0.2291,  0.3706], requires_grad=True)\ntensor([100.0000,   1.0507,  -0.5851,   0.2291,   0.3706],\n       grad_fn=<CopySlices>)\n"
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x.clone()\n",
    "\n",
    "# x and y are different objects. \n",
    "y[0] = 100\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation\n",
    "## Tensor: Size & Dimension\n",
    "\n",
    "For a N-dimensional Tensor x, its dimensions are numbered (0, 1, 2, ..., N-1). \n",
    "* The 0 dimension is the most outer dimension.\n",
    "* The N-1 dimension (or -1 dimension) is the most inner dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2],\n         [ 3,  4,  5],\n         [ 6,  7,  8],\n         [ 9, 10, 11]],\n\n        [[12, 13, 14],\n         [15, 16, 17],\n         [18, 19, 20],\n         [21, 22, 23]]])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "x = torch.arange(0, 24).view(2, 4, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 4, 3])\ntorch.Size([2, 4, 3])\n3\n"
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)\n",
    "print(x.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n4\n3\n"
    }
   ],
   "source": [
    "print(x.size(0))\n",
    "print(x.size(1))\n",
    "print(x.size(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Permute & Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2],\n         [ 3,  4,  5],\n         [ 6,  7,  8],\n         [ 9, 10, 11]],\n\n        [[12, 13, 14],\n         [15, 16, 17],\n         [18, 19, 20],\n         [21, 22, 23]]])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "x = torch.arange(0, 24).view(2, 4, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0,  3,  6,  9],\n         [12, 15, 18, 21]],\n\n        [[ 1,  4,  7, 10],\n         [13, 16, 19, 22]],\n\n        [[ 2,  5,  8, 11],\n         [14, 17, 20, 23]]])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# (2, 0, 1) are the ORIGINAL dimensions\n",
    "# The original dimension 2 changes to the dimension 0 now. (The first group elements are 0, 1, 2)\n",
    "# The original dimension 0 changes to the dimension 1 now. (The first group elements are 0, 12)\n",
    "# The original dimension 1 changes to the dimension 2 now. (The first group elements are 0, 3, 6, 9)\n",
    "x.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0, 12],\n         [ 3, 15],\n         [ 6, 18],\n         [ 9, 21]],\n\n        [[ 1, 13],\n         [ 4, 16],\n         [ 7, 19],\n         [10, 22]],\n\n        [[ 2, 14],\n         [ 5, 17],\n         [ 8, 20],\n         [11, 23]]])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Equivalent to x.permute(n-1, n-2, ..., 0), if x is n-dimensional. \n",
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0,  3,  6,  9],\n        [ 1,  4,  7, 10],\n        [ 2,  5,  8, 11]])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# torch.t() only applies to 2D Tensor. \n",
    "x[0].t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Squeeze & Unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[[[-1.1112, -0.3307]],\n\n          [[ 1.0290, -0.5752]]],\n\n\n         [[[ 0.2071, -0.6596]],\n\n          [[ 1.4697,  0.1795]]]]])"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "x = torch.randn(1, 2, 2, 1, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-1.1112, -0.3307],\n         [ 1.0290, -0.5752]],\n\n        [[ 0.2071, -0.6596],\n         [ 1.4697,  0.1795]]])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# Squeeze: Remove dimensions. \n",
    "x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[[-1.1112, -0.3307]],\n\n         [[ 1.0290, -0.5752]]],\n\n\n        [[[ 0.2071, -0.6596]],\n\n         [[ 1.4697,  0.1795]]]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1, 2, 3]])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# Unsqueeze: Insert dimensions. \n",
    "x = torch.tensor([1, 2, 3])\n",
    "x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1],\n        [2],\n        [3]])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1],\n        [2],\n        [3]])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "x.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.3530, -0.2241, -0.1479],\n        [ 0.6436,  1.1626, -0.2370],\n        [ 0.1363,  0.1794,  0.9533],\n        [-1.3683, -1.6694,  0.3587]])"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 3)\n",
    "# dim=0 means concatenating along the most outer dimension. (This dimension changes size.)\n",
    "# In this 2D case, it is equivalent to concatenating along rows. \n",
    "torch.cat([x, y], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.3530, -0.2241, -0.1479,  0.1363,  0.1794,  0.9533],\n        [ 0.6436,  1.1626, -0.2370, -1.3683, -1.6694,  0.3587]])"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# dim=1 means concatenating along the dimension 1. (This dimension changes size.)\n",
    "# In this 2D case, it is equivalent to concatenating along columns. \n",
    "# In this 2D case, it is equivalent to dim=-1. \n",
    "torch.cat([x, y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.352973  , -0.22413246, -0.14793782],\n       [ 0.64360964,  1.1625934 , -0.23699497],\n       [ 0.1363454 ,  0.17937094,  0.9533077 ],\n       [-1.3683468 , -1.6694335 ,  0.3586677 ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# axis=0 means concatenating along the most outer dimension.\n",
    "np.concatenate([x.numpy(), y.numpy()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0.352973   -0.22413246 -0.14793782  0.1363454   0.17937094  0.9533077 ]\n [ 0.64360964  1.1625934  -0.23699497 -1.3683468  -1.6694335   0.3586677 ]]\n"
    }
   ],
   "source": [
    "# axis=1 means concatenating along the dimension 1.\n",
    "print(np.concatenate([x.numpy(), y.numpy()], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor: Repeat & Tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0, 1, 2],\n        [3, 4, 5]])"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "x = torch.arange(6).view(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5],\n         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5],\n         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]],\n\n        [[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5],\n         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5],\n         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n         [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]])"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# torch.repeat behaves similar to numpy.tile, instead of numpy.repeat\n",
    "# Repeat along the dimension 0 for 2 times. \n",
    "# Repeat along the dimension 1 for 3 times. (Overlap the original dimension 0.)\n",
    "# Repeat along the dimension 2 for 4 times. (Overlap the original dimension 1.)\n",
    "x.repeat(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0., 1., 2.])\ntensor([0., 1., 2.])\ntensor(5.)\ntensor(5.)\n"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float)\n",
    "y = torch.arange(3, dtype=torch.float)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.dot: Vector-Vector inner production\n",
    "print(x.dot(y))\n",
    "# torch.matmal: Vector-Vector -> Scalar\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., 1., 2.],\n        [3., 4., 5.]])\ntensor([0., 1., 2.])\ntensor([ 5., 14.])\ntensor([ 5., 14.])\n"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype=torch.float).view(2, 3)\n",
    "y = torch.arange(3, dtype=torch.float)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.mv: Matrix-Vector production\n",
    "print(x.mv(y))\n",
    "# torch.matmal: Matrix-Vector -> Vector\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., 1., 2.],\n        [3., 4., 5.]])\ntensor([[0., 1.],\n        [2., 3.],\n        [4., 5.]])\ntensor([[10., 13.],\n        [28., 40.]])\ntensor([[10., 13.],\n        [28., 40.]])\n"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype=torch.float).view(2, 3)\n",
    "y = torch.arange(6, dtype=torch.float).view(3, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.mm: Matrix-Matrix production\n",
    "print(x.mm(y))\n",
    "# torch.matmal: Matrix-Matrix -> Matrix\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.]],\n\n        [[ 6.,  7.,  8.],\n         [ 9., 10., 11.]]])\ntensor([[[ 0.,  1.],\n         [ 2.,  3.],\n         [ 4.,  5.]],\n\n        [[ 6.,  7.],\n         [ 8.,  9.],\n         [10., 11.]]])\ntensor([[[ 10.,  13.],\n         [ 28.,  40.]],\n\n        [[172., 193.],\n         [244., 274.]]])\ntensor([[[ 10.,  13.],\n         [ 28.,  40.]],\n\n        [[172., 193.],\n         [244., 274.]]])\n"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float).view(2, 2, 3)\n",
    "y = torch.arange(12, dtype=torch.float).view(2, 3, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.bmm: Batched Matrix-Matrix production\n",
    "print(x.bmm(y))\n",
    "# torch.matmal: Batched Matrix-Batched Matrix -> Batched Matrix\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.]],\n\n        [[ 6.,  7.,  8.],\n         [ 9., 10., 11.]]])\ntensor([[[0., 1.],\n         [2., 3.],\n         [4., 5.]]])\ntensor([[[10., 13.],\n         [28., 40.]],\n\n        [[46., 67.],\n         [64., 94.]]])\n"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float).view(2, 2, 3)\n",
    "y = torch.arange(6, dtype=torch.float).view(1, 3, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# torch.matmal: The non-matrix (i.e. batch) dimensions are broadcasted. \n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "## Softmax\n",
    "\n",
    "$$\n",
    "f_i(x) = \\frac{e^{x_i}}{\\sum_j{e^{x_j}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.5000, 0.2689, 0.1192],\n        [0.5000, 0.7311, 0.8808]])\ntensor([[0.3333, 0.3333, 0.3333],\n        [0.0900, 0.2447, 0.6652]])\n"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3]], dtype=torch.float)\n",
    "\n",
    "# Specify the most outer dimension\n",
    "print(F.softmax(x, dim=0))\n",
    "# Specify the most inner dimension\n",
    "print(F.softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0.3333, 0.3333, 0.3333],\n         [0.0900, 0.2447, 0.6652]],\n\n        [[0.3333, 0.3333, 0.3333],\n         [0.6652, 0.2447, 0.0900]]])"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "x = torch.tensor([[[1, 1, 1], \n",
    "                   [1, 2, 3]], \n",
    "                  [[1, 1, 1], \n",
    "                   [3, 2, 1]]], dtype=torch.float)\n",
    "\n",
    "# Specify the most inner dimension\n",
    "F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[1., 1., 1.],\n         [1., 2., 3.]],\n\n        [[1., 1., 1.],\n         [3., 2., 1.]]])\ntensor([[[0.3333, 0.3333, 0.3333],\n         [0.0900, 0.2447, 0.6652]],\n\n        [[0.3333, 0.3333, 0.3333],\n         [0.6652, 0.2447, 0.0900]]])\n"
    }
   ],
   "source": [
    "# F.log_softmax cannot handle input over 3-rank, leading to wrong result (Depreciated)\n",
    "x = torch.tensor([[[1, 1, 1], [1, 2, 3]], \n",
    "                  [[1, 1, 1], [3, 2, 1]]], dtype=torch.float32)\n",
    "print(x)\n",
    "print(F.softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Softmax\n",
    "$$ \n",
    "f_i(x) = \\log \\left( \\frac{e^{x_i}}{\\sum_j{e^{x_j}}} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.6931, -1.3133, -2.1269],\n        [-0.6931, -0.3133, -0.1269]])\ntensor([[-1.0986, -1.0986, -1.0986],\n        [-2.4076, -1.4076, -0.4076]])\ntensor([[-1.0986, -1.0986, -1.0986],\n        [-2.4076, -1.4076, -0.4076]])\n"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3]], dtype=torch.float)\n",
    "\n",
    "# Specify the most outer dimension\n",
    "print(F.log_softmax(x, dim=0))\n",
    "# Specify the most inner dimension\n",
    "print(F.log_softmax(x, dim=-1))\n",
    "\n",
    "# Check...\n",
    "print(F.softmax(x, dim=-1).log())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.7311, 0.8808, 0.9526])"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "\n",
    "F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "$$\n",
    "f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.7616, 0.9640, 0.9951])"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "\n",
    "F.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\max (0, x)  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([-3., -2., -1.,  0.,  1.,  2.,  3.])\ntensor([0., 0., 0., 0., 1., 2., 3.])\n"
    }
   ],
   "source": [
    "x = torch.arange(-3, 4, dtype=torch.float)\n",
    "\n",
    "print(x)\n",
    "print(F.relu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "## MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(1.5000)\ntensor(1.5000)\n"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([1, 1, 2, 2], dtype=torch.float32)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "print(loss_func(x, y))\n",
    "print(((x - y)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(6.)\ntensor(6.)\n"
    }
   ],
   "source": [
    "# SSE Loss\n",
    "loss_func = nn.MSELoss(reduction='sum')\n",
    "print(loss_func(x, y))\n",
    "print(((x - y)**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth L1 Loss\n",
    "$$\n",
    "\\mathrm{SmoothL1Loss}(x, y) = \n",
    "\\begin{cases}\n",
    "    0.5 * (x - y)^2, & \\text{if } |x - y| < 1 \\\\\n",
    "    |x - y| - 0.5,   & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "It is less sensitive to outliers than the `MSELoss`. Also known as the Huber loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.6250)"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float)\n",
    "y = torch.tensor([1, 1, 2, 2], dtype=torch.float)\n",
    "\n",
    "loss_func = nn.SmoothL1Loss()\n",
    "loss_func(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLLoss (Negative Log-Likelihood Loss) \n",
    "$$\n",
    "\\mathrm{NLLLoss}(x, class) = -x[class] \\\\\n",
    "$$\n",
    "where x is a vector of log-likelihood of each class. \n",
    "\n",
    "The negative log likelihood loss. It is useful to train a classification task with n classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-1.0986, -1.0986, -1.0986],\n        [-2.4076, -1.4076, -0.4076]])"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3]], dtype=torch.float)\n",
    "y = torch.tensor([1, 0])\n",
    "\n",
    "# Use log_softmax to calculate log-likelihood \n",
    "x_ll = F.log_softmax(x, dim=-1)\n",
    "x_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(3.5062)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "loss_func = nn.NLLLoss(reduction='sum')\n",
    "loss_func(x_ll, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{CrossEntropyLoss}(x, class) &= -\\mathrm{LogSoftmax}(x) [class] \\\\\n",
    "                                    &= -\\log \\left( \\frac{e^{x[class]}}{\\sum_j e^{x[j]}} \\right) \\\\\n",
    "                                    &= -x[class] + \\log \\left( \\sum_j e^{x[j]} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\mathrm{Softmax}(x)$ is a vector of likelihood of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(3.5062)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3]], dtype=torch.float)\n",
    "y = torch.tensor([1, 0])\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "# NO need to calculate log_likelihood \n",
    "# Get the same result with log_softmax -> NLLLoss\n",
    "loss_func(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "## SGD\n",
    "Args:  \n",
    "params (iterable): iterable of parameters to optimize or dicts defining parameter groups  \n",
    "lr (float): learning rate  \n",
    "momentum (float, optional): momentum factor (default: 0)  \n",
    "weight_decay (float, optional): weight decay (L2 penalty) (default: 0)  \n",
    "dampening (float, optional): dampening for momentum (default: 0)  \n",
    "nesterov (bool, optional): enables Nesterov momentum (default: False)  \n",
    "\n",
    "## Adadelta\n",
    "Arguments:  \n",
    "params (iterable): iterable of parameters to optimize or dicts defining parameter groups  \n",
    "rho (float, optional): coefficient used for computing a running average of squared gradients (default: 0.9)  \n",
    "eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-6)  \n",
    "lr (float, optional): coefficient that scale delta before it is applied to the parameters (default: 1.0)  \n",
    "weight_decay (float, optional): weight decay (L2 penalty) (default: 0)  \n",
    "\n",
    "## Adam\n",
    "Arguments:\n",
    "params (iterable): iterable of parameters to optimize or dicts defining parameter groups  \n",
    "lr (float, optional): learning rate (default: 1e-3)  \n",
    "betas (Tuple [float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))  \n",
    "eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)  \n",
    "weight_decay (float, optional): weight decay (L2 penalty) (default: 0)  \n",
    "\n",
    "## L-BFGS\n",
    "Arguments:\n",
    "lr (float): learning rate (default: 1)  \n",
    "max_iter (int): maximal number of iterations per optimization step (default: 20)  \n",
    "max_eval (int): maximal number of function evaluations per optimization step (default: max_iter * 1.25).  \n",
    "tolerance_grad (float): termination tolerance on first order optimality (default: 1e-5).  \n",
    "tolerance_change (float): termination tolerance on function value/parameter changes (default: 1e-9).  \n",
    "history_size (int): update history size (default: 100).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.9825298, 0.9661818, 0.9434212, 0.9811514, 1.0246055],\n      dtype=float32)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "M = 500\n",
    "N = 5\n",
    "\n",
    "np.random.seed(515)\n",
    "x = np.random.randn(M, N).astype(np.float32)\n",
    "y = x.sum(axis=1) + np.random.randn(M).astype(np.float32)\n",
    "\n",
    "# Estimate the model with standard OLS. \n",
    "ols = LinearRegression()\n",
    "ols.fit(x, y)\n",
    "ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map data to Tensors\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[0.9825, 0.9662, 0.9434, 0.9811, 1.0246]], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# Test for SGD\n",
    "# Network\n",
    "fc = nn.Linear(5, 1)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(fc.parameters(), lr=0.01)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = fc(x)\n",
    "    # MUST put the predicted varibale at the first argument\n",
    "    loss = loss_func(y_pred, y.view(-1, 1))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for epoch in range(10000):\n",
    "    optimizer.step(closure=closure)\n",
    "\n",
    "fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[0.9825, 0.9662, 0.9434, 0.9812, 1.0246]], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "# Test for L-BFGS\n",
    "# Network\n",
    "fc = nn.Linear(5, 1)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.LBFGS(fc.parameters())\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = fc(x)\n",
    "    # MUST put the predicted varibale at the first argument\n",
    "    loss = loss_func(y_pred, y.view(-1, 1))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# Performs a single optimization step\n",
    "optimizer.step(closure=closure)\n",
    "\n",
    "fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[0.9825, 0.9662, 0.9434, 0.9812, 1.0246]], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "# Test for Adam\n",
    "# Network\n",
    "fc = nn.Linear(5, 1)\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(fc.parameters())\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = fc(x)\n",
    "    # MUST put the predicted varibale at the first argument\n",
    "    loss = loss_func(y_pred, y.view(-1, 1))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for epoch in range(10000):\n",
    "    optimizer.step(closure=closure)\n",
    "\n",
    "fc.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\ntensor([ 0.,  0.,  0.,  6.,  8.,  0.,  0., 14., 16., 18.])\n"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.arange(0, 10, dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "dropout_layer = nn.Dropout(p=0.5)\n",
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([ 0.0000,  1.2500,  2.5000,  0.0000,  5.0000,  6.2500,  7.5000,  0.0000,\n         0.0000, 11.2500])\n"
    }
   ],
   "source": [
    "dropout_layer = nn.Dropout(p=0.2)\n",
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
    }
   ],
   "source": [
    "dropout_layer.eval()\n",
    "print(dropout_layer.training)\n",
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\ntensor([ 0.0000,  1.2500,  2.5000,  3.7500,  5.0000,  6.2500,  0.0000,  8.7500,\n         0.0000, 11.2500])\n"
    }
   ],
   "source": [
    "dropout_layer.train()\n",
    "print(dropout_layer.training)\n",
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.3333, 0.3333, 0.3333],\n        [0.0900, 0.2447, 0.6652],\n        [0.0900, 0.2447, 0.6652]])"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "x = torch.tensor([[1, 1, 1], \n",
    "                  [1, 2, 3], \n",
    "                  [2, 3, 4]], dtype=torch.float)\n",
    "y = torch.tensor([[21, 21, 21], \n",
    "                  [21, 22, 23], \n",
    "                  [22, 23, 24]], dtype=torch.float)\n",
    "\n",
    "F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[False, False, False],\n        [False, False,  True],\n        [False,  True,  True]])\ntensor([[1., 1., 1.],\n        [1., 2., -inf],\n        [2., -inf, -inf]])\ntensor([[0.3333, 0.3333, 0.3333],\n        [0.2689, 0.7311, 0.0000],\n        [1.0000, 0.0000, 0.0000]])\n"
    }
   ],
   "source": [
    "# `mask` is a bool Tensor\n",
    "mask = (x > 2)\n",
    "print(mask)\n",
    "\n",
    "# Fill `x` with `-inf` where `mask` is True. \n",
    "x.masked_fill_(mask, -np.inf)\n",
    "print(x)\n",
    "\n",
    "print(F.softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 1.,  1.,  1.],\n        [ 1.,  2., 21.],\n        [ 2., 21., 21.]])\n"
    }
   ],
   "source": [
    "# Fill `x` with values from `y` where `mask` is True. \n",
    "x.masked_scatter_(mask, y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([[-0.4339,  0.8487,  0.6920, -0.3160, -2.1152],\n         [-0.3561,  0.4372,  0.4913,  0.3081,  0.1198],\n         [ 1.2377,  1.1168, -0.2473, -1.0438, -1.3453],\n         [ 0.7854,  0.7935,  0.5988, -1.5551, -0.3414],\n         [ 1.8530,  0.4681, -0.1577,  1.4437,  0.1835],\n         [ 1.3894,  1.5863,  0.9463, -0.8437,  0.9318],\n         [ 1.2590,  2.0050,  0.2484,  0.4397,  0.1124],\n         [ 0.6408,  0.4412, -0.2159, -0.7425,  0.5627]]),\n tensor([-1.3244,  1.0004, -0.2819,  0.2812,  3.7905,  4.0101,  4.0645,  0.6863]))"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "N, M = 100, 5\n",
    "BATCH_SIZE = 8\n",
    "X = torch.randn(N, M)\n",
    "y = X.sum(dim=-1)\n",
    "\n",
    "# Dataset: Slicing X, y simultaneously. \n",
    "dataset = TensorDataset(X, y)\n",
    "dataset[:BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 8\n1 8\n2 8\n3 8\n4 8\n5 8\n6 8\n7 8\n8 8\n9 8\n10 8\n11 8\n12 4\n"
    }
   ],
   "source": [
    "# DataLoader: Iterate over the dataset. \n",
    "# DO NOT drop the last samples by default. \n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "for i, (batch_X, batch_y) in enumerate(loader):\n",
    "    print(i, batch_X.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([[-0.1407,  0.8058,  0.3276, -0.7607, -1.5991],\n         [-1.3533,  1.2948,  1.4628, -0.6204,  0.9884],\n         [ 0.2800,  0.0732,  1.1133,  0.2823,  0.4544],\n         [ 0.2800,  0.0732,  1.1133,  0.2823,  0.4544],\n         [-0.8078,  1.1975, -1.3700,  1.5435, -0.0332],\n         [ 1.1910, -0.5899,  0.9647, -1.5094, -0.7595],\n         [-1.4583, -0.2817, -1.0300, -0.6884, -0.0335],\n         [-0.1073,  0.9985, -0.4987,  0.7611,  0.6183]]),\n tensor([-1.3671,  1.7722,  2.2032,  2.2032,  0.5301, -0.7030, -3.4918,  1.7718]))"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "sampler = RandomSampler(dataset, replacement=True, num_samples=BATCH_SIZE)\n",
    "dataset[list(sampler)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
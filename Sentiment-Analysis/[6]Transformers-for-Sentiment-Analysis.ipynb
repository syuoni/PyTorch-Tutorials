{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Sentiment Analysis\n",
    "This notebook follows this tutorial: https://github.com/bentrevett/pytorch-sentiment-analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "30522\n"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer.save_pretrained('./transformers_cache/bert-base-uncased/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./transformers_cache/bert-base-uncased/')\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['hello', 'world', 'how', 'are', 'you', '?']\n[7592, 2088, 2129, 2024, 2017, 1029]\n[101, 7592, 2088, 2129, 2024, 2017, 1029, 102]\n"
    }
   ],
   "source": [
    "# This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model.\n",
    "text = \"Hello WORLD how ARE yoU?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)\n",
    "\n",
    "indexes = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[CLS] [SEP] [PAD] [UNK]\n"
    }
   ],
   "source": [
    "# `cls_token`: The classifier token which is used when doing sequence classification (classification of the whole\n",
    "# sequence instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
    "init_token = tokenizer.cls_token\n",
    "# `sep_token`: The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences\n",
    "# for sequence classification or for a text and a question for question answering. It is also used as the last token of \n",
    "# a sequence built with special tokens.\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "101 102 0 100\n"
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "512\n"
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "print(max_input_length)\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "\n",
    "# `use_vocab`: Whether to use a Vocab object. If False, the data in this field should already be numerical.\n",
    "TEXT = Field(batch_first=True, use_vocab=False, \n",
    "             tokenize=tokenize_and_cut, preprocessing=tokenizer.convert_tokens_to_ids, \n",
    "             init_token=init_token_idx, eos_token=eos_token_idx, pad_token=pad_token_idx, unk_token=unk_token_idx,\n",
    "             include_lengths=True)\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL, root='data')\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2045, 2003, 1037, 2466, 1006, 4298, 9706, 10085, 2854, 21890, 2140, 1007, 2055, 2019, 3863, 2090, 5503, 12688, 1998, 6609]\n['there', 'is', 'a', 'story', '(', 'possibly', 'ap', '##oc', '##ry', '##pha', '##l', ')', 'about', 'an', 'exchange', 'between', 'bruce', 'willis', 'and', 'terry']\npos\n"
    }
   ],
   "source": [
    "# Note: The text has already been numericalized. \n",
    "print(train_data[0].text[:20])\n",
    "print(tokenizer.convert_ids_to_tokens(train_data[0].text[:20]))\n",
    "print(train_data[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, sort_within_batch=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  101,  1045,  3227,  ..., 14404,  1012,   102],\n        [  101,  2125, 19442,  ...,  1008,  1007,   102],\n        [  101,  2198, 22794,  ...,  2842,  1012,   102],\n        ...,\n        [  101,  2023,  2143,  ...,     0,     0,     0],\n        [  101,  2023,  2003,  ...,     0,     0,     0],\n        [  101,  2023,  2143,  ...,     0,     0,     0]], device='cuda:3')\ntensor([131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 130, 130,\n        130, 130, 130, 130, 130, 130, 130, 130, 129, 129, 129, 129, 129, 129,\n        129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 128,\n        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 127,\n        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,\n        126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126,\n        126, 126, 126, 126, 126, 126, 126, 125, 125, 125, 125, 125, 125, 125,\n        125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 124, 124, 124, 124,\n        124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124,\n        124, 124], device='cuda:3')\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    break\n",
    "text, text_lens = batch.text\n",
    "print(text)\n",
    "print(text_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "# bert.save_pretrained('./transformers_cache/bert-base-uncased/')\n",
    "bert = BertModel.from_pretrained('./transformers_cache/bert-base-uncased/').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 131])\ntorch.Size([128, 131, 768])\ntorch.Size([128, 768])\n"
    }
   ],
   "source": [
    "# Input: `input_ids`\n",
    "    # `input_ids`: (batch_size, sequence_length)\n",
    "# Output: (`last_hidden_state`, `pooler_output`)\n",
    "    # `last_hidden_state`: (batch_size, sequence_length, hidden_size)\n",
    "    # Sequence of hidden-states at the output of the last layer of the model.\n",
    "    # `pooler_output` (batch_size, hidden_size)\n",
    "    # Last layer hidden-state of the first token of the sequence (classification token)\n",
    "    # further processed by a Linear layer and a Tanh activation function. The Linear\n",
    "    # layer weights are trained from the next sentence prediction (classification)\n",
    "    # objective during pre-training. \n",
    "\n",
    "bert_outs, bert_pooled_outs = bert(batch.text[0])\n",
    "print(batch.text[0].size())\n",
    "print(bert_outs.size())\n",
    "print(bert_pooled_outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  101,  1045,  3227,  ..., 14404,  1012,   102],\n        [  101,  2125, 19442,  ...,  1008,  1007,   102],\n        [  101,  2198, 22794,  ...,  2842,  1012,   102],\n        ...,\n        [  101,  2023,  2143,  ...,     0,     0,     0],\n        [  101,  2023,  2003,  ...,     0,     0,     0],\n        [  101,  2023,  2143,  ...,     0,     0,     0]], device='cuda:3')\ntensor([[[ 2.2840e-01, -7.8270e-02,  7.5504e-02,  ..., -1.5385e-01,\n           6.9514e-01,  5.5175e-01],\n         [ 4.1330e-01, -1.6223e-01,  2.1426e-02,  ...,  4.3471e-02,\n           1.3220e+00,  3.5764e-01],\n         [-2.1609e-02, -3.3164e-02,  2.4292e-01,  ..., -1.8114e-01,\n           9.5946e-01,  5.3142e-01],\n         ...,\n         [-3.0407e-01,  3.3311e-01,  8.4997e-01,  ...,  4.5973e-02,\n          -4.1270e-02,  2.8242e-03],\n         [ 6.7624e-01,  3.7387e-01, -2.2169e-01,  ...,  7.6201e-02,\n          -4.2642e-01, -3.4363e-01],\n         [ 6.5550e-01,  7.9335e-01,  4.7385e-01,  ...,  3.5582e-01,\n           4.3942e-01, -1.8678e-01]],\n\n        [[ 3.1212e-01, -3.7655e-01,  2.6952e-01,  ..., -2.7812e-01,\n           7.4907e-01,  2.0235e-02],\n         [-3.6580e-01, -2.1610e-01,  1.6934e-02,  ...,  1.9811e-01,\n           9.9474e-01, -2.0388e-01],\n         [ 2.4771e-01, -2.0773e-01,  2.0288e-01,  ..., -3.1536e-01,\n           8.9033e-01, -5.4153e-01],\n         ...,\n         [ 7.7428e-01,  2.0459e-01,  6.0486e-01,  ...,  5.3782e-01,\n          -4.5975e-01,  9.9824e-02],\n         [ 2.3773e-01, -5.5331e-01,  6.5120e-01,  ..., -2.3560e-01,\n           1.2422e-01,  1.9643e-01],\n         [ 6.2429e-01,  2.1171e-01,  1.6856e-01,  ..., -6.2492e-02,\n           2.2111e-01, -4.7631e-01]],\n\n        [[ 1.4566e-01, -2.4896e-01,  3.0604e-02,  ..., -2.7637e-02,\n           3.7450e-01,  5.3023e-01],\n         [ 3.7417e-01, -5.9882e-02, -7.2972e-02,  ...,  3.1591e-01,\n           1.7445e-01, -7.8873e-01],\n         [ 7.1748e-01, -2.6484e-01,  2.2474e-01,  ...,  5.3682e-01,\n           2.1450e-01,  7.2157e-02],\n         ...,\n         [ 2.2579e-01, -3.2317e-01, -4.1938e-01,  ..., -3.1258e-01,\n           5.6036e-01, -3.4848e-01],\n         [ 6.1345e-01,  2.9029e-01, -2.9737e-01,  ...,  1.1271e-01,\n          -4.9535e-01, -3.2590e-01],\n         [ 6.2443e-01,  6.3614e-01,  4.4938e-01,  ...,  5.3111e-01,\n          -2.4724e-01, -2.5567e-02]],\n\n        ...,\n\n        [[-1.9393e-01, -1.2598e-01,  6.2694e-01,  ..., -4.2297e-01,\n           6.9880e-01,  1.6880e-01],\n         [-5.5754e-01,  1.6950e-03, -9.7466e-02,  ..., -6.2146e-01,\n           9.0946e-01,  4.3106e-01],\n         [ 2.6751e-01, -1.4542e-01, -2.7341e-02,  ..., -8.9880e-02,\n           2.3918e-01, -9.5312e-02],\n         ...,\n         [ 3.6828e-01, -1.5438e-01,  1.1981e+00,  ..., -7.6192e-01,\n           6.9088e-04, -7.3269e-01],\n         [ 2.5376e-01, -2.9915e-01,  1.1331e+00,  ..., -9.3092e-01,\n          -2.2553e-01, -6.6127e-01],\n         [ 3.3465e-01, -1.0768e-01,  1.3520e+00,  ..., -8.3464e-01,\n          -4.3145e-01, -6.5337e-01]],\n\n        [[-1.8061e-01, -3.8286e-01,  2.9679e-01,  ..., -7.7610e-02,\n           6.3945e-01,  2.3363e-01],\n         [-7.7804e-01, -1.9880e-01, -3.5138e-01,  ..., -3.1980e-01,\n           1.8031e+00,  6.8832e-01],\n         [-8.9142e-01, -1.4268e-01,  9.2099e-02,  ...,  1.0607e-01,\n           8.0182e-01,  8.7976e-01],\n         ...,\n         [ 7.8965e-02, -4.1151e-01,  1.0528e+00,  ..., -3.4586e-01,\n           8.4016e-02, -8.1477e-01],\n         [ 6.9090e-02, -4.8998e-01,  9.3180e-01,  ..., -4.6303e-01,\n          -1.0763e-01, -8.3199e-01],\n         [ 2.3101e-01, -3.2044e-01,  1.1125e+00,  ..., -5.0738e-01,\n          -2.9876e-01, -8.4490e-01]],\n\n        [[-3.7638e-01, -1.0717e-01,  1.0855e-01,  ..., -2.7029e-01,\n           9.6001e-01,  3.4835e-02],\n         [-2.4098e-01, -2.5605e-01, -8.7051e-03,  ..., -1.7663e-01,\n           1.7230e+00,  1.7410e-01],\n         [ 8.7892e-01,  1.4198e-01, -1.9821e-01,  ..., -1.1290e-01,\n           3.6788e-01,  3.0787e-01],\n         ...,\n         [ 4.6170e-01, -4.6541e-01,  1.1454e+00,  ..., -2.6988e-01,\n           1.6186e-01, -7.9433e-01],\n         [ 3.8778e-01, -5.6790e-01,  1.0484e+00,  ..., -3.8461e-01,\n          -4.4421e-03, -7.8997e-01],\n         [ 4.8185e-01, -3.0517e-01,  1.2303e+00,  ..., -4.7700e-01,\n          -2.3441e-01, -7.7372e-01]]], device='cuda:3',\n       grad_fn=<NativeLayerNormBackward>)\n"
    }
   ],
   "source": [
    "# The values at padding positions are NOT zeros?\n",
    "print(batch.text[0])\n",
    "print(bert_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n",
    "\n",
    "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it. The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert, hid_dim, out_dim, n_layers, bidirect, dropout):\n",
    "        super().__init__()\n",
    "        # Use `bert` to provide word embeddings. \n",
    "        self.bert = bert\n",
    "        emb_dim = bert.config.hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=n_layers, bidirectional=bidirect, batch_first=True, \n",
    "                          dropout=(0 if n_layers < 2 else dropout))\n",
    "        self.fc = nn.Linear((hid_dim*2 if bidirect else hid_dim), out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lens):\n",
    "        # text: (step, batch)\n",
    "        with torch.no_grad():\n",
    "            embedded, _ = self.bert(text)\n",
    "\n",
    "        # `<pad>` token: `bert.config.pad_token_id`\n",
    "        # Pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lens, batch_first=True)\n",
    "        # hidden: (num_layers*num_directions, batch, hid_dim)\n",
    "        packed_outs, hidden = self.rnn(packed_embedded)\n",
    "        # Unpack sequence, NOT used here. \n",
    "        # outs, out_lens = nn.utils.rnn.pad_packed_sequence(packed_outs, batch_first=True)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-2], hidden[-1]], dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "109482240\n112241409\n"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count trainable parameters. \n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "HID_DIM = 256\n",
    "OUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECT = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "classifier = Classifier(bert, HID_DIM, OUT_DIM, N_LAYERS, BIDIRECT, DROPOUT).to(device)\n",
    "\n",
    "print(count_parameters(bert))\n",
    "print(count_parameters(classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2759169\n"
    }
   ],
   "source": [
    "# Freeze BERT parameters (NOT train them). \n",
    "for name, param in classifier.named_parameters():\n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "print(count_parameters(classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters())\n",
    "# Binary cross entropy with logits. \n",
    "# The binary version of cross entropy loss. \n",
    "loss_func = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(classifier, iterator, optimizer, loss_func):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        text, text_lens = batch.text\n",
    "        preds = classifier(text, text_lens).squeeze(-1)\n",
    "        # Calculate loss\n",
    "        loss = loss_func(preds, batch.label)\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss and acc\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def eval_epoch(classifier, iterator, loss_func):\n",
    "    classifier.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            text, text_lens = batch.text\n",
    "            preds = classifier(text, text_lens).squeeze(-1)\n",
    "            # Calculate loss\n",
    "            loss = loss_func(preds, batch.label)\n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Epoch Time: 2m 33s\n\tTrain Loss: 0.415 | Train Acc: 80.85%\n\t Val. Loss: 0.303 |  Val. Acc: 88.03%\nEpoch: 02 | Epoch Time: 2m 33s\n\tTrain Loss: 0.249 | Train Acc: 90.04%\n\t Val. Loss: 0.230 |  Val. Acc: 91.18%\nEpoch: 03 | Epoch Time: 2m 33s\n\tTrain Loss: 0.214 | Train Acc: 91.44%\n\t Val. Loss: 0.217 |  Val. Acc: 91.24%\nEpoch: 04 | Epoch Time: 2m 33s\n\tTrain Loss: 0.187 | Train Acc: 92.95%\n\t Val. Loss: 0.224 |  Val. Acc: 91.38%\nEpoch: 05 | Epoch Time: 2m 33s\n\tTrain Loss: 0.155 | Train Acc: 94.25%\n\t Val. Loss: 0.222 |  Val. Acc: 91.96%\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_epoch(classifier, train_iterator, optimizer, loss_func)\n",
    "    valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(classifier.state_dict(), 'models/tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Val. Loss: 0.217 | Val. Acc: 91.24%\nTest Loss: 0.204 | Test Acc: 91.91%\n"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load('models/tut6-model.pt'))\n",
    "\n",
    "valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "test_loss, test_acc = eval_epoch(classifier, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_senti(classifier, tokenizer, sentence):\n",
    "    classifier.eval()\n",
    "    indexed = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    lens = len(indexed)\n",
    "\n",
    "    # Note: `batch_first=True`\n",
    "    indexed = torch.tensor(indexed, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    lens = torch.tensor(lens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    pred = torch.sigmoid(classifier(indexed, lens)).round().type(torch.long)\n",
    "    return LABEL.vocab.itos[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'pos'"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "predict_senti(classifier, tokenizer, \"This is a good film.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'neg'"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "predict_senti(classifier, tokenizer, \"This film is terrible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
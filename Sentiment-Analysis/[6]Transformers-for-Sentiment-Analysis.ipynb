{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Sentiment Analysis\n",
    "This notebook follows this tutorial: https://github.com/bentrevett/pytorch-sentiment-analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "30522\n"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer.save_pretrained('./transformers_cache/bert-base-uncased/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./transformers_cache/bert-base-uncased/')\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['hello', 'world', 'how', 'are', 'you', '?']\n[7592, 2088, 2129, 2024, 2017, 1029]\n[101, 7592, 2088, 2129, 2024, 2017, 1029, 102]\n"
    }
   ],
   "source": [
    "# This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model.\n",
    "text = \"Hello WORLD how ARE yoU?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)\n",
    "\n",
    "indexes = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[CLS] [SEP] [PAD] [UNK]\n"
    }
   ],
   "source": [
    "# `cls_token`: The classifier token which is used when doing sequence classification (classification of the whole\n",
    "# sequence instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
    "init_token = tokenizer.cls_token\n",
    "# `sep_token`: The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences\n",
    "# for sequence classification or for a text and a question for question answering. It is also used as the last token of \n",
    "# a sequence built with special tokens.\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "101 102 0 100\n"
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "512\n"
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "print(max_input_length)\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "\n",
    "# `use_vocab`: Whether to use a Vocab object. If False, the data in this field should already be numerical.\n",
    "TEXT = Field(batch_first=True, use_vocab=False, \n",
    "             tokenize=tokenize_and_cut, preprocessing=tokenizer.convert_tokens_to_ids, \n",
    "             init_token=init_token_idx, eos_token=eos_token_idx, pad_token=pad_token_idx, unk_token=unk_token_idx,\n",
    "             include_lengths=True)\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL, root='data')\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2061, 1045, 9357, 2006, 1996, 3617, 4942, 29234, 2099, 6833, 2028, 2305, 1037, 3232, 1997, 2086, 3283, 1998, 2245, 1045]\n['so', 'i', 'flipped', 'on', 'the', 'digital', 'sub', '##scribe', '##r', 'channels', 'one', 'night', 'a', 'couple', 'of', 'years', 'ago', 'and', 'thought', 'i']\npos\n"
    }
   ],
   "source": [
    "# Note: The text has already been numericalized. \n",
    "print(train_data[0].text[:20])\n",
    "print(tokenizer.convert_ids_to_tokens(train_data[0].text[:20]))\n",
    "print(train_data[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, sort_within_batch=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  101,  2178, 17151, 11741, 17743,  1010,  2023,  3972,  6961,  2046,\n          1996,  2088,  1997,  1996,  4242,  1998,  1996, 11189,  1010,  1998,\n          2009,  2515,  2200,  2092,  1012,  2009,  2987,  1005,  1056,  7001,\n          2000,  1996,  2502,  2569,  3896,  2058, 15872,  2066,  2137, 17312,\n          2015,  1010,  2009,  7679,  2062,  2006,  6832,  4254,  1012,  1037,\n          4659,  3722,  5436,  2008,  9423, 21025, 24700,  3240,  1004,  2522,\n          1012,  3288,  2000,  2166,  1012,  2009,  4076,  1996,  2466,  1997,\n          1037,  3232,  2040,  4965,  2019,  2214,  2160,  2008,  2001, 10743,\n          2188,  2000,  1037,  2200,  2214,  2450,  2040,  2196,  2253,  2648,\n          1010,  1998,  3005,  3129,  5419,  1999,  8075,  6214,  1037,  2301,\n          3283,  1012,  4326,  2477,  4088,  2000,  4148,  1999,  1996,  2160,\n          1010,  1998,  2198,  4205,  4269,  2000,  2735,  2046,  1996,  2158,\n          2040,  5419,  1010,  2040,  2001,  2941,  1037,  3742, 13422,  1012,\n          3811,  6749,  1012,  1022,  1013,  2184,   102],\n        [  101,  1037,  9996,  2517,  5896,  2007,  2053,  2066,  3085,  3494,\n          1012,  2004,  2005,  2009,  2108,  1037,  4038,  1010,  1045,  9471,\n          2000,  4756,  1012,  2009,  1005,  1055,  2055,  1016,  9530,  3401,\n         17572,  2814,  2040,  8040,  3286,  2000,  2131,  2308,  1999,  2205,\n          2793,  2007,  2068,  1006,  2053,  3348,  5019,  1007,  1998,  2178,\n          2767,  1006,  2040,  2003,  4100,  1011, 26047,  3436,  2135,  6881,\n          1007,  2040,  2823,  2036,  8040, 13596,  2021,  3701,  2003,  2641,\n          2004,  2108,  1996,  3124,  2040,  3040, 20179,  2015,  1012,  1996,\n          1017,  2814, 10329,  3113,  1998,  2991,  2005,  1996,  2168,  2450,\n          1006,  8282, 21392,  2102,  1007,  1012,  5064,  2023,  2003,  2589,\n          2302,  2428,  2151,  7472,  1012,  1996,  1017,  4364,  2644,  2108,\n          2814,  2004,  2027, 10329,  6052,  2014,  1012,  2016,  8040, 27479,\n          2068,  2041,  1997,  2037,  6860,  2138,  2027,  8040, 27479,  2308,\n          1012,  1011,  1011,  1037,  2919,  3185,   102],\n        [  101,  1996,  2878, 28988,  3185,  6907,  2038,  2000,  2022,  2081,\n          2039,  1997,  1996,  5409,  3152,  2412,  2081,  1012,  2023,  2028,\n         18058,  1037,  2843,  1997,  3554,  1010, 12382,  8310,  1997,  2668,\n          1010, 28988,  2015,  3554,  6505,  1010,  1998,  1037, 17137,  3723,\n          2237,  2008,  4152, 10676,  2039,  1998,  7950,  2091,  2028, 22200,\n          2012,  1037,  2051,  1012,  1996,  3772,  2003,  3458,  6659,  1012,\n          2054,  2412,  3047,  2000,  2728,  5232,  1010,  3781,  1012,  1029,\n          2012,  2028,  2391,  2002,  2001,  1999,  2070,  2350,  2996,  5453,\n          1010,  1998,  2059,  2002,  2074,  8105,  2185,  1012,  2023,  3185,\n          2428, 13783,  1010,  2021,  2065,  2017,  2031,  2025,  2464,  1037,\n         28988,  3185,  1999,  1037,  2146,  2051,  1010,  2009,  2003,  1037,\n          2204,  2028,  2000,  3422,  1012,  2012,  1996,  2203,  1997,  1996,\n          3185,  1010,  2017,  2323,  2514,  1037,  2978, 11669,  2100,  2005,\n          2383,  3427,  2009,   999,   102,     0,     0],\n        [  101,  1996,  2204,  3011,  2003,  3383,  1996,  2087, 11771,  2143,\n          1045,  1005,  2310,  2464,  1999,  2026,  2166,  1012,  1996,  5436,\n          2003,  4030,  1998, 11158,  1012,  1996,  3772,  2003,  3675,  4179,\n         29257,  1012,  2096,  1045,  2293,  2703, 14163,  3490,  1010,  1045,\n          2064,  2085,  2360,  1045,  2031,  2464,  1037,  2143,  2008,  2515,\n          2025,  2079,  2010,  2995,  3754,  3425,  1012,  1996,  2069,  7494,\n          4519,  1045,  2179,  2007,  2023,  2143,  2003,  2009,  1005,  1055,\n          2537,  3643,  1012,  1996,  2224,  1997,  5606,  1997, 26279,  2802,\n          1996,  2143,  9005,  1037,  2200, 19337,  2666, 12423,  1998,  5875,\n          4044,  1012,  2036,  1010,  1996,  3376,  3896,  2109,  2000,  3443,\n          1996, 12492,  1997,  8817,  1997, 23146, 15187,  4089,  1998,  2001,\n          6208,  2012,  1996,  2051,  1012,  2060,  2084,  1996,  2537,  3643,\n          1045,  2064,  2360,  2210,  2842,  2008,  2003,  2204,  2030, 14036,\n          2055,  2023,  2143,  1012,   102,     0,     0]])\ntensor([137, 137, 135, 135])\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    break\n",
    "text, text_lens = batch.text\n",
    "print(text)\n",
    "print(text_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "# bert.save_pretrained('./transformers_cache/bert-base-uncased/')\n",
    "bert = BertModel.from_pretrained('./transformers_cache/bert-base-uncased/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 137])\ntorch.Size([4, 137, 768])\ntorch.Size([4, 768])\n"
    }
   ],
   "source": [
    "# Input: `input_ids`\n",
    "    # `input_ids`: (batch_size, sequence_length)\n",
    "# Output: (`last_hidden_state`, `pooler_output`)\n",
    "    # `last_hidden_state`: (batch_size, sequence_length, hidden_size)\n",
    "    # Sequence of hidden-states at the output of the last layer of the model.\n",
    "    # `pooler_output` (batch_size, hidden_size)\n",
    "    # Last layer hidden-state of the first token of the sequence (classification token)\n",
    "    # further processed by a Linear layer and a Tanh activation function. The Linear\n",
    "    # layer weights are trained from the next sentence prediction (classification)\n",
    "    # objective during pre-training. \n",
    "\n",
    "bert_outs, bert_pooled_outs = bert(batch.text[0])\n",
    "print(batch.text[0].size())\n",
    "print(bert_outs.size())\n",
    "print(bert_pooled_outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  101,  2178, 17151, 11741, 17743,  1010,  2023,  3972,  6961,  2046,\n          1996,  2088,  1997,  1996,  4242,  1998,  1996, 11189,  1010,  1998,\n          2009,  2515,  2200,  2092,  1012,  2009,  2987,  1005,  1056,  7001,\n          2000,  1996,  2502,  2569,  3896,  2058, 15872,  2066,  2137, 17312,\n          2015,  1010,  2009,  7679,  2062,  2006,  6832,  4254,  1012,  1037,\n          4659,  3722,  5436,  2008,  9423, 21025, 24700,  3240,  1004,  2522,\n          1012,  3288,  2000,  2166,  1012,  2009,  4076,  1996,  2466,  1997,\n          1037,  3232,  2040,  4965,  2019,  2214,  2160,  2008,  2001, 10743,\n          2188,  2000,  1037,  2200,  2214,  2450,  2040,  2196,  2253,  2648,\n          1010,  1998,  3005,  3129,  5419,  1999,  8075,  6214,  1037,  2301,\n          3283,  1012,  4326,  2477,  4088,  2000,  4148,  1999,  1996,  2160,\n          1010,  1998,  2198,  4205,  4269,  2000,  2735,  2046,  1996,  2158,\n          2040,  5419,  1010,  2040,  2001,  2941,  1037,  3742, 13422,  1012,\n          3811,  6749,  1012,  1022,  1013,  2184,   102],\n        [  101,  1037,  9996,  2517,  5896,  2007,  2053,  2066,  3085,  3494,\n          1012,  2004,  2005,  2009,  2108,  1037,  4038,  1010,  1045,  9471,\n          2000,  4756,  1012,  2009,  1005,  1055,  2055,  1016,  9530,  3401,\n         17572,  2814,  2040,  8040,  3286,  2000,  2131,  2308,  1999,  2205,\n          2793,  2007,  2068,  1006,  2053,  3348,  5019,  1007,  1998,  2178,\n          2767,  1006,  2040,  2003,  4100,  1011, 26047,  3436,  2135,  6881,\n          1007,  2040,  2823,  2036,  8040, 13596,  2021,  3701,  2003,  2641,\n          2004,  2108,  1996,  3124,  2040,  3040, 20179,  2015,  1012,  1996,\n          1017,  2814, 10329,  3113,  1998,  2991,  2005,  1996,  2168,  2450,\n          1006,  8282, 21392,  2102,  1007,  1012,  5064,  2023,  2003,  2589,\n          2302,  2428,  2151,  7472,  1012,  1996,  1017,  4364,  2644,  2108,\n          2814,  2004,  2027, 10329,  6052,  2014,  1012,  2016,  8040, 27479,\n          2068,  2041,  1997,  2037,  6860,  2138,  2027,  8040, 27479,  2308,\n          1012,  1011,  1011,  1037,  2919,  3185,   102],\n        [  101,  1996,  2878, 28988,  3185,  6907,  2038,  2000,  2022,  2081,\n          2039,  1997,  1996,  5409,  3152,  2412,  2081,  1012,  2023,  2028,\n         18058,  1037,  2843,  1997,  3554,  1010, 12382,  8310,  1997,  2668,\n          1010, 28988,  2015,  3554,  6505,  1010,  1998,  1037, 17137,  3723,\n          2237,  2008,  4152, 10676,  2039,  1998,  7950,  2091,  2028, 22200,\n          2012,  1037,  2051,  1012,  1996,  3772,  2003,  3458,  6659,  1012,\n          2054,  2412,  3047,  2000,  2728,  5232,  1010,  3781,  1012,  1029,\n          2012,  2028,  2391,  2002,  2001,  1999,  2070,  2350,  2996,  5453,\n          1010,  1998,  2059,  2002,  2074,  8105,  2185,  1012,  2023,  3185,\n          2428, 13783,  1010,  2021,  2065,  2017,  2031,  2025,  2464,  1037,\n         28988,  3185,  1999,  1037,  2146,  2051,  1010,  2009,  2003,  1037,\n          2204,  2028,  2000,  3422,  1012,  2012,  1996,  2203,  1997,  1996,\n          3185,  1010,  2017,  2323,  2514,  1037,  2978, 11669,  2100,  2005,\n          2383,  3427,  2009,   999,   102,     0,     0],\n        [  101,  1996,  2204,  3011,  2003,  3383,  1996,  2087, 11771,  2143,\n          1045,  1005,  2310,  2464,  1999,  2026,  2166,  1012,  1996,  5436,\n          2003,  4030,  1998, 11158,  1012,  1996,  3772,  2003,  3675,  4179,\n         29257,  1012,  2096,  1045,  2293,  2703, 14163,  3490,  1010,  1045,\n          2064,  2085,  2360,  1045,  2031,  2464,  1037,  2143,  2008,  2515,\n          2025,  2079,  2010,  2995,  3754,  3425,  1012,  1996,  2069,  7494,\n          4519,  1045,  2179,  2007,  2023,  2143,  2003,  2009,  1005,  1055,\n          2537,  3643,  1012,  1996,  2224,  1997,  5606,  1997, 26279,  2802,\n          1996,  2143,  9005,  1037,  2200, 19337,  2666, 12423,  1998,  5875,\n          4044,  1012,  2036,  1010,  1996,  3376,  3896,  2109,  2000,  3443,\n          1996, 12492,  1997,  8817,  1997, 23146, 15187,  4089,  1998,  2001,\n          6208,  2012,  1996,  2051,  1012,  2060,  2084,  1996,  2537,  3643,\n          1045,  2064,  2360,  2210,  2842,  2008,  2003,  2204,  2030, 14036,\n          2055,  2023,  2143,  1012,   102,     0,     0]])\ntensor([[[-1.7359e-01, -5.2033e-01,  2.7581e-01,  ..., -1.4462e-02,\n           6.1759e-01,  9.1133e-02],\n         [-1.0069e-01,  2.5632e-03,  4.8250e-01,  ...,  1.9500e-01,\n           6.8012e-01,  3.7115e-01],\n         [-2.4368e-01,  3.8842e-01,  8.5267e-01,  ...,  7.7551e-02,\n          -2.5780e-01,  6.3124e-01],\n         ...,\n         [-3.6743e-01, -4.8628e-01,  1.8439e-01,  ...,  2.4265e-01,\n           8.1838e-03, -6.7754e-01],\n         [-6.9836e-01, -1.4516e-01,  1.3282e+00,  ..., -1.5765e-01,\n           2.1880e-01, -1.4604e+00],\n         [ 2.5759e-01,  1.1093e-01,  5.7027e-01,  ..., -6.2242e-02,\n          -1.4722e-01,  2.0966e-01]],\n\n        [[ 2.8337e-01, -4.2283e-01,  3.5022e-01,  ..., -3.2911e-01,\n           6.8706e-01,  4.0765e-01],\n         [-6.5830e-01,  3.8315e-01, -3.1671e-01,  ..., -2.0827e-01,\n           9.5797e-01,  9.6335e-01],\n         [-3.6252e-01,  3.1402e-01,  1.2631e-01,  ..., -1.1335e-01,\n           2.8945e-01,  1.8226e-01],\n         ...,\n         [ 5.0501e-01, -1.3572e-01,  1.9181e-01,  ..., -1.3386e-01,\n           7.6862e-01, -2.5434e-01],\n         [ 1.9332e-01, -2.4815e-01, -6.8278e-01,  ..., -1.3745e-01,\n           1.3141e-01, -4.1728e-01],\n         [ 3.7092e-01,  1.4724e-01,  5.6148e-01,  ...,  4.8457e-01,\n           1.2586e-04,  2.0256e-02]],\n\n        [[ 4.6494e-01, -3.6132e-01,  3.0940e-01,  ..., -4.5826e-01,\n           6.6099e-01,  1.4813e-01],\n         [-7.7935e-02, -3.1438e-01, -3.4185e-01,  ...,  4.5261e-02,\n           1.3040e+00,  2.7564e-01],\n         [ 5.0190e-01,  5.0134e-02,  1.5762e-01,  ..., -9.6311e-02,\n           7.0565e-01, -2.7854e-01],\n         ...,\n         [ 1.0572e+00, -2.1744e-01,  4.3467e-01,  ..., -1.2774e-01,\n           5.7588e-01, -3.0755e-01],\n         [ 4.6640e-01, -4.8538e-02,  8.5535e-01,  ...,  1.7799e-01,\n           1.2966e-01, -4.7421e-01],\n         [ 1.1250e-01, -4.7039e-01,  4.5927e-01,  ..., -1.3892e-01,\n           2.4824e-01, -3.3438e-01]],\n\n        [[ 2.9820e-01, -2.2596e-01,  2.8468e-01,  ..., -4.5273e-01,\n           4.2614e-01,  3.1918e-01],\n         [-1.5566e-01, -3.2632e-01, -6.5729e-01,  ...,  5.0115e-01,\n           1.4248e+00,  2.9713e-01],\n         [ 1.1258e-01,  3.7460e-01,  5.6407e-01,  ...,  2.1946e-01,\n           3.4130e-01, -6.9969e-01],\n         ...,\n         [ 4.0531e-01,  6.9494e-02,  3.1454e-01,  ...,  2.2290e-02,\n           3.7836e-02, -3.9771e-01],\n         [ 6.2331e-01, -2.0320e-01,  9.0403e-01,  ...,  3.2247e-01,\n           1.8380e-01, -3.6020e-01],\n         [ 1.4924e-01, -3.2600e-01,  5.7664e-01,  ..., -1.6044e-01,\n           6.8761e-02, -3.9545e-01]]], grad_fn=<NativeLayerNormBackward>)\n"
    }
   ],
   "source": [
    "# The values at padding positions are NOT zeros?\n",
    "print(batch.text[0])\n",
    "print(bert_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n",
    "\n",
    "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it. The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert, hid_dim, out_dim, n_layers, bidirect, dropout):\n",
    "        super().__init__()\n",
    "        # Use `bert` to provide word embeddings. \n",
    "        self.bert = bert\n",
    "        emb_dim = bert.config.hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=n_layers, bidirectional=bidirect, batch_first=True, \n",
    "                          dropout=(0 if n_layers < 2 else dropout))\n",
    "        self.fc = nn.Linear((hid_dim*2 if bidirect else hid_dim), out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lens):\n",
    "        # text: (step, batch)\n",
    "        with torch.no_grad():\n",
    "            embedded, _ = self.bert(text)\n",
    "\n",
    "        # `<pad>` token: `bert.config.pad_token_id`\n",
    "        # Pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lens, batch_first=True)\n",
    "        # hidden: (num_layers*num_directions, batch, hid_dim)\n",
    "        packed_outs, hidden = self.rnn(packed_embedded)\n",
    "        # Unpack sequence, NOT used here. \n",
    "        # outs, out_lens = nn.utils.rnn.pad_packed_sequence(packed_outs, batch_first=True)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-2], hidden[-1]], dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "109482240\n112241409\n"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count trainable parameters. \n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "HID_DIM = 256\n",
    "OUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECT = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "classifier = Classifier(bert, HID_DIM, OUT_DIM, N_LAYERS, BIDIRECT, DROPOUT).to(device)\n",
    "\n",
    "print(count_parameters(bert))\n",
    "print(count_parameters(classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2759169\n"
    }
   ],
   "source": [
    "# Freeze BERT parameters (NOT train them). \n",
    "for name, param in classifier.named_parameters():\n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "print(count_parameters(classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters())\n",
    "# Binary cross entropy with logits. \n",
    "# The binary version of cross entropy loss. \n",
    "loss_func = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(classifier, iterator, optimizer, loss_func):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        text, text_lens = batch.text\n",
    "        preds = classifier(text, text_lens).squeeze(-1)\n",
    "        # Calculate loss\n",
    "        loss = loss_func(preds, batch.label)\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss and acc\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def eval_epoch(classifier, iterator, loss_func):\n",
    "    classifier.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            text, text_lens = batch.text\n",
    "            preds = classifier(text, text_lens).squeeze(-1)\n",
    "            # Calculate loss\n",
    "            loss = loss_func(preds, batch.label)\n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_epoch(classifier, train_iterator, optimizer, loss_func)\n",
    "    valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(classifier.state_dict(), 'models/tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load('models/tut6-model.pt'))\n",
    "\n",
    "valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "test_loss, test_acc = eval_epoch(classifier, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_senti(classifier, tokenizer, sentence):\n",
    "    classifier.eval()\n",
    "    indexed = tokenizer.encode(text, add_special_tokens=True)\n",
    "    lens = len(indexed)\n",
    "\n",
    "    # Note: `batch_first=True`\n",
    "    indexed = torch.tensor(indexed, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    lens = torch.tensor(lens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    pred = torch.sigmoid(classifier(indexed, lens)).round().type(torch.long)\n",
    "    return LABEL.vocab.itos[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_senti(classifier, \"This is a good film.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predict_senti' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-dbaf5478323b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_senti\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"This film is terrible.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_senti' is not defined"
     ]
    }
   ],
   "source": [
    "predict_senti(classifier, \"This film is terrible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Sentiment Analysis\n",
    "This notebook follows this tutorial: https://github.com/bentrevett/pytorch-sentiment-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer.save_pretrained('./transformers_cache/bert-base-uncased/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./transformers_cache/bert-base-uncased/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "# bert.save_pretrained('./transformers_cache/bert-base-uncased/')\n",
    "bert = BertModel.from_pretrained('./transformers_cache/bert-base-uncased/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mSignature:\u001b[0m      \u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mType:\u001b[0m           BertModel\n\u001b[1;31mString form:\u001b[0m   \nBertModel(\n           (embeddings): BertEmbeddings(\n           (word_embeddings): Embedding(30522, 768, padding_i <...>\n           (dense): Linear(in_features=768, out_features=768, bias=True)\n           (activation): Tanh()\n           )\n           )\n\u001b[1;31mFile:\u001b[0m           e:\\anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\n\u001b[1;31mDocstring:\u001b[0m     \nThe bare Bert Model transformer outputting raw hidden-states without any specific head on top.\nThis model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ sub-class.\nUse it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general\nusage and behavior.\n\nParameters:\n    config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n        Initializing with a config file does not load the weights associated with the model, only the configuration.\n        Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n\n\nThe model can behave as an encoder (with only self-attention) as well\nas a decoder, in which case a layer of cross-attention is added between\nthe self-attention layers, following the architecture described in `Attention is all you need`_ by Ashish Vaswani,\nNoam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n\nTo behave as an decoder the model needs to be initialized with the\n:obj:`is_decoder` argument of the configuration set to :obj:`True`; an\n:obj:`encoder_hidden_states` is expected as an input to the forward pass.\n\n.. _`Attention is all you need`:\n    https://arxiv.org/abs/1706.03762\n\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n"
    }
   ],
   "source": [
    "bert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(5, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([8, 5, 3, 3])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "conv2d.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([ 0.1002, -0.0894,  0.1367,  0.1267, -0.0163,  0.0320,  0.1034,  0.1077],\n       requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "conv2d.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'film',\n 'is',\n 'the',\n 'best',\n 'the best',\n 'film is',\n 'This film',\n 'is the']"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "def append_bigrams(x):\n",
    "    bigrams = set([\"%s %s\" % (f, s) for f, s in zip(x[:-1], x[1:])])\n",
    "    x.extend(list(bigrams))\n",
    "    return x\n",
    "\n",
    "append_bigrams(\"This film is the best\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "\n",
    "TEXT = Field(tokenize='spacy', tokenizer_language='en_core_web_sm', preprocessing=append_bigrams, include_lengths=True)\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL, root=\"../assets/data\")\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE, \n",
    "                 vectors=\"glove.6B.100d\", vectors_cache=\"../assets/vector_cache\", \n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['There', 'is', 'a', 'story', '(', 'possibly', 'apocryphal', ')', 'about', 'an', 'exchange', 'between', 'Bruce', 'Willis', 'and', 'Terry', 'Gilliam', 'at', 'the', 'start', 'of', 'Twelve', 'Monkeys', '.', 'Gilliam', '(', 'allegedly', ')', 'produced', 'a', 'long', 'list', '(', 'think', 'about', 'the', 'aircraft', 'one', 'from', 'the', 'Fifth', 'Element', ')', 'and', 'handed', 'it', 'to', 'Butch', 'Bruce', '.', 'It', 'was', 'entitled', '\"', 'Things', 'Bruce', 'Willis', 'Does', 'When', 'He', 'Acts', '\"', '.', 'It', 'ended', 'with', 'a', 'simple', 'message', 'saying', ':', '\"', 'please', 'do', \"n't\", 'do', 'any', 'of', 'the', 'above', 'in', 'my', 'movie\".<br', '/><br', '/>There', 'is', 'a', 'fact', 'about', 'this', 'movie', '(', 'definitely', 'true', ')', '.', 'Gilliam', 'did', \"n't\", 'have', 'a', 'hand', 'in', 'the', 'writing.<br', '/><br', '/>I', 'would', 'contend', 'that', 'these', 'two', 'factors', 'played', 'a', 'huge', 'role', 'in', 'creating', 'the', 'extraordinary', '(', 'if', 'not', 'commercial', ')', 'success', 'that', 'is', 'The', 'Twelve', 'Monkeys.<br', '/><br', '/>Visually', ',', 'the', 'Twelve', 'Monkeys', 'is', 'all', 'that', 'we', 'have', 'rightly', 'come', 'to', 'expect', 'from', 'a', 'Gilliam', 'film', '.', 'It', 'is', 'also', 'full', 'of', 'Gilliamesque', 'surrealism', 'and', 'general', '(', 'but', 'magnificent', ')', 'strangeness', '.', 'Gilliam', 'delights', 'in', 'wrong', '-', 'footing', 'his', 'audience', '.', 'Although', 'the', 'ending', 'of', 'the', 'Twelve', 'Monkeys', 'will', 'surprise', 'no', 'one', 'who', 'has', 'sat', 'through', 'the', 'first', 'real', ',', 'Gilliam', 'borrows', 'heavily', 'from', 'Kafka', 'in', 'the', 'clockwork', ',', 'bureaucratic', 'relentless', 'movement', 'of', 'the', 'characters', 'towards', 'their', 'fate', '.', 'It', 'is', 'this', 'journey', ',', 'and', 'the', 'character', 'developments', 'they', 'undergo', ',', 'which', 'unsettles.<br', '/><br', '/>I', 'love', 'Gilliam', 'films', '(', 'Brazil', ',', 'in', 'particular', ')', '.', 'But', 'they', 'do', 'all', 'tend', 'to', 'suffer', 'from', 'the', 'same', 'weakness', '.', 'He', 'seems', 'to', 'have', 'so', 'many', 'ideas', ',', 'and', 'so', 'much', 'enthusiasm', ',', 'that', 'his', 'films', 'almost', 'invariably', 'end', 'up', 'as', 'a', 'tangled', 'mess', '(', 'Brazil', ',', 'in', 'particular', ')', '.', 'I', 'still', 'maintain', 'that', 'Brazil', 'is', 'Gilliam', \"'s\", 'tour', 'de', 'force', ',', 'but', 'there', \"'s\", 'no', 'denying', 'that', 'The', 'Twelve', 'Monkey', \"'s\", 'is', 'a', 'breath', 'of', 'fresh', 'air', 'in', 'the', 'tight', '-', 'plotting', 'department', '.', 'Style', ',', 'substance', 'and', 'form', 'seem', 'to', 'merge', 'in', 'a', 'way', 'not', 'usually', 'seen', 'from', 'the', 'ex', '-', 'Python.<br', '/><br', '/>Whatever', 'the', 'truth', 'of', 'the', 'rumour', 'above', ',', 'Gilliam', 'also', 'manages', 'to', 'get', 'a', 'first', 'rate', '(', 'and', 'very', 'atypical', ')', 'performance', 'out', 'of', 'the', 'bald', 'one', '.', 'Bruce', 'is', 'excellent', 'in', 'this', 'film', ',', 'as', 'are', 'all', 'the', 'cast', ',', 'particularly', 'a', 'suitably', 'bonkers', '-', 'and', 'very', 'scary', '-', 'Brad', 'Pitt.<br', '/><br', '/>It', \"'s\", 'been', 'over', 'a', 'decade', 'since', 'this', 'film', 'was', 'released', '.', 'When', 'I', 'watched', 'it', 'again', ',', 'I', 'realised', 'that', 'it', 'had', \"n't\", 'really', 'aged', '.', 'I', 'had', 'changed', ',', 'of', 'course', '.', 'And', 'this', 'made', 'me', 'look', 'at', 'the', 'film', 'with', 'fresh', 'eyes', '.', 'This', 'seems', 'to', 'me', 'to', 'be', 'a', 'fitting', 'tribute', 'to', 'a', 'film', 'that', ',', 'partly', 'at', 'least', ',', 'is', 'about', 'reflections', 'in', 'mirrors', ',', 'altered', 'perspectives', 'and', 'the', 'absurd', 'one', '-', 'way', 'journey', 'through', 'time', 'that', 'we', 'all', 'make', '.', 'A', 'first', 'rate', 'film', '.', '8/10', '.', ', but', 'films (', 'of the', 'entitled \"', '( but', 'a film', 'movie (', 'way not', 'the same', 'a Gilliam', 'de force', 'particularly a', '. I', 'expect from', 'in creating', 'through the', 'Gilliam film', 'a first', '\" Things', 'the above', \"'s been\", 'hand in', 'And this', 'aged .', 'that we', 'and handed', 'changed ,', 'make .', 'the ending', 'and form', 'of Gilliamesque', '. But', ', is', 'surprise no', \"do n't\", 'real ,', 'delights in', 'a simple', 'rate film', 'played a', 'fact about', 'a long', 'is The', ') success', 'so many', 'it to', 'think about', 'in my', 'Gilliam also', 'be a', 'been over', 'a story', 'that it', '\" please', '/>I love', 'in a', 'not usually', 'but magnificent', 'from a', 'Kafka in', 'I watched', '( if', 'Willis and', 'general (', 'this movie', '/>Visually ,', 'exchange between', 'altered perspectives', 'they do', ') about', ', particularly', 'my movie\".<br', 'the writing.<br', 'wrong -', 'character developments', 'air in', 'films almost', 'between Bruce', 'Monkeys will', 'seems to', 'the extraordinary', 'is all', \"Gilliam 's\", 'journey ,', 'have a', 'as a', 'realised that', 'one from', 'rightly come', 'Monkeys .', 'a breath', 'Twelve Monkey', 'to me', \"n't do\", 'almost invariably', ', and', 'least ,', 'weakness .', ', that', '/>I would', 'film that', 'Element )', 'He Acts', 'particular )', ', Gilliam', 'since this', 'When I', '. Although', 'will surprise', 'these two', 'Gilliam (', 'who has', 'film .', 'again ,', 'towards their', 'movement of', 'is this', 'Butch Bruce', 'tour de', 'Bruce is', 'the rumour', 'the aircraft', '/>There is', 'that his', 'It is', 'borrows heavily', 'was entitled', 'have so', 'a fitting', 'form seem', 'bureaucratic relentless', 'it again', 'strangeness .', 'mess (', 'do all', 'The Twelve', 'film with', 'to have', 'have rightly', 'his films', 'is about', 'of course', 'But they', 'watched it', '. Gilliam', \"'s no\", ', altered', 'heavily from', 'denying that', 'manages to', 'decade since', 'This seems', 'at least', 'eyes .', ': \"', 'over a', '\" .', 'huge role', '( possibly', 'ended with', 'from Kafka', 'end up', 'perspectives and', 'developments they', ', substance', 'message saying', 'enthusiasm ,', 'to get', 'and Terry', 'released .', ') .', 'a decade', 'a fact', 'invariably end', 'in this', 'allegedly )', 'cast ,', 'Brazil ,', 'clockwork ,', 'in the', '/><br />Whatever', '/>Whatever the', 'to expect', 'Brad Pitt.<br', 'performance out', 'about reflections', 'Acts \"', 'seem to', 'Bruce .', 'Gilliam borrows', 'all tend', 'atypical )', 'aircraft one', 'first real', ', which', 'surrealism and', 'one -', 'so much', 'Although the', 'me to', 'and very', 'Things Bruce', 'and the', 'is a', 'Fifth Element', 'the first', 'Style ,', 'factors played', 'truth of', '- way', 'story (', ') produced', '. 8/10', 'usually seen', 'with fresh', 'journey through', 'possibly apocryphal', 'all make', ', as', 'plotting department', '. A', 'the Twelve', 'still maintain', 'we all', 'this film', 'reflections in', 'breath of', 'is also', 'extraordinary (', 'to a', 'partly at', ', I', '/><br />I', 'definitely true', 'an exchange', 'a hand', 'fresh air', 'rumour above', 'I still', 'bald one', '( think', 'the start', 'the clockwork', 'He seems', 'a huge', '- Python.<br', \"'s tour\", 'There is', 'to merge', 'Bruce Willis', 'A first', 'Monkeys.<br /><br', 'get a', '. And', 'the character', 'rate (', 'but there', 'bonkers -', 'saying :', 'that is', 'any of', '. Style', 'two factors', 'out of', '. It', 'the truth', 'a suitably', 'many ideas', 'success that', 'above in', '. This', 'film was', 'that ,', 'full of', 'in particular', 'handed it', 'with a', 'I realised', 'force ,', 'me look', 'Brazil is', 'and so', 'at the', 'department .', 'tribute to', 'substance and', 'the absurd', 'seen from', 'audience .', 'tight -', 'and general', \"did n't\", 'ex -', 'about the', 'up as', 'as are', 'Does When', 'When He', 'footing his', 'come to', 'the film', '8/10 .', 'really aged', 'Gilliam did', ') strangeness', 'are all', 'simple message', 'It ended', '/><br />It', 'it had', 'relentless movement', 'love Gilliam', 'fate .', '- footing', 'Monkeys is', 'that these', 'maintain that', 'that Brazil', 'Gilliam at', 'scary -', 'about an', 'excellent in', 'in mirrors', 'one who', 'mirrors ,', \"Monkey 's\", 'they undergo', 'his audience', 'characters towards', '- plotting', 'movie\".<br /><br', 'I had', 'no denying', ', partly', 'about this', ', in', 'true )', \"n't have\", 'contend that', 'start of', 'tangled mess', 'course .', 'Gilliam films', 'Pitt.<br /><br', ', the', ') performance', 'is Gilliam', 'commercial )', '( definitely', 'do any', 'look at', 'much enthusiasm', \"had n't\", 'suffer from', \"'s is\", 'Twelve Monkeys', '( allegedly', 'fresh eyes', 'Gilliamesque surrealism', 'to be', 'creating the', 'made me', 'sat through', 'the cast', 'is excellent', 'through time', 'time that', 'same weakness', ', of', 'produced a', 'the characters', 'Twelve Monkeys.<br', 'the ex', 'was released', 'a way', 'the bald', 'also full', 'of Twelve', 'undergo ,', 'It was', 'please do', 'a tangled', 'of fresh', 'had changed', 'this made', '. Bruce', 'would contend', 'role in', 'unsettles.<br /><br', 'their fate', 'not commercial', 'Willis Does', 'we have', 'in wrong', 'Terry Gilliam', 'one .', '. He', 'merge in', 'above ,', 'which unsettles.<br', 'no one', \"/>It 's\", '. When', '- and', 'to suffer', '/><br />Visually', \"there 's\", 'list (', 'long list', 'fitting tribute', ') and', 'magnificent )', 'very atypical', 'that The', 'the Fifth', '/><br />There', 'all the', 'suitably bonkers', 'film ,', 'the tight', 'from the', 'this journey', 'very scary', 'all that', 'Python.<br /><br', 'tend to', '( Brazil', 'if not', 'first rate', 'ending of', 'has sat', 'apocryphal )', 'also manages', 'absurd one', 'way journey', \"n't really\", 'Gilliam delights', '( and', ', bureaucratic', '- Brad', 'to Butch', 'ideas ,', 'writing.<br /><br']\n['kill a', 'killed off', 'knew I', 'know ?', 'know and', 'know much', 'lady in', 'last minute', 'laughing .', 'learn about']\n"
    }
   ],
   "source": [
    "# The bi-grams are included in the `Dataset`.\n",
    "print(train_data[0].text)\n",
    "# The bi-grams are also included in the `TEXT.vocab`.\n",
    "print(TEXT.vocab.itos[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  452, 11558,    11,  ..., 20269,    56,  1003],\n        [  111,    67,    19,  ...,     3,    15,    48],\n        [   83,     2,  3042,  ...,    16,   199,    13],\n        ...,\n        [    1,     1,     1,  ...,     1,     1,     1],\n        [    1,     1,     1,  ...,     1,     1,     1],\n        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:0')\ntensor([ 457,  864,  503, 1190, 1293,  512,  989,  330,  309,  842, 1298, 1140,\n         192,  291,  588,  256,  572,  568,  379,  327, 1466,  430,  402,  460,\n         494,  515,  347, 1174,  107,  342, 2459,  358,  154,  297,  516,  741,\n         198,  407,  171,  234,  278, 1273,  855,  777,  657,  420,  539,  573,\n         344,  506,  224,  231,  309,  309,  460, 1182,  221,  408,  179,  677,\n         341,  660,  274,  240], device='cuda:0')\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    break\n",
    "text, text_lens = batch.text\n",
    "print(text)\n",
    "print(text_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim, out_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(emb_dim, out_dim)\n",
    "\n",
    "    def forward(self, text, text_lens):\n",
    "        # text: (step, batch)\n",
    "        embedded = self.emb(text)\n",
    "        \n",
    "        # Pooling along steps\n",
    "        # pooled: (batch, emb)\n",
    "        pooled = embedded.sum(dim=0) / text_lens.unsqueeze(1)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2500301"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "IN_DIM = len(TEXT.vocab)\n",
    "EMB_DIM = 100\n",
    "OUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "classifier = Classifier(IN_DIM, EMB_DIM, OUT_DIM, PAD_IDX).to(device)\n",
    "count_parameters(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344],\n        [-0.1077,  0.1105,  0.5981, -0.5436,  0.6740,  0.1066,  0.0389,  0.3548],\n        [-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "# Initialize Embeddings with Pre-Trained Vectors\n",
    "classifier.emb.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "classifier.emb.weight.data[UNK_IDX].zero_()\n",
    "classifier.emb.weight.data[PAD_IDX].zero_()\n",
    "\n",
    "print(classifier.emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters())\n",
    "# Binary cross entropy with logits. \n",
    "# The binary version of cross entropy loss. \n",
    "loss_func = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(classifier, iterator, optimizer, loss_func):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        text, text_lens = batch.text\n",
    "        preds = classifier(text, text_lens).squeeze(-1)\n",
    "        # Calculate loss\n",
    "        loss = loss_func(preds, batch.label)\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss and acc\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def eval_epoch(classifier, iterator, loss_func):\n",
    "    classifier.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            text, text_lens = batch.text\n",
    "            preds = classifier(text, text_lens).squeeze(-1)\n",
    "            # Calculate loss\n",
    "            loss = loss_func(preds, batch.label)\n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Epoch Time: 0m 8s\n\tTrain Loss: 0.670 | Train Acc: 67.51%\n\t Val. Loss: 0.629 |  Val. Acc: 76.70%\nEpoch: 02 | Epoch Time: 0m 7s\n\tTrain Loss: 0.553 | Train Acc: 80.55%\n\t Val. Loss: 0.484 |  Val. Acc: 83.57%\nEpoch: 03 | Epoch Time: 0m 7s\n\tTrain Loss: 0.413 | Train Acc: 86.61%\n\t Val. Loss: 0.386 |  Val. Acc: 86.27%\nEpoch: 04 | Epoch Time: 0m 7s\n\tTrain Loss: 0.327 | Train Acc: 89.30%\n\t Val. Loss: 0.335 |  Val. Acc: 87.45%\nEpoch: 05 | Epoch Time: 0m 7s\n\tTrain Loss: 0.274 | Train Acc: 90.97%\n\t Val. Loss: 0.308 |  Val. Acc: 87.72%\nEpoch: 06 | Epoch Time: 0m 7s\n\tTrain Loss: 0.237 | Train Acc: 92.23%\n\t Val. Loss: 0.289 |  Val. Acc: 88.66%\nEpoch: 07 | Epoch Time: 0m 7s\n\tTrain Loss: 0.208 | Train Acc: 93.27%\n\t Val. Loss: 0.278 |  Val. Acc: 89.11%\nEpoch: 08 | Epoch Time: 0m 7s\n\tTrain Loss: 0.185 | Train Acc: 94.07%\n\t Val. Loss: 0.274 |  Val. Acc: 88.73%\nEpoch: 09 | Epoch Time: 0m 7s\n\tTrain Loss: 0.165 | Train Acc: 94.80%\n\t Val. Loss: 0.266 |  Val. Acc: 89.16%\nEpoch: 10 | Epoch Time: 0m 7s\n\tTrain Loss: 0.147 | Train Acc: 95.55%\n\t Val. Loss: 0.262 |  Val. Acc: 89.28%\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_epoch(classifier, train_iterator, optimizer, loss_func)\n",
    "    valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(classifier.state_dict(), \"models/tut3-model.pt\")\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Val. Loss: 0.262 | Val. Acc: 89.28%\nTest Loss: 0.267 | Test Acc: 89.20%\n"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"models/tut3-model.pt\"))\n",
    "\n",
    "valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "test_loss, test_acc = eval_epoch(classifier, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_senti(classifier, sentence):\n",
    "    classifier.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[tok] for tok in tokenized]\n",
    "    lens = len(indexed)\n",
    "\n",
    "    indexed = torch.tensor(indexed, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    lens = torch.tensor(lens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    pred = torch.sigmoid(classifier(indexed, lens)).round().type(torch.long)\n",
    "    return LABEL.vocab.itos[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'pos'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "predict_senti(classifier, \"This is a good film.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'neg'"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "predict_senti(classifier, \"This film is terrible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Embeddings\n",
    "* The Embeddings of `<unk>` and `<pad>` tokens\n",
    "    * Because the `padding_idx` has been passed to `nn.Embedding`, so the `<pad>` embedding will remain zeros throughout training.  \n",
    "    * While the `<unk>` embedding will be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.0193,  0.0180, -0.0278, -0.0152, -0.0125,  0.0173, -0.0228, -0.0650],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0386, -0.3228,  0.7869, -0.3218,  0.1682, -0.0512, -0.3133,  0.3825],\n        [-0.0197,  0.0214,  0.6639, -0.4545,  0.7706,  0.0123,  0.1291,  0.4184],\n        [-0.3219,  0.1918,  0.4686, -0.6305, -0.3675, -0.0318,  0.1917,  0.1707]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "print(classifier.emb.weight[:5, :8])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
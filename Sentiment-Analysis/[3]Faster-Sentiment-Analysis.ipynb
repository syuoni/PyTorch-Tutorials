{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'film',\n 'is',\n 'the',\n 'best',\n 'film is',\n 'This film',\n 'is the',\n 'the best']"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "def append_bigrams(x):\n",
    "    bigrams = set([\"%s %s\" % (f, s) for f, s in zip(x[:-1], x[1:])])\n",
    "    x.extend(list(bigrams))\n",
    "    return x\n",
    "\n",
    "append_bigrams(\"This film is the best\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "\n",
    "TEXT = Field(tokenize='spacy', tokenizer_language='en_core_web_sm', preprocessing=append_bigrams, include_lengths=True)\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL, root=\"../assets/data\")\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE, \n",
    "                 vectors=\"glove.6B.100d\", vectors_cache=\"../assets/vector_cache\", \n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['There', 'is', 'a', 'story', '(', 'possibly', 'apocryphal', ')', 'about', 'an', 'exchange', 'between', 'Bruce', 'Willis', 'and', 'Terry', 'Gilliam', 'at', 'the', 'start', 'of', 'Twelve', 'Monkeys', '.', 'Gilliam', '(', 'allegedly', ')', 'produced', 'a', 'long', 'list', '(', 'think', 'about', 'the', 'aircraft', 'one', 'from', 'the', 'Fifth', 'Element', ')', 'and', 'handed', 'it', 'to', 'Butch', 'Bruce', '.', 'It', 'was', 'entitled', '\"', 'Things', 'Bruce', 'Willis', 'Does', 'When', 'He', 'Acts', '\"', '.', 'It', 'ended', 'with', 'a', 'simple', 'message', 'saying', ':', '\"', 'please', 'do', \"n't\", 'do', 'any', 'of', 'the', 'above', 'in', 'my', 'movie\".<br', '/><br', '/>There', 'is', 'a', 'fact', 'about', 'this', 'movie', '(', 'definitely', 'true', ')', '.', 'Gilliam', 'did', \"n't\", 'have', 'a', 'hand', 'in', 'the', 'writing.<br', '/><br', '/>I', 'would', 'contend', 'that', 'these', 'two', 'factors', 'played', 'a', 'huge', 'role', 'in', 'creating', 'the', 'extraordinary', '(', 'if', 'not', 'commercial', ')', 'success', 'that', 'is', 'The', 'Twelve', 'Monkeys.<br', '/><br', '/>Visually', ',', 'the', 'Twelve', 'Monkeys', 'is', 'all', 'that', 'we', 'have', 'rightly', 'come', 'to', 'expect', 'from', 'a', 'Gilliam', 'film', '.', 'It', 'is', 'also', 'full', 'of', 'Gilliamesque', 'surrealism', 'and', 'general', '(', 'but', 'magnificent', ')', 'strangeness', '.', 'Gilliam', 'delights', 'in', 'wrong', '-', 'footing', 'his', 'audience', '.', 'Although', 'the', 'ending', 'of', 'the', 'Twelve', 'Monkeys', 'will', 'surprise', 'no', 'one', 'who', 'has', 'sat', 'through', 'the', 'first', 'real', ',', 'Gilliam', 'borrows', 'heavily', 'from', 'Kafka', 'in', 'the', 'clockwork', ',', 'bureaucratic', 'relentless', 'movement', 'of', 'the', 'characters', 'towards', 'their', 'fate', '.', 'It', 'is', 'this', 'journey', ',', 'and', 'the', 'character', 'developments', 'they', 'undergo', ',', 'which', 'unsettles.<br', '/><br', '/>I', 'love', 'Gilliam', 'films', '(', 'Brazil', ',', 'in', 'particular', ')', '.', 'But', 'they', 'do', 'all', 'tend', 'to', 'suffer', 'from', 'the', 'same', 'weakness', '.', 'He', 'seems', 'to', 'have', 'so', 'many', 'ideas', ',', 'and', 'so', 'much', 'enthusiasm', ',', 'that', 'his', 'films', 'almost', 'invariably', 'end', 'up', 'as', 'a', 'tangled', 'mess', '(', 'Brazil', ',', 'in', 'particular', ')', '.', 'I', 'still', 'maintain', 'that', 'Brazil', 'is', 'Gilliam', \"'s\", 'tour', 'de', 'force', ',', 'but', 'there', \"'s\", 'no', 'denying', 'that', 'The', 'Twelve', 'Monkey', \"'s\", 'is', 'a', 'breath', 'of', 'fresh', 'air', 'in', 'the', 'tight', '-', 'plotting', 'department', '.', 'Style', ',', 'substance', 'and', 'form', 'seem', 'to', 'merge', 'in', 'a', 'way', 'not', 'usually', 'seen', 'from', 'the', 'ex', '-', 'Python.<br', '/><br', '/>Whatever', 'the', 'truth', 'of', 'the', 'rumour', 'above', ',', 'Gilliam', 'also', 'manages', 'to', 'get', 'a', 'first', 'rate', '(', 'and', 'very', 'atypical', ')', 'performance', 'out', 'of', 'the', 'bald', 'one', '.', 'Bruce', 'is', 'excellent', 'in', 'this', 'film', ',', 'as', 'are', 'all', 'the', 'cast', ',', 'particularly', 'a', 'suitably', 'bonkers', '-', 'and', 'very', 'scary', '-', 'Brad', 'Pitt.<br', '/><br', '/>It', \"'s\", 'been', 'over', 'a', 'decade', 'since', 'this', 'film', 'was', 'released', '.', 'When', 'I', 'watched', 'it', 'again', ',', 'I', 'realised', 'that', 'it', 'had', \"n't\", 'really', 'aged', '.', 'I', 'had', 'changed', ',', 'of', 'course', '.', 'And', 'this', 'made', 'me', 'look', 'at', 'the', 'film', 'with', 'fresh', 'eyes', '.', 'This', 'seems', 'to', 'me', 'to', 'be', 'a', 'fitting', 'tribute', 'to', 'a', 'film', 'that', ',', 'partly', 'at', 'least', ',', 'is', 'about', 'reflections', 'in', 'mirrors', ',', 'altered', 'perspectives', 'and', 'the', 'absurd', 'one', '-', 'way', 'journey', 'through', 'time', 'that', 'we', 'all', 'make', '.', 'A', 'first', 'rate', 'film', '.', '8/10', '.', '. Bruce', 'much enthusiasm', '/><br />Visually', 'But they', 'absurd one', '( but', \"'s no\", 'bald one', '. A', 'apocryphal )', \"did n't\", 'atypical )', 'will surprise', 'has sat', 'There is', 'the absurd', 'films almost', 'tight -', ', the', 'to a', 'way journey', 'was entitled', 'the rumour', 'all make', ') .', 'first rate', 'Gilliam delights', 'mess (', 'Gilliam also', 'rate film', 'so many', 'Twelve Monkeys.<br', 'and form', 'of Twelve', 'Fifth Element', '. But', 'This seems', 'to me', '/><br />Whatever', 'the ending', 'a film', 'force ,', \"had n't\", 'made me', 'is The', 'merge in', '. Although', 'Bruce .', 'of Gilliamesque', 'over a', 'Bruce is', 'tribute to', ') performance', 'is a', 'released .', 'seem to', 'a way', 'movie (', '- footing', 'up as', ', substance', 'are all', 'look at', 'a tangled', 'Twelve Monkey', 'sat through', 'but there', 'as are', 'with fresh', 'suitably bonkers', 'all that', 'magnificent )', 'changed ,', 'to Butch', 'not commercial', 'unsettles.<br /><br', '. 8/10', 'about this', 'a hand', 'a long', 'that we', 'one who', 'me to', '- plotting', 'developments they', '- Brad', '. Gilliam', \"n't do\", 'these two', 'first real', 'the tight', 'I still', 'is excellent', 'rumour above', 'no one', ', I', 'would contend', 'journey through', 'in creating', 'Terry Gilliam', 'come to', '/>Visually ,', 'cast ,', 'ending of', 'possibly apocryphal', 'between Bruce', 'invariably end', \"there 's\", 'usually seen', 'in particular', 'please do', '( Brazil', '( think', 'fate .', 'been over', 'the same', 'It was', 'of the', 'The Twelve', 'a suitably', 'bonkers -', 'clockwork ,', 'and Terry', 'to suffer', '/><br />I', 'Although the', '\" .', 'tend to', 'Gilliam did', 'get a', '\" please', 'do any', 'character developments', 'truth of', 'same weakness', 'success that', 'saying :', ') success', 'in mirrors', 'particular )', 'is this', ', partly', 'Brad Pitt.<br', 'full of', 'tangled mess', 'a simple', ', bureaucratic', ') and', '. I', 'Monkeys .', 'that it', 'his audience', 'since this', \"'s tour\", 'expect from', 'contend that', ', that', 'hand in', 'the extraordinary', 'Monkeys is', 'from Kafka', '( if', 'out of', 'be a', 'Willis and', 'movement of', 'this film', \"Monkey 's\", 'allegedly )', 'Bruce Willis', 'which unsettles.<br', '/>I love', 'huge role', 'end up', 'very scary', 'seen from', 'Does When', 'and handed', 'we all', 'tour de', '/>I would', ', as', '/><br />There', 'and the', 'undergo ,', 'above in', 'true )', 'substance and', 'the bald', 'a fitting', 'it again', 'to have', 'aged .', 'to get', 'Gilliam at', \"'s been\", 'all the', 'And this', 'ended with', 'the characters', 'the first', 'is also', 'an exchange', 'a huge', 'form seem', 'delights in', 'a fact', 'department .', 'of course', 'commercial )', 'his films', 'He seems', 'When He', 'from the', \"'s is\", 'in my', 'Brazil ,', 'enthusiasm ,', 'in this', 'we have', 'air in', '8/10 .', 'general (', ') strangeness', '( and', 'film that', 'that is', 'no denying', 'my movie\".<br', 'in a', 'the clockwork', 'decade since', 'film .', 'strangeness .', 'Brazil is', '( possibly', 'manages to', 'movie\".<br /><br', 'almost invariably', 'surprise no', 'relentless movement', 'entitled \"', 'that his', 'maintain that', 'perspectives and', 'denying that', 'any of', 'weakness .', 'way not', 'at the', ', and', ', particularly', 'through time', 'watched it', \"/>It 's\", \"do n't\", \"n't really\", 'really aged', 'exchange between', 'Twelve Monkeys', 'with a', 'in the', 'ex -', 'very atypical', 'one .', 'a breath', 'I watched', 'It is', 'a story', 'have a', 'to merge', 'I had', 'again ,', 'that ,', 'partly at', 'scary -', 'they undergo', 'story (', \"Gilliam 's\", 'through the', 'fitting tribute', 'played a', ', is', 'was released', 'the above', 'long list', 'Python.<br /><br', 'films (', 'have rightly', '. And', 'Gilliam (', 'rightly come', 'the character', '- Python.<br', 'Element )', 'have so', '. Style', 'the writing.<br', 'and general', 'that The', 'the aircraft', 'suffer from', '- way', 'message saying', 'make .', 'the cast', 'all tend', ') about', ') produced', 'and very', 'Gilliamesque surrealism', '( allegedly', 'aircraft one', 'fact about', 'about an', '. It', 'that these', ': \"', 'extraordinary (', 'Gilliam film', 'many ideas', 'realised that', 'Acts \"', 'is all', 'real ,', 'the Fifth', 'their fate', ', altered', ', in', 'is about', 'also manages', 'also full', 'It ended', 'characters towards', 'the film', '/>There is', 'of fresh', 'surrealism and', '/>Whatever the', 'When I', 'Monkeys will', 'bureaucratic relentless', 'rate (', 'one from', 'I realised', 'but magnificent', 'mirrors ,', '. He', 'do all', 'footing his', 'that Brazil', 'list (', '/><br />It', 'at least', 'to be', 'Style ,', 'it to', 'simple message', 'think about', 'in wrong', 'handed it', 'role in', 'definitely true', 'least ,', 'me look', 'one -', '. This', 'start of', 'ideas ,', 'a first', \"n't have\", 'fresh air', 'plotting department', 'film ,', 'they do', 'film with', '- and', ', of', 'factors played', '. When', 'had changed', 'to expect', 'de force', 'this made', 'altered perspectives', 'a decade', '\" Things', 'time that', 'if not', 'about reflections', 'excellent in', ', but', 'film was', 'who has', 'Pitt.<br /><br', 'and so', 'this journey', 'this movie', 'Butch Bruce', 'wrong -', 'the ex', 'audience .', 'love Gilliam', 'Gilliam films', 'produced a', 'course .', 'performance out', 'from a', 'not usually', 'reflections in', 'two factors', 'is Gilliam', 'still maintain', 'journey ,', 'the truth', 'borrows heavily', 'Monkeys.<br /><br', 'as a', 'heavily from', 'it had', 'the Twelve', 'the start', '( definitely', 'eyes .', 'particularly a', 'fresh eyes', 'breath of', 'He Acts', 'a Gilliam', ', which', 'Gilliam borrows', 'Things Bruce', 'above ,', 'writing.<br /><br', 'so much', 'towards their', 'seems to', 'about the', 'Kafka in', 'A first', 'Willis Does', ', Gilliam', 'creating the']\n['kill a', 'killed off', 'knew I', 'know ?', 'know and', 'know much', 'lady in', 'last minute', 'laughing .', 'learn about']\n"
    }
   ],
   "source": [
    "# The bi-grams are included in the `Dataset`.\n",
    "print(train_data[0].text)\n",
    "# The bi-grams are also included in the `TEXT.vocab`.\n",
    "print(TEXT.vocab.itos[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  452, 11558,    11,  ..., 20269,    56,  1003],\n        [  111,    67,    19,  ...,     3,    15,    48],\n        [   83,     2,  3042,  ...,    16,   199,    13],\n        ...,\n        [    1,     1,     1,  ...,     1,     1,     1],\n        [    1,     1,     1,  ...,     1,     1,     1],\n        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:0')\ntensor([ 457,  864,  503, 1190, 1293,  512,  989,  330,  309,  842, 1298, 1140,\n         192,  291,  588,  256,  572,  568,  379,  327, 1466,  430,  402,  460,\n         494,  515,  347, 1174,  107,  342, 2459,  358,  154,  297,  516,  741,\n         198,  407,  171,  234,  278, 1273,  855,  777,  657,  420,  539,  573,\n         344,  506,  224,  231,  309,  309,  460, 1182,  221,  408,  179,  677,\n         341,  660,  274,  240], device='cuda:0')\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    break\n",
    "text, text_lens = batch.text\n",
    "print(text)\n",
    "print(text_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim, out_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(emb_dim, out_dim)\n",
    "\n",
    "    def forward(self, text, text_lens):\n",
    "        # text: (step, batch)\n",
    "        embedded = self.emb(text)\n",
    "        \n",
    "        # Pooling along steps\n",
    "        # pooled: (batch, emb)\n",
    "        pooled = embedded.sum(dim=0) / text_lens.unsqueeze(1)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2500301"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "IN_DIM = len(TEXT.vocab)\n",
    "EMB_DIM = 100\n",
    "OUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "classifier = Classifier(IN_DIM, EMB_DIM, OUT_DIM, PAD_IDX).to(device)\n",
    "count_parameters(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344],\n        [-0.1077,  0.1105,  0.5981, -0.5436,  0.6740,  0.1066,  0.0389,  0.3548],\n        [-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "# Initialize Embeddings with Pre-Trained Vectors\n",
    "classifier.emb.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "classifier.emb.weight.data[UNK_IDX].zero_()\n",
    "classifier.emb.weight.data[PAD_IDX].zero_()\n",
    "\n",
    "print(classifier.emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters())\n",
    "# Binary cross entropy with logits. \n",
    "# The binary version of cross entropy loss. \n",
    "loss_func = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(classifier, iterator, optimizer, loss_func):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        text, text_lens = batch.text\n",
    "        preds = classifier(text, text_lens).squeeze(-1)\n",
    "        # Calculate loss\n",
    "        loss = loss_func(preds, batch.label)\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss and acc\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def eval_epoch(classifier, iterator, loss_func):\n",
    "    classifier.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            text, text_lens = batch.text\n",
    "            preds = classifier(text, text_lens).squeeze(-1)\n",
    "            # Calculate loss\n",
    "            loss = loss_func(preds, batch.label)\n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Epoch Time: 0m 8s\n\tTrain Loss: 0.670 | Train Acc: 67.70%\n\t Val. Loss: 0.629 |  Val. Acc: 76.65%\nEpoch: 02 | Epoch Time: 0m 7s\n\tTrain Loss: 0.553 | Train Acc: 80.36%\n\t Val. Loss: 0.484 |  Val. Acc: 83.87%\nEpoch: 03 | Epoch Time: 0m 7s\n\tTrain Loss: 0.413 | Train Acc: 86.61%\n\t Val. Loss: 0.386 |  Val. Acc: 86.35%\nEpoch: 04 | Epoch Time: 0m 7s\n\tTrain Loss: 0.327 | Train Acc: 89.23%\n\t Val. Loss: 0.335 |  Val. Acc: 87.35%\nEpoch: 05 | Epoch Time: 0m 7s\n\tTrain Loss: 0.274 | Train Acc: 90.94%\n\t Val. Loss: 0.308 |  Val. Acc: 87.84%\nEpoch: 06 | Epoch Time: 0m 7s\n\tTrain Loss: 0.237 | Train Acc: 92.18%\n\t Val. Loss: 0.289 |  Val. Acc: 88.82%\nEpoch: 07 | Epoch Time: 0m 7s\n\tTrain Loss: 0.208 | Train Acc: 93.24%\n\t Val. Loss: 0.278 |  Val. Acc: 89.01%\nEpoch: 08 | Epoch Time: 0m 7s\n\tTrain Loss: 0.184 | Train Acc: 94.08%\n\t Val. Loss: 0.274 |  Val. Acc: 88.67%\nEpoch: 09 | Epoch Time: 0m 7s\n\tTrain Loss: 0.165 | Train Acc: 94.81%\n\t Val. Loss: 0.266 |  Val. Acc: 89.20%\nEpoch: 10 | Epoch Time: 0m 7s\n\tTrain Loss: 0.147 | Train Acc: 95.57%\n\t Val. Loss: 0.262 |  Val. Acc: 89.12%\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_epoch(classifier, train_iterator, optimizer, loss_func)\n",
    "    valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(classifier.state_dict(), \"models/tut3-model.pt\")\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Val. Loss: 0.262 | Val. Acc: 89.12%\nTest Loss: 0.267 | Test Acc: 89.19%\n"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"models/tut3-model.pt\"))\n",
    "\n",
    "valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "test_loss, test_acc = eval_epoch(classifier, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_senti(classifier, sentence):\n",
    "    classifier.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[tok] for tok in tokenized]\n",
    "    lens = len(indexed)\n",
    "\n",
    "    indexed = torch.tensor(indexed, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    lens = torch.tensor(lens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    pred = torch.sigmoid(classifier(indexed, lens)).round().type(torch.long)\n",
    "    return LABEL.vocab.itos[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'pos'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "predict_senti(classifier, \"This is a good film.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'neg'"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "predict_senti(classifier, \"This film is terrible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Embeddings\n",
    "* The Embeddings of `<unk>` and `<pad>` tokens\n",
    "    * Because the `padding_idx` has been passed to `nn.Embedding`, so the `<pad>` embedding will remain zeros throughout training.  \n",
    "    * While the `<unk>` embedding will be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.0192,  0.0181, -0.0280, -0.0153, -0.0125,  0.0221, -0.0226, -0.0614],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0442, -0.3282,  0.7919, -0.3166,  0.1734, -0.0561, -0.3075,  0.3937],\n        [-0.0075,  0.0096,  0.6748, -0.4430,  0.7822,  0.0017,  0.1420,  0.4392],\n        [-0.3283,  0.1983,  0.4629, -0.6368, -0.3738, -0.0255,  0.1849,  0.1648]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "print(classifier.emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
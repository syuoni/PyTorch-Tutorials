{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Sentiment Analysis\n",
    "This notebook follows this tutorial: https://github.com/bentrevett/pytorch-sentiment-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'film',\n 'is',\n 'the',\n 'best',\n 'is the',\n 'film is',\n 'the best',\n 'This film']"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "def append_bigrams(x):\n",
    "    bigrams = set([\"%s %s\" % (f, s) for f, s in zip(x[:-1], x[1:])])\n",
    "    x.extend(list(bigrams))\n",
    "    return x\n",
    "\n",
    "append_bigrams(\"This film is the best\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "\n",
    "TEXT = Field(tokenize='spacy', preprocessing=append_bigrams, include_lengths=True)\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL, root='data')\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE, \n",
    "                 vectors=\"glove.6B.100d\", vectors_cache=\"vector_cache\", \n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['One', 'does', \"n't\", 'get', 'to', 'enjoy', 'this', 'gem', ',', 'the', '1936', 'Invisible', 'Ray', ',', 'often', '.', 'But', 'no', 'can', 'forget', 'it', '.', 'The', 'story', 'is', 'elegant', '.', 'Karloff', ',', 'austere', 'and', 'embittered', 'in', 'his', 'Carpathian', 'mountain', 'retreat', ',', 'is', 'Janos', 'Rukh', ',', 'genius', 'science', 'who', 'reads', 'ancient', 'beams', 'of', 'light', 'to', 'ascertain', 'events', 'in', 'the', 'great', 'geological', 'past', '\\x85', 'particularly', 'the', 'crash', 'of', 'a', 'potent', 'radioactive', 'meteor', 'in', 'Africa', '.', 'Joining', 'him', 'is', 'the', 'ever', '-', 'elegant', 'Lugosi', '(', 'as', 'a', 'rare', 'hero', ')', ',', 'who', 'studies', '\"', 'astro', '-', 'chemistry', '.', '\"', 'Frances', 'Drake', 'is', 'the', 'lovely', ',', 'underused', 'young', 'wife', ';', 'Frank', 'Lawton', 'the', 'romantic', 'temptation', ';', 'and', 'the', 'divine', 'Violet', 'Kemble', 'Cooper', 'is', 'Mother', 'Rukh', ',', 'in', 'a', 'performance', 'worthy', 'of', 'Maria', 'Ospenskya.<br', '/><br', '/>The', 'story', 'moves', 'swiftly', 'in', 'bold', 'episodes', ',', 'with', 'special', 'effects', 'that', 'are', 'still', 'handsome', '.', 'It', 'also', 'contains', 'some', 'wonderful', 'lines', '.', 'One', 'Rukh', 'restores', 'his', 'mother', \"'s\", 'sight', ',', 'he', 'asks', ',', '\"', 'Mother', ',', 'can', 'you', 'see', ',', 'can', 'you', 'see', '?', '\"', '\"', 'Yes', ',', 'I', 'can', 'see', '\\x85', 'more', 'clearly', 'than', 'ever', '.', 'And', 'what', 'I', 'see', 'frightens', 'me', '.', '\"', 'Even', 'better', 'when', 'mother', 'Rukh', 'says', ',', '\"', 'He', 'broke', 'the', 'first', 'law', 'of', 'science', '.', '\"', 'I', 'am', 'not', 'alone', 'among', 'my', 'acquaintance', 'in', 'having', 'puzzled', 'for', 'many', 'many', 'years', 'exactly', 'what', 'this', 'first', 'law', 'of', 'science', 'is.<br', '/><br', '/>This', 'movie', 'is', 'definitely', 'desert', 'island', 'material', '.', 'Maria Ospenskya.<br', 'his mother', ', is', 'is elegant', 'and embittered', 'mother Rukh', 'enjoy this', 'meteor in', 'Joining him', 'frightens me', 'It also', 'past \\x85', 'see ?', 'performance worthy', 'is Janos', 'to ascertain', 'also contains', 'Janos Rukh', 'me .', 'who studies', 'forget it', 'with special', 'no can', 'that are', 'I see', 'definitely desert', 'Invisible Ray', 'austere and', 'particularly the', '/>This movie', ', often', 'his Carpathian', 'worthy of', 'episodes ,', '( as', 'is.<br /><br', 'studies \"', 'he asks', 'who reads', '. The', 'the crash', 'and the', 'puzzled for', 'But no', '\" I', '. Joining', '- elegant', 'Even better', \"mother 's\", ', \"', 'of light', 'is Mother', 'still handsome', 'you see', 'alone among', 'of a', '; Frank', 'Frances Drake', 'the great', '. It', 'hero )', 'genius science', 'of science', 'often .', 'asks ,', 'movie is', '. Karloff', 'not alone', '. And', 'desert island', 'the lovely', 'can you', 'young wife', 'Lawton the', 'can see', ', can', 'broke the', 'for many', '/>The story', 'wonderful lines', '. One', ', I', 'see frightens', 'elegant Lugosi', 'divine Violet', 'to enjoy', 'Kemble Cooper', 'in a', 'science who', 'am not', 'exactly what', 'mountain retreat', 'the ever', 'Lugosi (', 'story is', \"does n't\", 'One does', ', austere', ', with', ') ,', 'The story', 'can forget', 'ancient beams', 'wife ;', 'moves swiftly', 'swiftly in', 'Rukh restores', ', the', 'among my', 'Karloff ,', 'of Maria', 'it .', 'ascertain events', 'in having', '. But', 'underused young', 'in bold', 'I can', '\" Even', 'law of', 'lines .', 'years exactly', 'Rukh says', 'light to', 'than ever', '\" He', 'having puzzled', 'island material', 'beams of', ', underused', 'One Rukh', '\\x85 particularly', 'a performance', ', genius', 'says ,', 'what this', 'embittered in', 'better when', 'He broke', 'a rare', 'crash of', 'Violet Kemble', 'elegant .', 'see ,', 'many many', 'geological past', 'special effects', 'when mother', 'bold episodes', 'Africa .', 'material .', 'a potent', 'Drake is', \"'s sight\", 'this first', 'Yes ,', 'Cooper is', 'Ray ,', 'astro -', ', who', 'what I', 'see \\x85', '. \"', 'are still', 'sight ,', '; and', '\" Mother', 'is definitely', ', in', \"n't get\", 'in the', 'some wonderful', 'science is.<br', 'clearly than', 'many years', 'the 1936', 'Carpathian mountain', 'him is', 'events in', 'this gem', 'great geological', 'is the', '? \"', 'ever -', '/><br />This', 'restores his', 'ever .', 'chemistry .', 'potent radioactive', 'And what', 'first law', 'Ospenskya.<br /><br', 'Mother ,', '\" \"', 'as a', 'the romantic', ', he', 'my acquaintance', 'temptation ;', 'handsome .', 'reads ancient', 'the divine', 'gem ,', 'lovely ,', 'effects that', '- chemistry', 'rare hero', 'Rukh ,', '\" astro', 'in his', '1936 Invisible', 'retreat ,', 'contains some', 'Mother Rukh', '\\x85 more', '\" Yes', 'science .', '\" Frances', 'get to', 'Frank Lawton', '/><br />The', 'in Africa', 'acquaintance in', 'story moves', 'the first', 'more clearly', 'I am', 'radioactive meteor', 'romantic temptation']\n['old to', 'one must', 'ongoing', 'only complaint', 'or else', 'or interesting', 'or one', 'or worse', 'order .', 'outer']\n"
    }
   ],
   "source": [
    "# The bi-grams are included in the `Dataset`.\n",
    "print(train_data[0].text)\n",
    "# The bi-grams are also included in the `TEXT.vocab`.\n",
    "print(TEXT.vocab.itos[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 4202,   351,  2367,  ...,    11,    11,  3447],\n        [ 2559,     6,  4158,  ...,   815,   229, 21850],\n        [    5,  1966, 23814,  ...,    11,    16,  1379],\n        ...,\n        [    1,     1,     1,  ...,     1,     1,     1],\n        [    1,     1,     1,  ...,     1,     1,     1],\n        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:0')\ntensor([ 235,  498,  957,  709,  281,  326,  294,  404,  616,  475,  656,  598,\n         335,  332,  336,  669, 2071,  961,  312,  525,  537,  273,  242,  517,\n         573,  290,  572,  501,  303,  739,   95,   61,  582,  813,  628,  249,\n         459,  400,  254,  205,  325,  290,  201, 1083,  181,  696,  270,  492,\n         365,  780,  970, 1056,  339,  373,  235,  840,  369,   92,  398,  482,\n         575,  778,  286,  687], device='cuda:0')\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    break\n",
    "text, text_lens = batch.text\n",
    "print(text)\n",
    "print(text_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim, out_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(emb_dim, out_dim)\n",
    "\n",
    "    def forward(self, text, text_lens):\n",
    "        # text: (step, batch)\n",
    "        embedded = self.emb(text)\n",
    "        \n",
    "        # Pooling along steps\n",
    "        # pooled: (batch, emb)\n",
    "        pooled = embedded.sum(dim=0) / text_lens.unsqueeze(1)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2500301"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "IN_DIM = len(TEXT.vocab)\n",
    "EMB_DIM = 100\n",
    "OUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "classifier = Classifier(IN_DIM, EMB_DIM, OUT_DIM, PAD_IDX).to(device)\n",
    "count_parameters(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344],\n        [-0.1077,  0.1105,  0.5981, -0.5436,  0.6740,  0.1066,  0.0389,  0.3548],\n        [-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "# Initialize Embeddings with Pre-Trained Vectors\n",
    "classifier.emb.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "classifier.emb.weight.data[UNK_IDX].zero_()\n",
    "classifier.emb.weight.data[PAD_IDX].zero_()\n",
    "\n",
    "print(classifier.emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters())\n",
    "# Binary cross entropy with logits. \n",
    "# The binary version of cross entropy loss. \n",
    "loss_func = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(classifier, iterator, optimizer, loss_func):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        text, text_lens = batch.text\n",
    "        preds = classifier(text, text_lens).squeeze(-1)\n",
    "        # Calculate loss\n",
    "        loss = loss_func(preds, batch.label)\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss and acc\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def eval_epoch(classifier, iterator, loss_func):\n",
    "    classifier.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            text, text_lens = batch.text\n",
    "            preds = classifier(text, text_lens).squeeze(-1)\n",
    "            # Calculate loss\n",
    "            loss = loss_func(preds, batch.label)\n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Epoch Time: 0m 8s\n\tTrain Loss: 0.672 | Train Acc: 64.65%\n\t Val. Loss: 0.635 |  Val. Acc: 73.87%\nEpoch: 02 | Epoch Time: 0m 11s\n\tTrain Loss: 0.556 | Train Acc: 79.72%\n\t Val. Loss: 0.496 |  Val. Acc: 82.27%\nEpoch: 03 | Epoch Time: 0m 13s\n\tTrain Loss: 0.415 | Train Acc: 85.98%\n\t Val. Loss: 0.398 |  Val. Acc: 85.66%\nEpoch: 04 | Epoch Time: 0m 11s\n\tTrain Loss: 0.327 | Train Acc: 89.04%\n\t Val. Loss: 0.346 |  Val. Acc: 86.87%\nEpoch: 05 | Epoch Time: 0m 11s\n\tTrain Loss: 0.273 | Train Acc: 90.82%\n\t Val. Loss: 0.317 |  Val. Acc: 87.76%\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_epoch(classifier, train_iterator, optimizer, loss_func)\n",
    "    valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(classifier.state_dict(), 'models/tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Val. Loss: 0.317 | Val. Acc: 87.76%\nTest Loss: 0.315 | Test Acc: 87.75%\n"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load('models/tut3-model.pt'))\n",
    "\n",
    "valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "test_loss, test_acc = eval_epoch(classifier, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_senti(classifier, sentence):\n",
    "    classifier.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[tok] for tok in tokenized]\n",
    "    lens = len(indexed)\n",
    "\n",
    "    indexed = torch.tensor(indexed, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    lens = torch.tensor(lens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    pred = torch.sigmoid(classifier(indexed, lens)).round().type(torch.long)\n",
    "    return LABEL.vocab.itos[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'pos'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "predict_senti(classifier, \"This is a good film.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'neg'"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "predict_senti(classifier, \"This film is terrible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Embeddings\n",
    "* The Embeddings of `unk` and `<pad>` tokens\n",
    "    * Because the `padding_idx` has been passed to `nn.Embedding`, so the `<pad>` embedding will remain zeros throughout training.  \n",
    "    * While the `<unk>` embedding will be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.0263,  0.0257, -0.0321, -0.0201, -0.0059,  0.0239,  0.0258,  0.0236],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0313, -0.3120,  0.7726, -0.3239,  0.1757, -0.0388, -0.4670,  0.2690],\n        [-0.0344,  0.0401,  0.6453, -0.4638,  0.7781,  0.0203, -0.0433,  0.2862],\n        [-0.3167,  0.1872,  0.4716, -0.6247, -0.3226,  0.0084,  0.1469,  0.1400]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "print(classifier.emb.weight[:5, :8])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
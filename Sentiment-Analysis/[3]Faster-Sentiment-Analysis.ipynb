{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Sentiment Analysis\n",
    "This notebook follows this tutorial: https://github.com/bentrevett/pytorch-sentiment-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['This',\n 'film',\n 'is',\n 'the',\n 'best',\n 'This film',\n 'is the',\n 'film is',\n 'the best']"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "def append_bigrams(x):\n",
    "    bigrams = set([\"%s %s\" % (f, s) for f, s in zip(x[:-1], x[1:])])\n",
    "    x.extend(list(bigrams))\n",
    "    return x\n",
    "\n",
    "append_bigrams(\"This film is the best\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "\n",
    "TEXT = Field(tokenize='spacy', preprocessing=append_bigrams, include_lengths=True)\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL, root='data')\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE, \n",
    "                 vectors=\"glove.6B.100d\", vectors_cache=\"vector_cache\", \n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Given', 'how', 'corny', 'these', 'movies', 'are', ',', 'you', 'got', 'ta', 'figure', 'that', 'they', 'must', 'have', 'had', 'fun', 'making', 'them', '.', 'The', 'movie', 'focuses', 'on', 'a', 'house', 'that', 'strangely', 'accommodates', 'whomever', 'lives', 'there', '.', 'The', 'inhabitants', 'were', ':', 'author', 'Charles', 'Hillyer', '(', 'Denholm', 'Elliott', '(', 'with', 'hair', '!', ')', ')', ',', 'who', 'gets', 'haunted', 'by', 'one', 'of', 'his', 'own', 'creations', ';', 'Philip', 'Grayson', '(', 'Peter', 'Cushing', ')', ',', 'who', 'gets', 'a', 'little', 'too', 'close', 'to', 'a', 'wax', 'statue', ';', 'John', 'Reid', '(', 'Christopher', 'Lee', ')', ',', 'whose', 'daughter', \"'s\", 'cuteness', 'is', 'apparently', 'a', 'facade', ';', 'and', 'actor', 'Paul', 'Henderson', '(', 'Jon', 'Pertwee', ')', ',', 'on', 'the', 'verge', 'of', 'getting', 'a', 'little', 'too', 'much', 'into', 'character.<br', '/><br', '/>\"The', 'House', 'That', 'Dripped', 'Blood', '\"', 'is', 'actually', 'worth', 'seeing', '(', 'well', 'duh', ';', 'it', 'stars', 'Peter', 'Cushing', 'and', 'Christopher', 'Lee', ')', '.', 'Aside', 'from', 'just', 'being', 'neat', ',', 'there', 'might', 'be', 'some', 'undertones', ':', 'it', 'might', 'be', 'calling', 'into', 'question', 'the', 'issue', 'of', 'real', 'horror', 'vs.', 'assumed', 'horror', '.', 'Like', 'in', '\"', 'The', 'Shining', '\"', ',', 'we', 'might', 'ask', 'whether', 'the', 'house', '/', 'hotel', 'itself', 'holds', 'some', 'memory', 'of', 'past', 'events', '.', 'And', 'if', 'absolutely', 'nothing', 'else', ',', 'Ingrid', 'Pitt', '(', 'as', 'Paul', \"'s\", 'co', '-', 'star', ')', 'is', 'HOT', 'HOT', 'HOT', '!', 'Around', 'the', 'time', 'that', 'this', 'came', 'out', ',', 'she', 'also', 'starred', 'in', '\"', 'Countess', 'Dracula', '\"', 'and', '\"', 'The', 'Vampire', 'Lovers', '\"', '(', 'also', 'with', 'Peter', 'Cushing', ')', '.', 'Maybe', 'she', '-', 'like', 'Barbara', 'Steele', '-', 'will', 'remain', 'known', 'only', 'as', 'a', 'scream', 'queen', ',', 'but', 'mark', 'my', 'words', ':', 'SHE', 'IS', 'A', 'HOT', 'SCREAM', 'QUEEN', '!', 'I', \"'d\", 'like', 'to', 'see', 'Ingrid', 'Pitt', 'and', 'Barbara', 'Steele', 'co', '-', 'star', 'in', 'something.<br', '/><br', '/>I', 'guess', 'that', 'the', 'only', 'weird', 'scene', '(', 'so', 'to', 'speak', ')', 'is', 'where', 'Denholm', 'Elliott', 'is', 'wearing', 'a', 'pink', 'shirt', 'and', 'fluffy', 'jacket', '.', 'You', 'read', 'that', 'right', '.', 'What', 'kind', 'of', 'a', 'name', 'is', '\"', 'Denholm', '\"', 'anyway', '?', 'Oh', 'well', '.', 'A', 'very', 'cool', 'movie', '.', 'a facade', 'getting a', 'ta figure', 'inhabitants were', 'as a', '! I', 'holds some', 'HOT HOT', 'we might', 'see Ingrid', 'where Denholm', 'but mark', 'the time', ': author', 'from just', ', she', 'is wearing', 'time that', 'vs. assumed', 'Lee )', 'neat ,', 'Elliott (', \"I 'd\", 'What kind', 'the house', ', but', 'the verge', ': SHE', 'Given how', 'house that', '( as', '! )', 'Oh well', 'you got', '. A', 'Cushing )', 'gets a', '\" Countess', 'shirt and', 'accommodates whomever', 'Like in', '( Denholm', 'that the', 'well .', '\" is', 'on a', '( so', 'in something.<br', '- will', '. The', 'Christopher Lee', 'verge of', 'Shining \"', '/><br />\"The', 'little too', 'horror .', 'in \"', 'she also', 'that strangely', \"'s cuteness\", 'The inhabitants', 'That Dripped', '. Aside', 'the issue', ', Ingrid', '( Peter', 'SHE IS', 'is where', 'that right', 'mark my', 'with hair', ': it', '; Philip', 'a little', 'horror vs.', 'absolutely nothing', 'much into', 'them .', 'House That', 'a scream', 'of a', 'The Shining', '. What', 'into question', 'A very', 'corny these', 'a house', 'so to', 'it might', 'as Paul', 'remain known', 'Vampire Lovers', 'is HOT', 'stars Peter', ', we', 'The movie', 'of his', 'issue of', 'gets haunted', 'there .', 'that this', '; and', 'cool movie', 'Barbara Steele', 'real horror', 'haunted by', ') ,', 'Aside from', '\" The', 'HOT !', 'Around the', 'Jon Pertwee', '/>\"The House', 'known only', '( Christopher', 'also starred', 'queen ,', 'QUEEN !', 'Dracula \"', 'very cool', 'actor Paul', 'on the', 'movie focuses', 'will remain', 'anyway ?', '\" ,', 'and fluffy', 'like to', 'movies are', 'guess that', 'strangely accommodates', 'of real', 'these movies', 'focuses on', 'nothing else', '/ hotel', 'IS A', 'also with', 'a pink', 'hotel itself', 'John Reid', 'worth seeing', 'to see', 'something.<br /><br', 'close to', 'they must', 'a name', 'this came', 'whomever lives', ') is', 'character.<br /><br', ', whose', 'Cushing and', 'only as', 'jacket .', 'had fun', 'The Vampire', 'facade ;', 'there might', 'got ta', 'one of', \"'s co\", 'and Christopher', 'assumed horror', 'of past', 'figure that', 'be some', 'starred in', 'the only', 'and actor', '; it', 'how corny', 'fun making', 'might be', '! Around', '. Maybe', 'his own', '; John', 'Denholm Elliott', 'out ,', 'well duh', '\" anyway', 'cuteness is', 'memory of', 'is \"', 'kind of', '\" Denholm', 'Dripped Blood', 'events .', 'And if', 'own creations', 'Ingrid Pitt', 'wearing a', 'weird scene', 'have had', 'question the', 'Elliott is', 'if absolutely', 'Blood \"', 'like Barbara', 'whose daughter', 'co -', 'read that', 'Pertwee )', 'Hillyer (', 'too much', 'Lovers \"', 'Denholm \"', 'Reid (', 'of getting', ', there', '. Like', 'who gets', 'itself holds', 'apparently a', 'scream queen', \"Paul 's\", 'Pitt (', \"daughter 's\", 'star in', ') .', 'into character.<br', 'pink shirt', 'Philip Grayson', 'my words', 'Steele -', ', who', 'be calling', 'wax statue', 'Charles Hillyer', '/>I guess', 'undertones :', 'else ,', ', you', 'star )', '? Oh', '\" (', 'seeing (', 'SCREAM QUEEN', 'with Peter', 'HOT SCREAM', 'by one', 'Paul Henderson', 'Grayson (', 'making them', 'only weird', 'movie .', 'right .', 'being neat', 'house /', 'statue ;', 'that they', 'some undertones', 'Henderson (', 'it stars', '( with', 'might ask', ', on', '. And', 'Maybe she', 'she -', 'creations ;', 'speak )', 'lives there', 'Countess Dracula', \"'d like\", 'are ,', 'author Charles', 'actually worth', 'were :', '/><br />I', 'words :', 'whether the', 'You read', 'duh ;', '( Jon', 'is apparently', 'Peter Cushing', '( well', '\" and', 'to a', 'a wax', 'calling into', 'and \"', 'too close', 'to speak', 'is actually', '( also', 'A HOT', 'Pitt and', 'and Barbara', 'Steele co', '- like', 'scene (', 'must have', 'past events', 'fluffy jacket', 'hair !', '. You', ') )', 'just being', 'some memory', 'ask whether', '- star', 'came out', 'name is']\n['of big', 'of interesting', 'of nature', 'of playing', 'of taking', 'of trouble', 'old daughter', 'old enough', 'on Broadway', 'on another']\n"
    }
   ],
   "source": [
    "# The bi-grams are included in the `Dataset`.\n",
    "print(train_data[0].text)\n",
    "# The bi-grams are also included in the `TEXT.vocab`.\n",
    "print(TEXT.vocab.itos[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1422,   11,   68,  ..., 1365,   11,   11],\n        [ 163,   79,   22,  ...,    0, 2279,   79],\n        [   3, 1014,    9,  ..., 1583,  916,   73],\n        ...,\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\ntensor([ 427,  354,  572,  561,  138,  539,  417,  329,  221,  426,  252,  718,\n        1237, 1014,  307,  644,  351,  706, 1642, 1592,  286,  305, 1734,  321,\n          94,  466,  157,  260,   90, 1141,  389, 1230,  297,  362,  326,  101,\n         840,  543,  601, 1093,  403,  314,  263,  811,  292,  555,  524,  292,\n         142,  566,  413,  306,  240,  370,  524,  282,  383,  785,  294,  352,\n         670,  339,  656,  276], device='cuda:0')\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    break\n",
    "text, text_lens = batch.text\n",
    "print(text)\n",
    "print(text_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim, out_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(emb_dim, out_dim)\n",
    "\n",
    "    def forward(self, text, text_lens):\n",
    "        # text: (step, batch)\n",
    "        embedded = self.emb(text)\n",
    "        \n",
    "        # Pooling along steps\n",
    "        # pooled: (batch, emb)\n",
    "        pooled = embedded.sum(dim=0) / text_lens.unsqueeze(1)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2500301"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "IN_DIM = len(TEXT.vocab)\n",
    "EMB_DIM = 100\n",
    "OUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "classifier = Classifier(IN_DIM, EMB_DIM, OUT_DIM, PAD_IDX).to(device)\n",
    "count_parameters(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344],\n        [-0.1077,  0.1105,  0.5981, -0.5436,  0.6740,  0.1066,  0.0389,  0.3548],\n        [-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "# Initialize Embeddings with Pre-Trained Vectors\n",
    "classifier.emb.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "classifier.emb.weight.data[UNK_IDX].zero_()\n",
    "classifier.emb.weight.data[PAD_IDX].zero_()\n",
    "\n",
    "print(classifier.emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters())\n",
    "# Binary cross entropy with logits. \n",
    "# The binary version of cross entropy loss. \n",
    "loss_func = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(classifier, iterator, optimizer, loss_func):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        text, text_lens = batch.text\n",
    "        preds = classifier(text, text_lens).squeeze(-1)\n",
    "        # Calculate loss\n",
    "        loss = loss_func(preds, batch.label)\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss and acc\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def eval_epoch(classifier, iterator, loss_func):\n",
    "    classifier.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            text, text_lens = batch.text\n",
    "            preds = classifier(text, text_lens).squeeze(-1)\n",
    "            # Calculate loss\n",
    "            loss = loss_func(preds, batch.label)\n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (torch.round(torch.sigmoid(preds)) == batch.label).sum().item() / preds.size(0)\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Epoch Time: 0m 7s\n\tTrain Loss: 0.669 | Train Acc: 66.01%\n\t Val. Loss: 0.625 |  Val. Acc: 74.74%\nEpoch: 02 | Epoch Time: 0m 7s\n\tTrain Loss: 0.547 | Train Acc: 80.22%\n\t Val. Loss: 0.482 |  Val. Acc: 82.79%\nEpoch: 03 | Epoch Time: 0m 7s\n\tTrain Loss: 0.410 | Train Acc: 86.42%\n\t Val. Loss: 0.385 |  Val. Acc: 85.84%\nEpoch: 04 | Epoch Time: 0m 7s\n\tTrain Loss: 0.325 | Train Acc: 89.25%\n\t Val. Loss: 0.335 |  Val. Acc: 87.12%\nEpoch: 05 | Epoch Time: 0m 7s\n\tTrain Loss: 0.275 | Train Acc: 90.78%\n\t Val. Loss: 0.305 |  Val. Acc: 88.28%\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_epoch(classifier, train_iterator, optimizer, loss_func)\n",
    "    valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(classifier.state_dict(), 'models/tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Val. Loss: 0.305 | Val. Acc: 88.28%\nTest Loss: 0.314 | Test Acc: 87.72%\n"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load('models/tut3-model.pt'))\n",
    "\n",
    "valid_loss, valid_acc = eval_epoch(classifier, valid_iterator, loss_func)\n",
    "test_loss, test_acc = eval_epoch(classifier, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_senti(classifier, sentence):\n",
    "    classifier.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[tok] for tok in tokenized]\n",
    "    lens = len(indexed)\n",
    "\n",
    "    indexed = torch.tensor(indexed, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    lens = torch.tensor(lens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    pred = torch.sigmoid(classifier(indexed, lens)).round().type(torch.long)\n",
    "    return LABEL.vocab.itos[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'pos'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "predict_senti(classifier, \"This is a good film.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'neg'"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "predict_senti(classifier, \"This film is terrible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Embeddings\n",
    "* The Embeddings of `unk` and `<pad>` tokens\n",
    "    * Because the `padding_idx` has been passed to `nn.Embedding`, so the `<pad>` embedding will remain zeros throughout training.  \n",
    "    * While the `<unk>` embedding will be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0193, -0.0252, -0.0565, -0.0322, -0.0053,  0.0131, -0.0153, -0.0453],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.1143, -0.1685,  0.7600, -0.3274,  0.1637, -0.0413, -0.3092,  0.4015],\n        [-0.1759,  0.1776,  0.6234, -0.4766,  0.7496,  0.0301,  0.1125,  0.4083],\n        [-0.3518,  0.2210,  0.4499, -0.6349, -0.3693,  0.0153,  0.1888,  0.1646]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "print(classifier.emb.weight[:5, :8])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "This notebook implements the model in:  \n",
    "Bahdanau, D., Cho, K., & Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint [arXiv:1409.0473](https://arxiv.org/abs/1409.0473).  \n",
    "\n",
    "This model is based on an *Encoder-Decoder* framework, in which the encoder and the decoder are both RNNs.  \n",
    "This model encodes all the information of the source sequence into a fixed-length context vector $z$, and then utilizes $z$ as an input in *every step* when generating the target sequence, instead of utilizing $z$ only in the beginning of generation. This design aims to relieve the *information compression*.  \n",
    "\n",
    "![Learning to Align and Translate](fig/learning-to-align-and-translate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenize German text. \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenize English text.\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', \n",
    "            lower=True, include_lengths=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', \n",
    "            lower=True, include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=['.de', '.en'], \n",
    "                                                    # fields=[SRC, TRG], \n",
    "                                                    fields=[('src', SRC), ('trg', TRG)], \n",
    "                                                    root='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
    }
   ],
   "source": [
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7855, 5893)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "len(SRC.vocab), len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  2,   2,   2,  ...,   2,   2,   2],\n        [  5,   5,  43,  ...,   5,  18,  18],\n        [ 13,  13, 253,  ...,  13,  30,   0],\n        ...,\n        [  1,   1,   1,  ...,   1,   1,   1],\n        [  1,   1,   1,  ...,   1,   1,   1],\n        [  1,   1,   1,  ...,   1,   1,   1]], device='cuda:0')\ntensor([14, 17, 12, 11, 17, 21, 12, 16, 14, 11, 23, 23,  8, 11,  9, 14, 19, 20,\n        12, 16,  9, 11, 13, 20, 21, 29, 13, 22, 14, 16, 10,  9, 15, 12, 17, 10,\n        14, 22, 17, 20, 23, 23, 12, 17, 15, 19, 17, 15, 16,  7, 14, 15, 16, 12,\n        17, 14, 18, 18, 14, 14, 17, 21, 12, 12,  9, 19, 12, 14, 12, 11, 10, 13,\n        18, 14,  9, 11, 10, 12, 10, 25, 14, 18, 15, 16, 15, 18, 13,  9, 21, 11,\n        20, 12, 13, 14, 14, 17, 10, 13, 18, 30, 14, 12, 13,  9, 10, 15, 13, 10,\n        12, 15, 13, 18, 17, 13, 11, 12, 10, 16, 12, 13, 24, 14, 19, 19, 10, 20,\n        12, 11], device='cuda:0')\ntensor([[   2,    2,    2,  ...,    2,    2,    2],\n        [   4,    4,   48,  ...,    4,   16,   16],\n        [   9,    9,   25,  ...,    9,   30, 1110],\n        ...,\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\ntensor([13, 17, 11, 13, 17, 28, 11, 19, 13, 12, 18, 21,  9, 13, 11, 15, 20, 19,\n        15, 14,  9, 11, 14, 25, 17, 27, 18, 21, 13, 14, 11, 12, 15, 12, 20, 10,\n        16, 22, 18, 19, 23, 24, 12, 18, 14, 22, 19, 13, 18,  6, 13, 19, 16, 13,\n        16, 14, 23, 18, 16, 17, 15, 24, 12, 16,  9, 17, 14, 15, 11, 16, 10, 14,\n        19, 12, 12, 13, 12, 13, 12, 27, 12, 18, 11, 17, 14, 15, 14, 10, 26, 12,\n        20, 14, 13, 14, 12, 18, 12, 15, 22, 29, 16, 12, 16, 11, 14, 17, 12, 12,\n        12, 17, 13, 17, 17, 15, 13, 14, 10, 15, 12, 14, 19, 13, 20, 21, 10, 20,\n        12, 12], device='cuda:0')\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    batch_src, batch_src_lens = batch.src\n",
    "    batch_trg, batch_trg_lens = batch.trg\n",
    "    break\n",
    "print(batch_src)\n",
    "print(batch_src_lens)\n",
    "print(batch_trg)\n",
    "print(batch_trg_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### Encoder\n",
    "* Use GRU instead of LSTM.  \n",
    "* Bidirectional GRU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, num_layers=n_layers, \n",
    "                          bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        # src: (step, batch)\n",
    "        embedded = self.dropout(self.emb(src))\n",
    "\n",
    "        # Pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_lens, enforce_sorted=False)\n",
    "        # hidden: (num_layers*num_directions, batch, hid_dim)\n",
    "        packed_outs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        # outs: (step, batch, hid_dim*num_directions)\n",
    "        outs, _ = nn.utils.rnn.pad_packed_sequence(packed_outs)\n",
    "\n",
    "        # hidden: (num_layers, num_directions=2, batch, hid_dim)\n",
    "        hidden = hidden.view(self.rnn.num_layers, 2, hidden.size(1), hidden.size(2))\n",
    "        # hidden: (num_layers, batch, hid_dim*2)\n",
    "        hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=-1)\n",
    "        # hidden: (num_layers, batch, dec_hid_dim)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([30, 128, 1024])\ntorch.Size([2, 128, 512])\n"
    }
   ],
   "source": [
    "SRC_IN_DIM = len(SRC.vocab)\n",
    "TRG_IN_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "ENC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "DEC_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "encoder = Encoder(SRC_IN_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, \n",
    "                  N_LAYERS, ENC_DROPOUT, ENC_PAD_IDX).to(device)\n",
    "enc_outs, hidden = encoder(batch_src, batch_src_lens)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(enc_outs.size())\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "* Query: the last hidden state of the decoder\n",
    "* Keys: the outputs of the encoder\n",
    "* Values: the outputs of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Linear(enc_hid_dim*2 + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, dec_outs, enc_outs, mask):\n",
    "        \"\"\"\n",
    "        One-step forward. \n",
    "        \"\"\"\n",
    "        # enc_outs: (step, batch, enc_hid_dim*2)\n",
    "        # dec_outs: (step=1, batch, dec_hid_dim)\n",
    "        concated = torch.cat([enc_outs, dec_outs.repeat(enc_outs.size(0), 1, 1)], dim=-1)\n",
    "        projected = torch.tanh(self.proj(concated))\n",
    "        # energy: (step, batch)\n",
    "        energy = self.v(projected).squeeze(-1)\n",
    "        # Ignore the padding positions. \n",
    "        energy.masked_fill_(mask, -np.inf)\n",
    "        return F.softmax(energy, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([30, 128])\n"
    }
   ],
   "source": [
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM).to(device)\n",
    "\n",
    "# The `dec_outs` equals the hidden state at the top layer. \n",
    "mask = (batch_src == encoder.emb.padding_idx)\n",
    "atten = attention(hidden[-1].unsqueeze(0), enc_outs, mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(atten.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000], device='cuda:0', grad_fn=<SumBackward1>)\ntensor([[True, True, True,  ..., True, True, True],\n        [True, True, True,  ..., True, True, True],\n        [True, True, True,  ..., True, True, True],\n        ...,\n        [True, True, True,  ..., True, True, True],\n        [True, True, True,  ..., True, True, True],\n        [True, True, True,  ..., True, True, True]], device='cuda:0')\n"
    }
   ],
   "source": [
    "print(atten.sum(dim=0))\n",
    "# Check the masked positions (i.e., padding positions).\n",
    "print((atten == 0) == mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "* Use GRU instead of LSTM.  \n",
    "* Combine the `embedding` and the `attentioned values` as the input to the RNN.  \n",
    "* Combine the `embedding`, the (last layer) `hidden` and `attentioned values` as the input to the output FC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(in_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim + enc_hid_dim*2, dec_hid_dim, num_layers=n_layers, dropout=dropout)\n",
    "        # The output dimension equals the input dimension for the decoder.\n",
    "        self.fc = nn.Linear(emb_dim + enc_hid_dim*2 + dec_hid_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, hidden, enc_outs, mask):\n",
    "        \"\"\"\n",
    "        One-step forward. \n",
    "        \"\"\"\n",
    "        # trg: (step=1, batch)\n",
    "        # enc_outs: (step, batch, enc_hid_dim*2)\n",
    "        embedded = self.dropout(self.emb(trg))\n",
    "\n",
    "        # atten: (step, batch)\n",
    "        atten = self.attention(hidden[-1].unsqueeze(0), enc_outs, mask)\n",
    "\n",
    "        # (batch, enc_hid_dim*2, step) * (batch, step, 1) -> (batch, enc_hid_dim*2, 1)\n",
    "        attened_values = enc_outs.permute(1, 2, 0).bmm(atten.T.unsqueeze(-1))\n",
    "        # attened_values: (1, batch, enc_hid_dim*2)\n",
    "        attened_values = attened_values.permute(2, 0, 1)\n",
    "\n",
    "        # outs: (step=1, batch, dec_hid_dim)\n",
    "        # hidden: (num_layers*num_directions, batch, dec_hid_dim)\n",
    "        outs, hidden = self.rnn(torch.cat([embedded, attened_values], dim=-1), \n",
    "                                hidden)\n",
    "        # preds: (step=1, batch, out_dim=in_dim)\n",
    "        preds = self.fc(torch.cat([embedded, outs, attened_values], dim=-1))\n",
    "        return preds, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([29, 128])\ntorch.Size([1, 128, 5893])\ntorch.Size([2, 128, 512])\n"
    }
   ],
   "source": [
    "decoder = Decoder(TRG_IN_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, \n",
    "                  N_LAYERS, DEC_DROPOUT, DEC_PAD_IDX).to(device)\n",
    "\n",
    "preds, hidden = decoder(batch_trg[0].unsqueeze(0), hidden, enc_outs, mask)\n",
    "\n",
    "print(batch_trg.size())\n",
    "print(preds.size())\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_lens, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: (step, batch)\n",
    "        # trg: (step, batch)\n",
    "        enc_outs, hidden = self.encoder(src, src_lens)\n",
    "        mask = (src == self.encoder.emb.padding_idx)\n",
    "\n",
    "        preds = []\n",
    "        # The first input to the decoder is the <sos> token. \n",
    "        # trg_t: (step=1, batch)\n",
    "        trg_t = trg[0].unsqueeze(0)\n",
    "        for t in range(1, trg.size(0)):\n",
    "            # preds_t: (step=1, batch, trg_out_dim)\n",
    "            preds_t, hidden = self.decoder(trg_t, hidden, enc_outs, mask)\n",
    "\n",
    "            # top1: (step=1, batch)\n",
    "            top1 = preds_t.argmax(dim=-1)\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                trg_t = trg[t].unsqueeze(0)\n",
    "            else:\n",
    "                trg_t = top1\n",
    "            preds.append(preds_t)\n",
    "        # preds: (step-1, batch, trg_out_dim)\n",
    "        return torch.cat(preds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([29, 128])\ntorch.Size([28, 128, 5893])\n"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "preds = model(batch_src, batch_src_lens, batch_trg)\n",
    "\n",
    "print(batch_trg.size())\n",
    "print(preds.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 20,518,917 trainable parameters\n"
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "N_LAYERS = 1\n",
    "encoder = Encoder(SRC_IN_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, \n",
    "                  N_LAYERS, ENC_DROPOUT, ENC_PAD_IDX)\n",
    "decoder = Decoder(TRG_IN_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, \n",
    "                  N_LAYERS, DEC_DROPOUT, DEC_PAD_IDX)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [-4.5238e-03,  1.4508e-03, -9.9096e-03,  6.5350e-03,  5.8116e-03,\n          1.2935e-02, -7.2838e-03,  7.9034e-05],\n        [-1.5500e-02,  1.6991e-03,  3.9914e-03,  1.8100e-02, -1.2051e-02,\n         -1.4182e-03, -3.5701e-03, -5.3757e-03],\n        [-7.6994e-03, -7.2831e-03,  8.9148e-03,  2.0916e-03, -1.6530e-03,\n          3.9528e-03,  9.3621e-03,  1.3413e-02]], device='cuda:0',\n       grad_fn=<SliceBackward>)\ntensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [-1.5861e-04,  9.4898e-03,  8.5254e-03,  1.7641e-03,  5.5870e-03,\n         -8.4771e-03,  3.2216e-05, -5.3901e-03],\n        [ 1.6275e-02, -4.0321e-03,  3.1122e-03, -7.2026e-04, -1.3443e-02,\n          1.1409e-03, -1.0637e-02,  6.9476e-03],\n        [ 3.3516e-03, -1.1459e-02,  1.2039e-03,  9.0777e-03,  2.1523e-02,\n         -9.6664e-03,  2.7848e-03, -4.8540e-04]], device='cuda:0',\n       grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "# Initialize Embeddings \n",
    "ENC_UNK_IDX = SRC.vocab.stoi[SRC.unk_token]\n",
    "DEC_UNK_IDX = TRG.vocab.stoi[TRG.unk_token]\n",
    "\n",
    "model.encoder.emb.weight.data[ENC_UNK_IDX].zero_()\n",
    "model.encoder.emb.weight.data[ENC_PAD_IDX].zero_()\n",
    "model.decoder.emb.weight.data[DEC_UNK_IDX].zero_()\n",
    "model.decoder.emb.weight.data[DEC_PAD_IDX].zero_()\n",
    "\n",
    "print(model.encoder.emb.weight[:5, :8])\n",
    "print(model.decoder.emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=DEC_PAD_IDX, reduction='mean')\n",
    "optimizer = optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must ensure we turn `teacher forcing` off for evaluation. This will cause the model to only use it's own predictions to make further predictions within a sentence, which mirrors how it would be used in deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, loss_func, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        batch_src, batch_src_lens = batch.src\n",
    "        batch_trg, batch_trg_lens = batch.trg\n",
    "        # preds: (step-1, batch, trg_out_dim)\n",
    "        preds = model(batch_src, batch_src_lens, batch_trg)\n",
    "        \n",
    "        # Calculate loss\n",
    "        preds_flattened = preds.view(-1, preds.size(-1))\n",
    "        batch_trg_flattened = batch_trg[1:].flatten()\n",
    "        loss = loss_func(preds_flattened, batch_trg_flattened)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "def eval_epoch(model, iterator, loss_func):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            batch_src, batch_src_lens = batch.src\n",
    "            batch_trg, batch_trg_lens = batch.trg\n",
    "            # preds: (step-1, batch, trg_out_dim)\n",
    "            preds = model(batch_src, batch_src_lens, batch_trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            # Calculate loss\n",
    "            preds_flattened = preds.view(-1, preds.size(-1))\n",
    "            batch_trg_flattened = batch_trg[1:].flatten()\n",
    "            loss = loss_func(preds_flattened, batch_trg_flattened)\n",
    "            \n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Epoch Time: 0m 35s\n\tTrain Loss: 5.024 | Train PPL: 151.984\n\t Val. Loss: 4.704 |  Val. PPL: 110.428\nEpoch: 02 | Epoch Time: 0m 35s\n\tTrain Loss: 4.068 | Train PPL:  58.458\n\t Val. Loss: 4.041 |  Val. PPL:  56.882\nEpoch: 03 | Epoch Time: 0m 35s\n\tTrain Loss: 3.313 | Train PPL:  27.458\n\t Val. Loss: 3.516 |  Val. PPL:  33.656\nEpoch: 04 | Epoch Time: 0m 35s\n\tTrain Loss: 2.809 | Train PPL:  16.588\n\t Val. Loss: 3.360 |  Val. PPL:  28.801\nEpoch: 05 | Epoch Time: 0m 35s\n\tTrain Loss: 2.474 | Train PPL:  11.873\n\t Val. Loss: 3.175 |  Val. PPL:  23.928\nEpoch: 06 | Epoch Time: 0m 35s\n\tTrain Loss: 2.224 | Train PPL:   9.241\n\t Val. Loss: 3.181 |  Val. PPL:  24.065\nEpoch: 07 | Epoch Time: 0m 34s\n\tTrain Loss: 1.995 | Train PPL:   7.349\n\t Val. Loss: 3.179 |  Val. PPL:  24.019\nEpoch: 08 | Epoch Time: 0m 35s\n\tTrain Loss: 1.820 | Train PPL:   6.172\n\t Val. Loss: 3.232 |  Val. PPL:  25.341\nEpoch: 09 | Epoch Time: 0m 35s\n\tTrain Loss: 1.671 | Train PPL:   5.318\n\t Val. Loss: 3.263 |  Val. PPL:  26.135\nEpoch: 10 | Epoch Time: 0m 35s\n\tTrain Loss: 1.551 | Train PPL:   4.714\n\t Val. Loss: 3.287 |  Val. PPL:  26.770\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, loss_func, CLIP)\n",
    "    valid_loss = eval_epoch(model, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models/tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Val. Loss: 3.175 |  Val. PPL:  23.928\nTest Loss: 3.211 |  Test PPL:  24.795\n"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/tut3-model.pt'))\n",
    "\n",
    "valid_loss = eval_epoch(model, valid_iterator, loss_func)\n",
    "test_loss = eval_epoch(model, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test PPL: {np.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Embeddings\n",
    "* The Embeddings of `unk` and `<pad>` tokens\n",
    "    * Because the `padding_idx` has been passed to `nn.Embedding`, so the `<pad>` embedding will remain zeros throughout training.  \n",
    "    * While the `<unk>` embedding will be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.0418,  0.0088,  0.0405,  0.0268, -0.0360, -0.0228,  0.0061, -0.0192],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0620, -0.0245, -0.0425, -0.0468, -0.0037,  0.0530, -0.0134, -0.0309],\n        [ 0.0422,  0.0133, -0.0600,  0.0280, -0.0171,  0.0272,  0.0024,  0.0028],\n        [ 0.0183, -0.0108, -0.0133,  0.0024,  0.0036,  0.0234,  0.0097,  0.0079]],\n       device='cuda:0', grad_fn=<SliceBackward>)\ntensor([[ 0.0510, -0.0335,  0.0091, -0.0296, -0.0272, -0.0422, -0.0251, -0.0260],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.1081,  0.1522,  0.1345,  0.0872,  0.1275,  0.1146,  0.1422, -0.1024],\n        [ 0.0774,  0.0697,  0.0453, -0.0665, -0.1012,  0.0455, -0.0533, -0.0150],\n        [ 0.0457, -0.0376,  0.0403, -0.0075,  0.0633, -0.0005,  0.0212,  0.0859]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "print(model.encoder.emb.weight[:5, :8])\n",
    "print(model.decoder.emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
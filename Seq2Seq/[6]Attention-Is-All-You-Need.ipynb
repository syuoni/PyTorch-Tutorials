{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Is All You Need\n",
    "\n",
    "This notebook implements the model in:  \n",
    "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in neural information processing systems, 5998-6008. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762).  \n",
    "\n",
    "This model is based on an *Encoder-Decoder* framework, in which the encoder and the decoder are both based on Self-Attention.  \n",
    "\n",
    "![Transformer](fig/transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenize German text. \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenize English text.\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# Set `batch_first=True` in the `Field`.\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', \n",
    "            lower=True, include_lengths=True, batch_first=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', \n",
    "            lower=True, include_lengths=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=['.de', '.en'], \n",
    "                                                    # fields=[SRC, TRG], \n",
    "                                                    fields=[('src', SRC), ('trg', TRG)], \n",
    "                                                    root='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
    }
   ],
   "source": [
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7855, 5893)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "len(SRC.vocab), len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  2,   5,  13,  ...,   1,   1,   1],\n        [  2,   5,  13,  ...,   1,   1,   1],\n        [  2,  43, 253,  ...,   1,   1,   1],\n        ...,\n        [  2,   5,  13,  ...,   1,   1,   1],\n        [  2,  18,  30,  ...,   1,   1,   1],\n        [  2,  18,   0,  ...,   1,   1,   1]])\ntensor([14, 17, 12, 11, 17, 21, 12, 16, 14, 11, 23, 23,  8, 11,  9, 14, 19, 20,\n        12, 16,  9, 11, 13, 20, 21, 29, 13, 22, 14, 16, 10,  9, 15, 12, 17, 10,\n        14, 22, 17, 20, 23, 23, 12, 17, 15, 19, 17, 15, 16,  7, 14, 15, 16, 12,\n        17, 14, 18, 18, 14, 14, 17, 21, 12, 12,  9, 19, 12, 14, 12, 11, 10, 13,\n        18, 14,  9, 11, 10, 12, 10, 25, 14, 18, 15, 16, 15, 18, 13,  9, 21, 11,\n        20, 12, 13, 14, 14, 17, 10, 13, 18, 30, 14, 12, 13,  9, 10, 15, 13, 10,\n        12, 15, 13, 18, 17, 13, 11, 12, 10, 16, 12, 13, 24, 14, 19, 19, 10, 20,\n        12, 11])\ntensor([[   2,    4,    9,  ...,    1,    1,    1],\n        [   2,    4,    9,  ...,    1,    1,    1],\n        [   2,   48,   25,  ...,    1,    1,    1],\n        ...,\n        [   2,    4,    9,  ...,    1,    1,    1],\n        [   2,   16,   30,  ...,    1,    1,    1],\n        [   2,   16, 1110,  ...,    1,    1,    1]])\ntensor([13, 17, 11, 13, 17, 28, 11, 19, 13, 12, 18, 21,  9, 13, 11, 15, 20, 19,\n        15, 14,  9, 11, 14, 25, 17, 27, 18, 21, 13, 14, 11, 12, 15, 12, 20, 10,\n        16, 22, 18, 19, 23, 24, 12, 18, 14, 22, 19, 13, 18,  6, 13, 19, 16, 13,\n        16, 14, 23, 18, 16, 17, 15, 24, 12, 16,  9, 17, 14, 15, 11, 16, 10, 14,\n        19, 12, 12, 13, 12, 13, 12, 27, 12, 18, 11, 17, 14, 15, 14, 10, 26, 12,\n        20, 14, 13, 14, 12, 18, 12, 15, 22, 29, 16, 12, 16, 11, 14, 17, 12, 12,\n        12, 17, 13, 17, 17, 15, 13, 14, 10, 15, 12, 14, 19, 13, 20, 21, 10, 20,\n        12, 12])\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    batch_src, batch_src_lens = batch.src\n",
    "    batch_trg, batch_trg_lens = batch.trg\n",
    "    break\n",
    "print(batch_src)\n",
    "print(batch_src_lens)\n",
    "print(batch_trg)\n",
    "print(batch_trg_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "### Multi-Head Attention\n",
    "\n",
    "![Transformer Attention](fig/transformer-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_m: int, d_k: int, d_v: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_m = d_m\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.WQ = nn.Linear(d_m, d_k*n_heads)\n",
    "        self.WK = nn.Linear(d_m, d_k*n_heads)\n",
    "        self.WV = nn.Linear(d_m, d_v*n_heads)\n",
    "        self.WO = nn.Linear(d_v*n_heads, d_m)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dotproduct(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, \n",
    "                          mask: torch.Tensor):\n",
    "        # Q: (batch, head, trg_step, d_k)\n",
    "        # K: (batch, head, src_step, d_k)\n",
    "        # V: (batch, head, src_step, d_v)\n",
    "        # mask: (batch, trg_step, src_step)\n",
    "\n",
    "        # energy/attens: (batch, head, trg_step, src_step)\n",
    "        energy = Q.matmul(K.permute(0, 1, 3, 2)) / (self.d_k ** 0.5)\n",
    "        energy.masked_fill_(mask.unsqueeze(1), -np.inf)\n",
    "        attens = F.softmax(energy, dim=-1)\n",
    "        # Note: Why applying dropout on attention?\n",
    "        # attened_values: (batch, head, trg_step, d_v)\n",
    "        return self.dropout(attens).matmul(V), attens\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, \n",
    "                mask: torch.Tensor):\n",
    "        # Q/K/V: (batch, src_step/trg_step, d_m)\n",
    "        # mask: (batch, trg_step, src_step)\n",
    "\n",
    "        # Q: (batch, head, trg_step, d_k)\n",
    "        # K: (batch, head, src_step, d_k)\n",
    "        # V: (batch, head, src_step, d_v)\n",
    "        Q = self.WQ(Q).view(Q.size(0), -1, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        K = self.WK(K).view(K.size(0), -1, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        V = self.WV(V).view(V.size(0), -1, self.n_heads, self.d_v).permute(0, 2, 1, 3)\n",
    "        # attened_values: (batch, head, trg_step, d_v)\n",
    "        # attens: (batch, head, trg_step, src_step)\n",
    "        attened_values, attens = self.scaled_dotproduct(Q, K, V, mask)\n",
    "\n",
    "        attened_values = attened_values.permute(0, 2, 1, 3).contiguous()\n",
    "        attened_values = attened_values.view(attened_values.size(0), -1, self.d_v*self.n_heads)\n",
    "        # attened_values: (batch, trg_step, d_m)\n",
    "        return self.WO(attened_values), attens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30])\ntorch.Size([128, 8, 30, 30])\ntorch.Size([128, 30, 256])\n"
    }
   ],
   "source": [
    "SRC_IN_DIM = len(SRC.vocab)\n",
    "TRG_IN_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "ENC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "DEC_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "attention = MultiHeadAttention(HID_DIM, HID_DIM//ENC_HEADS, HID_DIM//ENC_HEADS, \n",
    "                               ENC_HEADS, ENC_DROPOUT)\n",
    "emb = nn.Embedding(SRC_IN_DIM, HID_DIM, padding_idx=ENC_PAD_IDX)\n",
    "\n",
    "# mask: (batch, 1, src_step)\n",
    "mask = (batch_src == emb.padding_idx).unsqueeze(1)\n",
    "Q = emb(batch_src)\n",
    "attened_values, attens = attention(Q, Q, Q, mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(attens.size())\n",
    "print(attened_values.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(3.5763e-07, grad_fn=<MaxBackward1>)\ntensor(True)\n"
    }
   ],
   "source": [
    "print((attens.sum(dim=-1) - 1).abs().max())\n",
    "print(((attens == 0) == mask.unsqueeze(1)).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-Wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosWiseFFN(nn.Module):\n",
    "    def __init__(self, hid_dim: int, ffn_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(hid_dim, ffn_dim)\n",
    "        self.fc2 = nn.Linear(ffn_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30, 256])\ntorch.Size([128, 30, 256])\n"
    }
   ],
   "source": [
    "ffn = PosWiseFFN(HID_DIM, ENC_PF_DIM, ENC_DROPOUT)\n",
    "print(Q.size())\n",
    "print(ffn(Q).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "`Layer Normalization` over a mini-batch: \n",
    "$$\n",
    "y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "$$\n",
    "where $\\gamma$ and $\\beta$ are learnable parameters in shape of `normalized_shape`.  \n",
    "`nn.LayerNorm(normalized_shape)`: The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim: int, n_heads: int, ffn_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(hid_dim, hid_dim//n_heads, hid_dim//n_heads, \n",
    "                                                 n_heads, dropout)\n",
    "        self.self_atten_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ffn = PosWiseFFN(hid_dim, ffn_dim, dropout)\n",
    "        self.ffn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        # x: (batch, src_step, hid_dim)\n",
    "        # mask: (batch, 1, src_step)\n",
    "\n",
    "        # attened: (batch, src_step, hid_dim)\n",
    "        # attens: (batch, head, src_step, src_step)\n",
    "        attened, attens = self.self_attention(x, x, x, mask)\n",
    "        attened_x_comb = self.self_atten_layer_norm(x + self.dropout(attened))\n",
    "\n",
    "        ffned = self.ffn(attened_x_comb)\n",
    "        ffned_attened_x_comb = self.ffn_layer_norm(attened_x_comb + self.dropout(ffned))\n",
    "        return ffned_attened_x_comb, attens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30, 256])\ntorch.Size([128, 8, 30, 30])\n"
    }
   ],
   "source": [
    "encoder_layer = EncoderLayer(HID_DIM, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT)\n",
    "\n",
    "outs, attens = encoder_layer(Q, mask)\n",
    "print(outs.size())\n",
    "print(attens.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "* Use Self-Attention instead of RNN.  \n",
    "* *Token Embeddings* + *Positional Embeddings*.  \n",
    "* Encoder Layers  \n",
    "    * Self-Attention -> LayerNorm\n",
    "    * Position-Wise FFN -> LayerNorm   \n",
    "    * Residual Connection\n",
    "\n",
    "![Transformer Encoder](fig/transformer-encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, hid_dim: int, ffn_dim: int, n_layers: int, \n",
    "                 n_heads: int, dropout: float, pad_idx: int, max_len: int=100):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(in_dim, hid_dim, padding_idx=pad_idx)\n",
    "        self.pos_emb = nn.Embedding(max_len, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, ffn_dim, dropout) \n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = hid_dim ** 0.5\n",
    "\n",
    "    def forward(self, src: torch.Tensor, mask: torch.Tensor):\n",
    "        # src: (batch, src_step)\n",
    "        # mask: (batch, 1, src_step)\n",
    "        pos = torch.arange(src.size(1), device=src.device).repeat(src.size(0), 1)\n",
    "        \n",
    "        # Element-wise addition\n",
    "        # embedded: (batch, src_step, hid_dim)\n",
    "        embedded = self.tok_emb(src)*self.scale + self.pos_emb(pos)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Note: Does here need masking?\n",
    "        # NO. Although the padding positions may be polluted in the self-attention layer, \n",
    "        # The elements in padding positions are impossible to pass to valid (non-padding)\n",
    "        # positions, as long as the self-attention is applied with masking. \n",
    "        x = embedded\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, attens = layer(x, mask)\n",
    "        # x: (batch, src_step, hid_dim)\n",
    "        # attens: (batch, head, src_step, src_step)\n",
    "        return x, attens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30])\ntorch.Size([128, 30, 256])\ntorch.Size([128, 8, 30, 30])\n"
    }
   ],
   "source": [
    "encoder = Encoder(SRC_IN_DIM, HID_DIM, ENC_PF_DIM, ENC_LAYERS, ENC_HEADS,  \n",
    "                  ENC_DROPOUT, ENC_PAD_IDX).to(device)\n",
    "\n",
    "# mask: (batch, 1, src_step)\n",
    "mask = (batch_src == encoder.tok_emb.padding_idx).unsqueeze(1)\n",
    "enc_outs, attens = encoder(batch_src, mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(enc_outs.size())\n",
    "print(attens.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim: int, n_heads: int, ffn_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(hid_dim, hid_dim//n_heads, hid_dim//n_heads, \n",
    "                                                 n_heads, dropout)\n",
    "        self.self_atten_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attention = MultiHeadAttention(hid_dim, hid_dim//n_heads, hid_dim//n_heads, \n",
    "                                                n_heads, dropout)\n",
    "        self.enc_atten_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ffn = PosWiseFFN(hid_dim, ffn_dim, dropout)\n",
    "        self.ffn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y: torch.Tensor, x: torch.Tensor, \n",
    "                trg_mask: torch.Tensor, src_mask: torch.Tensor):\n",
    "        # x/y: (batch, src_step/trg_step, hid_dim)\n",
    "        # trg_mask: (batch, trg_step, trg_step)\n",
    "        # src_mask: (batch, 1, src_step)\n",
    "\n",
    "        # self_attened: (batch, trg_step, hid_dim)\n",
    "        # self_attens: (batch, head, trg_step, trg_step)\n",
    "        self_attened, self_attens = self.self_attention(y, y, y, trg_mask)\n",
    "        sa_y_comb = self.self_atten_layer_norm(y + self.dropout(self_attened))\n",
    "\n",
    "        # enc_attened: (batch, trg_step, hid_dim)\n",
    "        # enc_attens: (batch, head, trg_step, src_step)\n",
    "        enc_attened, enc_attens = self.enc_attention(sa_y_comb, x, x, src_mask)\n",
    "        ea_sa_y_comb = self.enc_atten_layer_norm(sa_y_comb + self.dropout(enc_attened))\n",
    "\n",
    "        ffned = self.ffn(ea_sa_y_comb)\n",
    "        ffned_ea_sa_y_comb = self.ffn_layer_norm(ea_sa_y_comb + self.dropout(ffned))\n",
    "        return ffned_ea_sa_y_comb, self_attens, enc_attens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "* Use Self-Attention instead of RNN.  \n",
    "* *Token Embeddings* + *Positional Embeddings*.  \n",
    "* Decoder Layers  \n",
    "    * Self-Attention -> LayerNorm\n",
    "    * Encoder-Attention -> LayerNorm\n",
    "    * Position-Wise FFN -> LayerNorm   \n",
    "    * Residual Connection\n",
    "\n",
    "![Transformer Decoder](fig/transformer-decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, hid_dim: int, ffn_dim: int, n_layers: int, \n",
    "                 n_heads: int, dropout: float, pad_idx: int, max_len: int=100):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(in_dim, hid_dim, padding_idx=pad_idx)\n",
    "        self.pos_emb = nn.Embedding(max_len, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, n_heads, ffn_dim, dropout) \n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(hid_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = hid_dim ** 0.5\n",
    "\n",
    "    def forward(self, trg: torch.Tensor, enc_outs: torch.Tensor, \n",
    "                trg_mask: torch.Tensor, src_mask: torch.Tensor):\n",
    "        # trg: (batch, trg_step)\n",
    "        # enc_outs: (batch, src_step, hid_dim)\n",
    "        # trg_mask: (batch, trg_step, trg_step)\n",
    "        # src_mask: (batch, 1, src_step)\n",
    "        pos = torch.arange(trg.size(1), device=trg.device).repeat(trg.size(0), 1)\n",
    "        \n",
    "        # Element-wise addition\n",
    "        # embedded: (batch, trg_step, hid_dim)\n",
    "        embedded = self.tok_emb(trg)*self.scale + self.pos_emb(pos)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Note: Does here need masking?\n",
    "        # NO. Although the padding positions may be polluted in the self-attention layer, \n",
    "        # The elements in padding positions are impossible to pass to valid (non-padding)\n",
    "        # positions, as long as the self-attention is applied with masking. \n",
    "        y = embedded\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            y, self_attens, enc_attens = layer(y, enc_outs, trg_mask, src_mask)\n",
    "        # y: (batch, trg_step, hid_dim)\n",
    "        # self_attens: (batch, head, trg_step, trg_step)\n",
    "        # enc_attens: (batch, head, trg_step, src_step)\n",
    "        return self.fc(y), self_attens, enc_attens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mask on Target Sequence for Decoder's Self-Attention\n",
    "The mask on target sequence is \"subsequent\" masking, which forbids the decoder to obtain information from subsequent steps. For example, the target sequence length is 5; the queries are $\\{ q_1, q_2, \\dots, q_5 \\}$, and the keys/values are $\\{ k_1/v_1, k_2/v_2, \\dots, k_5/v_5 \\}$ (They are actually the same vectors). The masking matrix should be:  \n",
    "$$\n",
    "\\begin{matrix}\n",
    "& k_1/v_1 & k_2/v_2 & k_3/v_3 & k_4/v_4 & k_5/v_5\\\\\n",
    "q_1 & False & True  & True  & True  & True\\\\\n",
    "q_2 & False & False & True  & True  & True\\\\\n",
    "q_3 & False & False & False & True  & True\\\\\n",
    "q_4 & False & False & False & False & True\\\\\n",
    "q_5 & False & False & False & False & False\\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "where $True$ denotes `masked` and $False$ denotes `not-masked`.  \n",
    "\n",
    "#### Do we need to consider the masking for padding positions?  \n",
    "NO. Because they will be ignored in the loss function. Or if so, when using the model for inference, we have no idea where the padding positions are, and we cannot create a masking for them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[False,  True,  True,  True,  True],\n        [False, False,  True,  True,  True],\n        [False, False, False,  True,  True],\n        [False, False, False, False,  True],\n        [False, False, False, False, False]])"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Use `torch.triu` to create the masking matrix\n",
    "torch.ones(5, 5, dtype=torch.bool).triu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30])\ntorch.Size([128, 29])\ntorch.Size([128, 29, 5893])\ntorch.Size([128, 8, 29, 29])\ntorch.Size([128, 8, 29, 30])\n"
    }
   ],
   "source": [
    "decoder = Decoder(TRG_IN_DIM, HID_DIM, DEC_PF_DIM, DEC_LAYERS, DEC_HEADS,  \n",
    "                  DEC_DROPOUT, DEC_PAD_IDX).to(device)\n",
    "\n",
    "# src_mask: (batch, 1, src_step)\n",
    "src_mask = (batch_src == encoder.tok_emb.padding_idx).unsqueeze(1)\n",
    "# `trg_mask` is \"subsequent\" masking, which forbids the decoder to obtain information \n",
    "# from subsequent steps. \n",
    "# trg_mask: (batch=1, trg_step, trg_step)\n",
    "trg_mask = torch.ones(batch_trg.size(1), batch_trg.size(1), dtype=torch.bool).triu(1).unsqueeze(0)\n",
    "\n",
    "dec_outs, self_attens, enc_attens = decoder(batch_trg, enc_outs, trg_mask, src_mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(batch_trg.size())\n",
    "print(dec_outs.size())\n",
    "print(self_attens.size())\n",
    "print(enc_attens.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src: (batch, src_step)\n",
    "        # trg: (batch, trg_step-1)\n",
    "        # For the target sequence, the `<eos>` token should be sliced off before passing to the decoder. \n",
    "        # As there are no more tokens to be predicted after `<eos>`. \n",
    "        trg = trg[:, :-1]\n",
    "\n",
    "        # src_mask: (batch, 1, src_step)\n",
    "        # trg_mask: (batch=1, trg_step-1, trg_step-1)\n",
    "        src_mask = (src == self.encoder.tok_emb.padding_idx).unsqueeze(1)\n",
    "        trg_mask = torch.ones(trg.size(1), trg.size(1), dtype=torch.bool, \n",
    "                              device=trg.device).triu(1).unsqueeze(0)\n",
    "\n",
    "        enc_outs, enc_self_attens = self.encoder(src, src_mask)\n",
    "        dec_outs, dec_self_attens, dec_enc_attens = self.decoder(trg, enc_outs, trg_mask, src_mask)\n",
    "\n",
    "        # dec_outs: (batch, trg_step-1, trg_out_dim)\n",
    "        return dec_outs, enc_self_attens, dec_self_attens, dec_enc_attens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30])\ntorch.Size([128, 29])\ntorch.Size([128, 28, 5893])\n"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "preds, *_ = model(batch_src, batch_trg)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(batch_trg.size())\n",
    "print(preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 9,038,853 trainable parameters\n"
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            else:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "encoder = Encoder(SRC_IN_DIM, HID_DIM, ENC_PF_DIM, ENC_LAYERS, ENC_HEADS,  \n",
    "                  ENC_DROPOUT, ENC_PAD_IDX).to(device)\n",
    "decoder = Decoder(TRG_IN_DIM, HID_DIM, DEC_PF_DIM, DEC_LAYERS, DEC_HEADS,  \n",
    "                  DEC_DROPOUT, DEC_PAD_IDX).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0260, -0.0044, -0.0200, -0.0101, -0.0272,  0.0007, -0.0065, -0.0061],\n        [-0.0133, -0.0256, -0.0228,  0.0077, -0.0145,  0.0068, -0.0269,  0.0054],\n        [-0.0099,  0.0212, -0.0124, -0.0217, -0.0002, -0.0022, -0.0258,  0.0262]],\n       grad_fn=<SliceBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0066,  0.0069, -0.0171,  0.0148, -0.0293, -0.0163, -0.0232, -0.0149],\n        [ 0.0005,  0.0302,  0.0241,  0.0073, -0.0169,  0.0214, -0.0047,  0.0152],\n        [ 0.0031, -0.0079,  0.0172,  0.0286, -0.0277, -0.0084,  0.0134, -0.0082]],\n       grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "# Initialize Embeddings \n",
    "ENC_UNK_IDX = SRC.vocab.stoi[SRC.unk_token]\n",
    "DEC_UNK_IDX = TRG.vocab.stoi[TRG.unk_token]\n",
    "\n",
    "model.encoder.tok_emb.weight.data[ENC_UNK_IDX].zero_()\n",
    "model.encoder.tok_emb.weight.data[ENC_PAD_IDX].zero_()\n",
    "model.decoder.tok_emb.weight.data[DEC_UNK_IDX].zero_()\n",
    "model.decoder.tok_emb.weight.data[DEC_PAD_IDX].zero_()\n",
    "\n",
    "print(model.encoder.tok_emb.weight[:5, :8])\n",
    "print(model.decoder.tok_emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=DEC_PAD_IDX, reduction='mean')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, loss_func, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        batch_src, batch_src_lens = batch.src\n",
    "        batch_trg, batch_trg_lens = batch.trg\n",
    "        # preds: (batch, trg_step-1, trg_out_dim)\n",
    "        preds, *_ = model(batch_src, batch_trg)\n",
    "        \n",
    "        # Calculate loss\n",
    "        preds_flattened = preds.view(-1, preds.size(-1))\n",
    "        batch_trg_flattened = batch_trg[:, 1:].flatten()\n",
    "        loss = loss_func(preds_flattened, batch_trg_flattened)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "def eval_epoch(model, iterator, loss_func):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            batch_src, batch_src_lens = batch.src\n",
    "            batch_trg, batch_trg_lens = batch.trg\n",
    "            # preds: (batch, trg_step-1, trg_out_dim)\n",
    "            preds, *_ = model(batch_src, batch_trg)\n",
    "            \n",
    "            # Calculate loss\n",
    "            preds_flattened = preds.view(-1, preds.size(-1))\n",
    "            batch_trg_flattened = batch_trg[:, 1:].flatten()\n",
    "            loss = loss_func(preds_flattened, batch_trg_flattened)\n",
    "            \n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-eff30e62375f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mepoch_secs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-4839230b5166>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, iterator, optimizer, loss_func, clip)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Backward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, loss_func, CLIP)\n",
    "    valid_loss = eval_epoch(model, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models/tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/tut6-model.pt', map_location=device))\n",
    "\n",
    "valid_loss = eval_epoch(model, valid_iterator, loss_func)\n",
    "test_loss = eval_epoch(model, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test PPL: {np.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Embeddings\n",
    "* The Embeddings of `<unk>` and `<pad>` tokens\n",
    "    * Because the `padding_idx` has been passed to `nn.Embedding`, so the `<pad>` embedding will remain zeros throughout training.  \n",
    "    * While the `<unk>` embedding will be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.0046,  0.0022,  0.0002, -0.0012,  0.0007, -0.0045, -0.0034, -0.0048],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0225, -0.0023, -0.0184, -0.0074, -0.0261,  0.0022, -0.0043, -0.0106],\n        [-0.0168, -0.0238, -0.0235,  0.0104, -0.0139,  0.0086, -0.0249,  0.0010],\n        [-0.0130,  0.0233, -0.0115, -0.0185, -0.0011, -0.0008, -0.0238,  0.0257]],\n       grad_fn=<SliceBackward>)\ntensor([[ 0.0030, -0.0018,  0.0016, -0.0015, -0.0037, -0.0025,  0.0024,  0.0003],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0034,  0.0057, -0.0209,  0.0188, -0.0304, -0.0179, -0.0200, -0.0182],\n        [ 0.0005,  0.0302,  0.0241,  0.0073, -0.0169,  0.0214, -0.0047,  0.0152],\n        [ 0.0039, -0.0103,  0.0142,  0.0281, -0.0286, -0.0082,  0.0163, -0.0054]],\n       grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "print(model.encoder.tok_emb.weight[:5, :8])\n",
    "print(model.decoder.tok_emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
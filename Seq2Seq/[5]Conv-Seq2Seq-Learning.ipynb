{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Sequence to Sequence Learning\n",
    "\n",
    "This notebook implements the model in:  \n",
    "Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y. N. 2017. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, 1243-1252. [arXiv:1705.03122](https://arxiv.org/abs/1705.03122).  \n",
    "\n",
    "This model is based on an *Encoder-Decoder* framework, in which the encoder and the decoder are both CNNs.  \n",
    "This model encodes all the information of the source sequence into the encoder's hidden states $\\{h_1, h_2, \\dots, h_{T_x} \\}$, and allows the decoder (via an *attention mechanism*) to look at the entire hidden states in *every step* when generating the target sequence, instead of compressing all the information into a fixed-length context vector. This design aims to further relieve the *information compression*.  \n",
    "\n",
    "![Conv Seq2Seq Learning](fig/conv-seq2seq-learning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenize German text. \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenize English text.\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# Set `batch_first=True` in the `Field`.\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', \n",
    "            lower=True, include_lengths=True, batch_first=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', \n",
    "            lower=True, include_lengths=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=['.de', '.en'], \n",
    "                                                    # fields=[SRC, TRG], \n",
    "                                                    fields=[('src', SRC), ('trg', TRG)], \n",
    "                                                    root='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
    }
   ],
   "source": [
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7855, 5893)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "len(SRC.vocab), len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  2,   5,  13,  ...,   1,   1,   1],\n        [  2,   5,  13,  ...,   1,   1,   1],\n        [  2,  43, 253,  ...,   1,   1,   1],\n        ...,\n        [  2,   5,  13,  ...,   1,   1,   1],\n        [  2,  18,  30,  ...,   1,   1,   1],\n        [  2,  18,   0,  ...,   1,   1,   1]], device='cuda:0')\ntensor([14, 17, 12, 11, 17, 21, 12, 16, 14, 11, 23, 23,  8, 11,  9, 14, 19, 20,\n        12, 16,  9, 11, 13, 20, 21, 29, 13, 22, 14, 16, 10,  9, 15, 12, 17, 10,\n        14, 22, 17, 20, 23, 23, 12, 17, 15, 19, 17, 15, 16,  7, 14, 15, 16, 12,\n        17, 14, 18, 18, 14, 14, 17, 21, 12, 12,  9, 19, 12, 14, 12, 11, 10, 13,\n        18, 14,  9, 11, 10, 12, 10, 25, 14, 18, 15, 16, 15, 18, 13,  9, 21, 11,\n        20, 12, 13, 14, 14, 17, 10, 13, 18, 30, 14, 12, 13,  9, 10, 15, 13, 10,\n        12, 15, 13, 18, 17, 13, 11, 12, 10, 16, 12, 13, 24, 14, 19, 19, 10, 20,\n        12, 11], device='cuda:0')\ntensor([[   2,    4,    9,  ...,    1,    1,    1],\n        [   2,    4,    9,  ...,    1,    1,    1],\n        [   2,   48,   25,  ...,    1,    1,    1],\n        ...,\n        [   2,    4,    9,  ...,    1,    1,    1],\n        [   2,   16,   30,  ...,    1,    1,    1],\n        [   2,   16, 1110,  ...,    1,    1,    1]], device='cuda:0')\ntensor([13, 17, 11, 13, 17, 28, 11, 19, 13, 12, 18, 21,  9, 13, 11, 15, 20, 19,\n        15, 14,  9, 11, 14, 25, 17, 27, 18, 21, 13, 14, 11, 12, 15, 12, 20, 10,\n        16, 22, 18, 19, 23, 24, 12, 18, 14, 22, 19, 13, 18,  6, 13, 19, 16, 13,\n        16, 14, 23, 18, 16, 17, 15, 24, 12, 16,  9, 17, 14, 15, 11, 16, 10, 14,\n        19, 12, 12, 13, 12, 13, 12, 27, 12, 18, 11, 17, 14, 15, 14, 10, 26, 12,\n        20, 14, 13, 14, 12, 18, 12, 15, 22, 29, 16, 12, 16, 11, 14, 17, 12, 12,\n        12, 17, 13, 17, 17, 15, 13, 14, 10, 15, 12, 14, 19, 13, 20, 21, 10, 20,\n        12, 12], device='cuda:0')\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    batch_src, batch_src_lens = batch.src\n",
    "    batch_trg, batch_trg_lens = batch.trg\n",
    "    break\n",
    "print(batch_src)\n",
    "print(batch_src_lens)\n",
    "print(batch_trg)\n",
    "print(batch_trg_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### Encoder\n",
    "* Use CNN instead of RNN.  \n",
    "* Residual connection.  \n",
    "* *Token Embeddings* + *Positional Embeddings*.  \n",
    "* Convolutional Blocks\n",
    "    * Add $(filter\\_size - 1)/2$ padding elements on *each side* of the sequence.   \n",
    "    * Use *GLU* (*Gated Linear Units*) as the activation function.  \n",
    "        * GLU will half the hidden dimension.  \n",
    "    * Residual connection  \n",
    "\n",
    "\n",
    "GLU formula:  \n",
    "$$\n",
    "\\mathrm{GLU}(a, b) = a \\otimes \\mathrm{sigmoid}(b)\n",
    "$$\n",
    "where $a$ and $b$ are inputs in the same shape, and $\\otimes$ is element-wise product.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, pad_idx, max_len=100):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.scale = 0.5**0.5\n",
    "\n",
    "        self.tok_emb = nn.Embedding(in_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.pos_emb = nn.Embedding(max_len, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        # The `kernel_size` must be odd. \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(hid_dim, hid_dim*2, kernel_size=kernel_size, \n",
    "                                              padding=(kernel_size-1)//2) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        # src/mask: (batch, step)\n",
    "        pos = torch.arange(src.size(1), device=src.device).repeat(src.size(0), 1)\n",
    "        \n",
    "        # Element-wise addition\n",
    "        # embedded: (batch, step, emb_dim)\n",
    "        embedded = self.tok_emb(src) + self.pos_emb(pos)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # hidden: (batch, hid_dim=channels, step)\n",
    "        hidden = self.emb2hid(embedded).permute(0, 2, 1)\n",
    "        # Fill the padding positions with zeros, consistent with CNN padding. \n",
    "        # Fill the padding positions every time before passing to a conv-layer. \n",
    "        hidden.masked_fill_(mask.unsqueeze(1), 0)\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # conved: (batch, hid_dim*2, step)\n",
    "            conved = conv(self.dropout(hidden))\n",
    "            # `F.glu`: the `input` is split in half along `dim` to form `a` and `b`. \n",
    "            # conved: (batch, hid_dim, step)\n",
    "            conved = F.glu(conved, dim=1)\n",
    "            # Residual connection\n",
    "            hidden = (hidden + conved) * self.scale\n",
    "            # Fill the padding positions with zeros, consistent with CNN padding. \n",
    "            # Fill the padding positions every time before passing to a conv-layer. \n",
    "            hidden.masked_fill_(mask.unsqueeze(1), 0)\n",
    "\n",
    "        # conv_outs: (batch, step, emb_dim)\n",
    "        conv_outs = self.hid2emb(hidden.permute(0, 2, 1))\n",
    "        conv_outs.masked_fill_(mask.unsqueeze(2), 0)\n",
    "        # Residual connection\n",
    "        # combined: (batch, step, emb_dim)\n",
    "        combined = (embedded + conv_outs) * self.scale\n",
    "        combined.masked_fill_(mask.unsqueeze(2), 0)\n",
    "\n",
    "        return conv_outs, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30])\ntorch.Size([128, 30, 256])\ntorch.Size([128, 30, 256])\n"
    }
   ],
   "source": [
    "SRC_IN_DIM = len(SRC.vocab)\n",
    "TRG_IN_DIM = len(TRG.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_N_LAYERS = 10\n",
    "DEC_N_LAYERS = 10\n",
    "ENC_KERNEL_SIZE = 3 # Must be odd\n",
    "DEC_KERNEL_SIZE = 3 # Can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "ENC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "DEC_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "encoder = Encoder(SRC_IN_DIM, EMB_DIM, HID_DIM, ENC_N_LAYERS, ENC_KERNEL_SIZE, \n",
    "                  ENC_DROPOUT, ENC_PAD_IDX).to(device)\n",
    "\n",
    "mask = (batch_src == encoder.tok_emb.padding_idx)\n",
    "enc_conv_outs, enc_combined = encoder(batch_src, mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(enc_conv_outs.size())\n",
    "print(enc_combined.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(True, device='cuda:0')\ntensor(True, device='cuda:0')\n"
    }
   ],
   "source": [
    "print(((enc_conv_outs == 0) == mask.unsqueeze(2)).all())\n",
    "print(((enc_combined == 0) == mask.unsqueeze(2)).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "* Use CNN instead of RNN.  \n",
    "* Residual connection.  \n",
    "* *Token Embeddings* + *Positional Embeddings*.  \n",
    "* Convolutional Blocks\n",
    "    * Add $filter\\_size - 1$ padding elements at the *beginning* of the sequence.   \n",
    "    * Use *GLU* (*Gated Linear Units*) as the activation function.  \n",
    "        * GLU will half the hidden dimension.  \n",
    "    * Residual connection  \n",
    "* Attention  \n",
    "    * Query: the embedding of current word  \n",
    "    * Keys: the encoded representations - `conv_outs`  \n",
    "        * `conv_outs` is good for getting a larger context over the encoded sequence  \n",
    "    * Values: the encoded representations - `combined`  \n",
    "        * `combined` has more information about the specific token and is thus more useful for prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    A dot-product attention. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, queries, keys, values, mask):\n",
    "        \"\"\"\n",
    "        Multi-step forward. \n",
    "        \"\"\"\n",
    "        # queries: (batch, trg_step, emb_dim)\n",
    "        # keys: (batch, src_step, emb_dim)\n",
    "        # values: (batch, src_step, emb_dim)\n",
    "        # mask: (batch, src_step)\n",
    "\n",
    "        # energy/attens: (batch, trg_step, src_step)\n",
    "        energy = queries.bmm(keys.permute(0, 2, 1))\n",
    "        energy.masked_fill_(mask.unsqueeze(1), -np.inf)\n",
    "\n",
    "        # Attention along the source sequence\n",
    "        attens = F.softmax(energy, dim=-1)\n",
    "\n",
    "        # Apply the attention\n",
    "        # attened_values: (batch, trg_step, emb_dim)\n",
    "        return attens, attens.bmm(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30, 30])\ntorch.Size([128, 30, 256])\n"
    }
   ],
   "source": [
    "attention = Attention()\n",
    "\n",
    "# Use `combined` as an pseudo queries for checking. \n",
    "attens, attened_values = attention(enc_combined, enc_conv_outs, enc_combined, mask)\n",
    "print(attens.size())\n",
    "print(attened_values.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n        ...,\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n       device='cuda:0', grad_fn=<SumBackward1>)\ntensor([[[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]],\n\n        [[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]],\n\n        [[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]],\n\n        ...,\n\n        [[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]],\n\n        [[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]],\n\n        [[True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         ...,\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True],\n         [True, True, True,  ..., True, True, True]]], device='cuda:0')\n"
    }
   ],
   "source": [
    "print(attens.sum(dim=-1))\n",
    "print((attens == 0) == mask.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is it necessary to fill the length padding positions with zeros?  \n",
    "NO. Because the convolutional paddings are all at the sequence begining, so that the conv-layer would be forbidden to look at the token that it tries to predict. Hence, the elements in the length padding positions are also forbidden to pass to the valid (non-padding) positions in the upper layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, pad_idx, max_len=100):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.scale = 0.5**0.5\n",
    "\n",
    "        self.tok_emb = nn.Embedding(in_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.pos_emb = nn.Embedding(max_len, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "\n",
    "        self.attention = Attention()\n",
    "        self.atten_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.atten_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(hid_dim, hid_dim*2, \n",
    "                                              kernel_size=kernel_size) for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(emb_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, trg, enc_conv_outs, enc_combined, mask):\n",
    "        # trg: (batch, trg_step)\n",
    "        pos = torch.arange(trg.size(1), device=trg.device).repeat(trg.size(0), 1)\n",
    "\n",
    "        # Element-wise addition\n",
    "        # embedded: (batch, trg_step, emb_dim)\n",
    "        embedded = self.tok_emb(trg) + self.pos_emb(pos)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # hidden: (batch, hid_dim=channels, trg_step)\n",
    "        hidden = self.emb2hid(embedded).permute(0, 2, 1)\n",
    "        # Note: For the decoder, NO need to fill the padding positions with zeros. \n",
    "        # begin_padding: (batch, hid_dim, kernel_size-1)\n",
    "        begin_padding = torch.zeros(hidden.size(0), hidden.size(1), self.kernel_size-1, device=trg.device)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # Manually pad zeros at the beginning. \n",
    "            padded_hidden = torch.cat([begin_padding, self.dropout(hidden)], dim=-1)\n",
    "            # conved: (batch, hid_dim*2, trg_step)\n",
    "            conved = conv(padded_hidden)\n",
    "            # `F.glu`: the `input` is split in half along `dim` to form `a` and `b`. \n",
    "            # conved: (batch, hid_dim, step)\n",
    "            conved = F.glu(conved, dim=1)\n",
    "\n",
    "            # conved_projected: (batch, trg_step, emb_dim)\n",
    "            conved_projected = self.atten_hid2emb(conved.permute(0, 2, 1))\n",
    "            # attens: (batch, trg_step, src_step)\n",
    "            # attened_values: (batch, trg_step, emb_dim)\n",
    "            attens, attened_values = self.attention((embedded + conved_projected)*self.scale, \n",
    "                                                    enc_conv_outs, enc_combined, mask)\n",
    "\n",
    "            # attened_projected: (batch, hid_dim, trg_step)\n",
    "            attened_projected = self.atten_emb2hid(attened_values).permute(0, 2, 1)\n",
    "            conved_attened = (conved + attened_projected) * self.scale\n",
    "\n",
    "            # Residual connection\n",
    "            # Note: For the decoder, NO need to fill the padding positions with zeros. \n",
    "            hidden = (hidden + conved_attened) * self.scale\n",
    "            \n",
    "        # conv_outs: (batch, trg_step, emb_dim)\n",
    "        conv_outs = self.hid2emb(hidden.permute(0, 2, 1))\n",
    "        # preds: (batch, trg_step, out_dim=in_dim)\n",
    "        preds = self.fc(self.dropout(conv_outs))\n",
    "\n",
    "        # Only attention at the top layer?\n",
    "        return preds, attens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 29])\ntorch.Size([128, 29, 5893])\ntorch.Size([128, 29, 30])\n"
    }
   ],
   "source": [
    "decoder = Decoder(TRG_IN_DIM, EMB_DIM, HID_DIM, DEC_N_LAYERS, DEC_KERNEL_SIZE, \n",
    "                  DEC_DROPOUT, DEC_PAD_IDX).to(device)\n",
    "\n",
    "preds, attens = decoder(batch_trg, enc_conv_outs, enc_combined, mask)\n",
    "\n",
    "print(batch_trg.size())\n",
    "print(preds.size())\n",
    "print(attens.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src/mask: (batch, src_step)\n",
    "        # trg: (batch, trg_step)\n",
    "        mask = (src == self.encoder.tok_emb.padding_idx)\n",
    "        enc_conv_outs, enc_combined = self.encoder(src, mask)\n",
    "        # For the target sequence, the `<eos>` token should be sliced off before passing to the decoder. \n",
    "        # As there are no more tokens to be predicted after `<eos>`. \n",
    "        preds, attens = self.decoder(trg[:, :-1], enc_conv_outs, enc_combined, mask)\n",
    "\n",
    "        # preds: (batch, trg_step-1, trg_out_dim)\n",
    "        # attens: (batch, trg_step-1, src_step)\n",
    "        return preds, attens\n",
    "\n",
    "    def translate(self, src, src_lens, sos: int, trg_max_len: int=50):\n",
    "        pass\n",
    "        # Create an pseudo target sequence. \n",
    "        # pseudo_trg = torch.ones(trg_max_len, src.size(1), \n",
    "        #                         dtype=torch.long, device=src.device) * sos\n",
    "        # preds, attens = self(src, src_lens, pseudo_trg, teacher_forcing_ratio=0)\n",
    "        # return preds.argmax(dim=-1), attens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 30])\ntorch.Size([128, 29])\ntorch.Size([128, 28, 5893])\ntorch.Size([128, 28, 30])\n"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "preds, attens = model(batch_src, batch_trg)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(batch_trg.size())\n",
    "print(preds.size())\n",
    "print(attens.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 37,351,685 trainable parameters\n"
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "encoder = Encoder(SRC_IN_DIM, EMB_DIM, HID_DIM, ENC_N_LAYERS, ENC_KERNEL_SIZE, \n",
    "                  ENC_DROPOUT, ENC_PAD_IDX).to(device)\n",
    "decoder = Decoder(TRG_IN_DIM, EMB_DIM, HID_DIM, DEC_N_LAYERS, DEC_KERNEL_SIZE, \n",
    "                  DEC_DROPOUT, DEC_PAD_IDX).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0036, -0.0057,  0.0141,  0.0074,  0.0130,  0.0027,  0.0067,  0.0105],\n        [-0.0087, -0.0138, -0.0105,  0.0118,  0.0058, -0.0029, -0.0062,  0.0097],\n        [ 0.0180,  0.0115, -0.0045,  0.0070, -0.0107, -0.0151, -0.0111,  0.0012]],\n       device='cuda:0', grad_fn=<SliceBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0144, -0.0001, -0.0140, -0.0133, -0.0042,  0.0002, -0.0093,  0.0181],\n        [ 0.0090,  0.0090,  0.0139,  0.0056, -0.0101,  0.0040,  0.0132,  0.0016],\n        [ 0.0082, -0.0218, -0.0114, -0.0015, -0.0214,  0.0220,  0.0035,  0.0087]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "# Initialize Embeddings \n",
    "ENC_UNK_IDX = SRC.vocab.stoi[SRC.unk_token]\n",
    "DEC_UNK_IDX = TRG.vocab.stoi[TRG.unk_token]\n",
    "\n",
    "model.encoder.tok_emb.weight.data[ENC_UNK_IDX].zero_()\n",
    "model.encoder.tok_emb.weight.data[ENC_PAD_IDX].zero_()\n",
    "model.decoder.tok_emb.weight.data[DEC_UNK_IDX].zero_()\n",
    "model.decoder.tok_emb.weight.data[DEC_PAD_IDX].zero_()\n",
    "\n",
    "print(model.encoder.tok_emb.weight[:5, :8])\n",
    "print(model.decoder.tok_emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=DEC_PAD_IDX, reduction='mean')\n",
    "optimizer = optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, loss_func, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        batch_src, batch_src_lens = batch.src\n",
    "        batch_trg, batch_trg_lens = batch.trg\n",
    "        # preds: (batch, trg_step-1, trg_out_dim)\n",
    "        preds, _ = model(batch_src, batch_trg)\n",
    "        \n",
    "        # Calculate loss\n",
    "        preds_flattened = preds.view(-1, preds.size(-1))\n",
    "        batch_trg_flattened = batch_trg[:, 1:].flatten()\n",
    "        loss = loss_func(preds_flattened, batch_trg_flattened)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "def eval_epoch(model, iterator, loss_func):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            batch_src, batch_src_lens = batch.src\n",
    "            batch_trg, batch_trg_lens = batch.trg\n",
    "            # preds: (batch, trg_step-1, trg_out_dim)\n",
    "            preds, _ = model(batch_src, batch_trg)\n",
    "            \n",
    "            # Calculate loss\n",
    "            preds_flattened = preds.view(-1, preds.size(-1))\n",
    "            batch_trg_flattened = batch_trg[:, 1:].flatten()\n",
    "            loss = loss_func(preds_flattened, batch_trg_flattened)\n",
    "            \n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Epoch Time: 0m 27s\n\tTrain Loss: 32.218 | Train PPL: 98154046763205.812\n\t Val. Loss: 3.929 |  Val. PPL:  50.859\nEpoch: 02 | Epoch Time: 0m 26s\n\tTrain Loss: 3.781 | Train PPL:  43.859\n\t Val. Loss: 3.267 |  Val. PPL:  26.244\nEpoch: 03 | Epoch Time: 0m 26s\n\tTrain Loss: 3.154 | Train PPL:  23.430\n\t Val. Loss: 2.650 |  Val. PPL:  14.153\nEpoch: 04 | Epoch Time: 0m 26s\n\tTrain Loss: 2.630 | Train PPL:  13.873\n\t Val. Loss: 2.268 |  Val. PPL:   9.663\nEpoch: 05 | Epoch Time: 0m 26s\n\tTrain Loss: 2.259 | Train PPL:   9.576\n\t Val. Loss: 2.007 |  Val. PPL:   7.443\nEpoch: 06 | Epoch Time: 0m 26s\n\tTrain Loss: 1.992 | Train PPL:   7.332\n\t Val. Loss: 1.882 |  Val. PPL:   6.565\nEpoch: 07 | Epoch Time: 0m 26s\n\tTrain Loss: 1.807 | Train PPL:   6.091\n\t Val. Loss: 1.799 |  Val. PPL:   6.047\nEpoch: 08 | Epoch Time: 0m 26s\n\tTrain Loss: 1.651 | Train PPL:   5.215\n\t Val. Loss: 1.748 |  Val. PPL:   5.744\nEpoch: 09 | Epoch Time: 0m 26s\n\tTrain Loss: 1.529 | Train PPL:   4.616\n\t Val. Loss: 1.699 |  Val. PPL:   5.466\nEpoch: 10 | Epoch Time: 0m 26s\n\tTrain Loss: 1.431 | Train PPL:   4.184\n\t Val. Loss: 1.674 |  Val. PPL:   5.336\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "CLIP = 0.1\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, loss_func, CLIP)\n",
    "    valid_loss = eval_epoch(model, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models/tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Val. Loss: 1.674 |  Val. PPL:   5.336\nTest Loss: 1.751 |  Test PPL:   5.759\n"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/tut5-model.pt', map_location=device))\n",
    "\n",
    "valid_loss = eval_epoch(model, valid_iterator, loss_func)\n",
    "test_loss = eval_epoch(model, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test PPL: {np.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
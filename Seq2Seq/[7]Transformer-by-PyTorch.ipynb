{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer by PyTorch (Attention Is All You Need)\n",
    "\n",
    "![Transformer](fig/transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mInit signature:\u001b[0m\n\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0md_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mnhead\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mnum_encoder_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mnum_decoder_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mdim_feedforward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mcustom_encoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mcustom_decoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m     \nA transformer model. User is able to modify the attributes as needed. The architecture\nis based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer,\nNiki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\nProcessing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805)\nmodel with corresponding parameters.\n\nArgs:\n    d_model: the number of expected features in the encoder/decoder inputs (default=512).\n    nhead: the number of heads in the multiheadattention models (default=8).\n    num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n    num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n    dim_feedforward: the dimension of the feedforward network model (default=2048).\n    dropout: the dropout value (default=0.1).\n    activation: the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).\n    custom_encoder: custom encoder (default=None).\n    custom_decoder: custom decoder (default=None).\n\nExamples::\n    >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n    >>> src = torch.rand((10, 32, 512))\n    >>> tgt = torch.rand((20, 32, 512))\n    >>> out = transformer_model(src, tgt)\n\nNote: A full example to apply nn.Transformer module for the word language model is available in\nhttps://github.com/pytorch/examples/tree/master/word_language_model\n\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n\u001b[1;31mFile:\u001b[0m           e:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\n\u001b[1;31mType:\u001b[0m           type\n\u001b[1;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "nn.Transformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
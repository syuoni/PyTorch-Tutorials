{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer by PyTorch (Attention Is All You Need)\n",
    "\n",
    "![Transformer](fig/transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 515\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenize German text. \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenize English text.\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# Set `batch_first=False` in the `Field`.\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', \n",
    "            lower=True, include_lengths=True, batch_first=False)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', \n",
    "            lower=True, include_lengths=True, batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=['.de', '.en'], \n",
    "                                                    # fields=[SRC, TRG], \n",
    "                                                    fields=[('src', SRC), ('trg', TRG)], \n",
    "                                                    root=\"../assets/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
    }
   ],
   "source": [
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7854, 5893)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "len(SRC.vocab), len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[  2,   2,   2,  ...,   2,   2,   2],\n        [  5,   5,  43,  ...,   5,  18,  18],\n        [ 13,  13, 253,  ...,  13,  30,   0],\n        ...,\n        [  1,   1,   1,  ...,   1,   1,   1],\n        [  1,   1,   1,  ...,   1,   1,   1],\n        [  1,   1,   1,  ...,   1,   1,   1]], device='cuda:0')\ntensor([14, 17, 12, 11, 17, 21, 12, 16, 14, 11, 23, 23,  8, 11,  9, 14, 19, 20,\n        12, 16,  9, 11, 13, 20, 21, 29, 13, 22, 14, 16, 10,  9, 15, 12, 17, 10,\n        14, 22, 17, 20, 23, 23, 12, 17, 15, 19, 17, 15, 16,  7, 14, 15, 16, 12,\n        17, 14, 18, 18, 14, 14, 17, 21, 12, 12,  9, 19, 12, 14, 12, 11, 10, 13,\n        18, 14,  9, 11, 10, 12, 10, 25, 14, 18, 15, 16, 15, 18, 13,  9, 21, 11,\n        20, 12, 13, 14, 14, 17, 10, 13, 18, 30, 14, 12, 13,  9, 10, 15, 13, 10,\n        12, 15, 13, 18, 17, 13, 11, 12, 10, 16, 12, 13, 24, 14, 19, 19, 10, 20,\n        12, 11], device='cuda:0')\ntensor([[   2,    2,    2,  ...,    2,    2,    2],\n        [   4,    4,   48,  ...,    4,   16,   16],\n        [   9,    9,   25,  ...,    9,   30, 1110],\n        ...,\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\ntensor([13, 17, 11, 13, 17, 28, 11, 19, 13, 12, 18, 21,  9, 13, 11, 15, 20, 19,\n        15, 14,  9, 11, 14, 25, 17, 27, 18, 21, 13, 14, 11, 12, 15, 12, 20, 10,\n        16, 22, 18, 19, 23, 24, 12, 18, 14, 22, 19, 13, 18,  6, 13, 19, 16, 13,\n        16, 14, 23, 18, 16, 17, 15, 24, 12, 16,  9, 17, 14, 15, 11, 16, 10, 14,\n        19, 12, 12, 13, 12, 13, 12, 27, 12, 18, 11, 17, 14, 15, 14, 10, 26, 12,\n        20, 14, 13, 14, 12, 18, 12, 15, 22, 29, 16, 12, 16, 11, 14, 17, 12, 12,\n        12, 17, 13, 17, 17, 15, 13, 14, 10, 15, 12, 14, 19, 13, 20, 21, 10, 20,\n        12, 12], device='cuda:0')\n"
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    batch_src, batch_src_lens = batch.src\n",
    "    batch_trg, batch_trg_lens = batch.trg\n",
    "    break\n",
    "print(batch_src)\n",
    "print(batch_src_lens)\n",
    "print(batch_trg)\n",
    "print(batch_trg_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "### Multi-Head Attention\n",
    "\n",
    "`nn.MultiheadAttention.forward`\n",
    "* `key_padding_mask` is an binary mask - when the value is `True`, the corresponding value on the (`trg_step` * `src_step`) energy matrix will be filled with `-inf` before passing to `softmax`.  \n",
    "* `attn_mask` is an additive mask (i.e. the values will be added to the energy matrix before `softmax`). Hence, the value being `-inf` means \"masked\", and the value being `0` means \"not-masked\". \n",
    "    * This mask aims to prevent attention to certain positions.  \n",
    "    * A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([29, 128])\ntorch.Size([128, 29, 30])\ntorch.Size([29, 128, 256])\n"
    }
   ],
   "source": [
    "SRC_IN_DIM = len(SRC.vocab)\n",
    "TRG_IN_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "ENC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "DEC_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "\n",
    "attention = nn.MultiheadAttention(embed_dim=HID_DIM, num_heads=ENC_HEADS, \n",
    "                                  dropout=ENC_DROPOUT).to(device)\n",
    "src_emb = nn.Embedding(SRC_IN_DIM, HID_DIM, padding_idx=ENC_PAD_IDX).to(device)\n",
    "trg_emb = nn.Embedding(TRG_IN_DIM, HID_DIM, padding_idx=DEC_PAD_IDX).to(device)\n",
    "# The dropout may cause the sum of attention not equaling 1. \n",
    "attention.eval()\n",
    "src_emb.eval()\n",
    "trg_emb.eval()\n",
    "\n",
    "# mask: (batch, src_step)\n",
    "mask = (batch_src == src_emb.padding_idx).T\n",
    "# K: (src_step, batch, hid_dim)\n",
    "K = src_emb(batch_src)\n",
    "# Q: (trg_step, batch, hid_dim)\n",
    "Q = trg_emb(batch_trg)\n",
    "\n",
    "# attened_values: (trg_step, batch, hid_dim)\n",
    "# attens: (batch, trg_step, src_step)\n",
    "# attens is the average attention weights over heads\n",
    "attened_values, attens = attention(Q, K, K, key_padding_mask=mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(batch_trg.size())\n",
    "print(attens.size())\n",
    "print(attened_values.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(1.1921e-07, device='cuda:0', grad_fn=<MaxBackward1>)\ntensor(True, device='cuda:0')\n"
    }
   ],
   "source": [
    "print((attens.sum(dim=-1) - 1).abs().max())\n",
    "print(((attens == 0) == mask.unsqueeze(1)).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([30, 128, 256])\n"
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=HID_DIM, nhead=ENC_HEADS, dim_feedforward=ENC_PF_DIM, dropout=ENC_DROPOUT).to(device)\n",
    "\n",
    "# mask: (batch, src_step)\n",
    "mask = (batch_src == src_emb.padding_idx).T\n",
    "# outs: (src_step, batch, hid_dim)\n",
    "outs = encoder_layer(K, src_key_padding_mask=mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([30, 128, 256])\n"
    }
   ],
   "source": [
    "encoder = nn.TransformerEncoder(encoder_layer, num_layers=ENC_LAYERS).to(device)\n",
    "\n",
    "enc_outs = encoder(K, src_key_padding_mask=mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(enc_outs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0., -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., -inf],\n        [0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Use `torch.triu` to create the masking matrix\n",
    "# This is an additive masking matrix\n",
    "(torch.ones(5, 5) * -np.inf).triu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([29, 128])\ntorch.Size([29, 128, 256])\n"
    }
   ],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=HID_DIM, nhead=DEC_HEADS, dim_feedforward=DEC_PF_DIM, dropout=DEC_DROPOUT).to(device)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=DEC_LAYERS).to(device)\n",
    "\n",
    "# src_mask: (batch, src_step)\n",
    "src_mask = (batch_src == src_emb.padding_idx).T\n",
    "# trg_mask: (trg_step, trg_step)\n",
    "trg_mask = (torch.ones(batch_trg.size(0), batch_trg.size(0), device=device) * -np.inf).triu(1)\n",
    "\n",
    "dec_outs = decoder(Q, enc_outs, tgt_mask=trg_mask, memory_key_padding_mask=src_mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(batch_trg.size())\n",
    "print(dec_outs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([29, 128])\ntorch.Size([29, 128, 256])\n"
    }
   ],
   "source": [
    "transformer = nn.Transformer(d_model=HID_DIM, custom_encoder=encoder, custom_decoder=decoder).to(device)\n",
    "\n",
    "trans_outs = transformer(K, Q, src_key_padding_mask=src_mask, memory_key_padding_mask=src_mask, tgt_mask=trg_mask)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(batch_trg.size())\n",
    "print(trans_outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False False\ntensor(True, device='cuda:0')\n"
    }
   ],
   "source": [
    "transformer.eval()\n",
    "print(encoder.training, decoder.training)\n",
    "\n",
    "enc_outs = encoder(K, src_key_padding_mask=src_mask)\n",
    "dec_outs = decoder(Q, enc_outs, tgt_mask=trg_mask, memory_key_padding_mask=src_mask)\n",
    "\n",
    "trans_outs = transformer(K, Q, src_key_padding_mask=src_mask, memory_key_padding_mask=src_mask, tgt_mask=trg_mask)\n",
    "\n",
    "print((dec_outs == trans_outs).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, transformer: nn.Transformer, src_voc_dim: int, trg_voc_dim: int, \n",
    "                 src_pad_idx: int, trg_pad_idx: int, dropout: float, max_len: int=100):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        hid_dim = transformer.d_model\n",
    "\n",
    "        self.src_tok_emb = nn.Embedding(src_voc_dim, hid_dim, padding_idx=src_pad_idx)\n",
    "        self.src_pos_emb = nn.Embedding(max_len, hid_dim)\n",
    "        self.trg_tok_emb = nn.Embedding(trg_voc_dim, hid_dim, padding_idx=trg_pad_idx)\n",
    "        self.trg_pos_emb = nn.Embedding(max_len, hid_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hid_dim, trg_voc_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = hid_dim ** 0.5\n",
    "\n",
    "    def forward(self, src: torch.Tensor, trg: torch.Tensor):\n",
    "        # src: (src_step, batch)\n",
    "        # trg: (trg_step-1, batch)\n",
    "        # For the target sequence, the `<eos>` token should be sliced off before passing to the decoder. \n",
    "        # As there are no more tokens to be predicted after `<eos>`. \n",
    "        trg = trg[:-1]\n",
    "\n",
    "        # src_mask: (batch, src_step)\n",
    "        src_mask = (src == self.src_tok_emb.padding_idx).T\n",
    "        # trg_mask: (trg_step-1, trg_step-1)\n",
    "        trg_mask = (torch.ones(trg.size(0), trg.size(0), device=trg.device) * -np.inf).triu(1)\n",
    "        \n",
    "        # src_embedded: (src_step, batch, hid_dim)\n",
    "        src_pos = torch.arange(src.size(0), device=src.device).unsqueeze(-1).repeat(1, src.size(1))\n",
    "        src_embedded = self.dropout(self.src_tok_emb(src)*self.scale + self.src_pos_emb(src_pos))\n",
    "        # trg_embedded: (trg_step-1, batch, hid_dim)\n",
    "        trg_pos = torch.arange(trg.size(0), device=trg.device).unsqueeze(-1).repeat(1, trg.size(1))\n",
    "        trg_embedded = self.dropout(self.trg_tok_emb(trg)*self.scale + self.trg_pos_emb(trg_pos))\n",
    "\n",
    "        # trans_outs: (trg_step-1, batch, hid_dim)\n",
    "        trans_outs = transformer(src_embedded, trg_embedded, src_key_padding_mask=src_mask, memory_key_padding_mask=src_mask, tgt_mask=trg_mask)\n",
    "        # preds: (trg_step-1, batch, trg_voc_dim)\n",
    "        return self.fc(trans_outs)\n",
    "\n",
    "    def translate(self, src: torch.Tensor, sos: int, trg_max_len: int=50):\n",
    "        # Ensure `dropout` off, or the result would change randomly. \n",
    "        self.eval()\n",
    "        # src: (src_step, batch)\n",
    "        # src_mask: (batch, src_step)\n",
    "        src_mask = (src == self.src_tok_emb.padding_idx).T\n",
    "\n",
    "        # src_embedded: (src_step, batch, hid_dim)\n",
    "        src_pos = torch.arange(src.size(0), device=src.device).unsqueeze(-1).repeat(1, src.size(1))\n",
    "        src_embedded = self.dropout(self.src_tok_emb(src)*self.scale + self.src_pos_emb(src_pos))\n",
    "\n",
    "        # Create an target sequence. \n",
    "        # trg: (trg_step, batch)\n",
    "        trg = torch.ones(trg_max_len, src.size(1), \n",
    "                         dtype=torch.long, device=src.device) * sos\n",
    "        trg_pos = torch.arange(trg.size(0), device=trg.device).unsqueeze(-1).repeat(1, trg.size(1))\n",
    "\n",
    "        # The inference would be slow, since there is much repeated computation. \n",
    "        for t in range(1, trg.size(0)):\n",
    "            # (2) The input target sequence should be `trg[:(t+1)]`; with `<eos>` token sliced off, \n",
    "            # it becomes `trg[:t]`. Note that the elements in step `t-1` is just predicted in the \n",
    "            # last loop.  \n",
    "            # trg_mask_t: (trg_step-1=t, trg_step-1=t)\n",
    "            trg_mask_t = (torch.ones(t, t, device=trg.device) * -np.inf).triu(1)\n",
    "\n",
    "            # trg_embedded_t: (trg_step-1=t, batch, hid_dim)\n",
    "            trg_embedded_t = self.dropout(self.trg_tok_emb(trg[:t])*self.scale + self.trg_pos_emb(trg_pos[:t]))\n",
    "\n",
    "            # trans_outs_t: (trg_step-1=t, batch, hid_dim)\n",
    "            trans_outs_t = transformer(src_embedded, trg_embedded_t, src_key_padding_mask=src_mask, memory_key_padding_mask=src_mask, tgt_mask=trg_mask_t)\n",
    "\n",
    "            # preds_t: (batch, trg_step-1=t, trg_out_dim)\n",
    "            preds_t = self.fc(trans_outs_t)\n",
    "\n",
    "            # top1: (batch, )\n",
    "            top1 = preds_t[-1].argmax(dim=-1)\n",
    "            trg[t] = top1\n",
    "            \n",
    "        # The decoder's output at the last step is the desired result (over all steps). \n",
    "        # `trg[1:]` equals `preds_t.argmax(dim=-1)` - if the `dropout` off\n",
    "        assert (trg[1:] == preds_t.argmax(dim=-1)).all().item()\n",
    "        return trg[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([30, 128])\ntorch.Size([29, 128])\ntorch.Size([28, 128, 5893])\n"
    }
   ],
   "source": [
    "model = Seq2Seq(transformer, SRC_IN_DIM, TRG_IN_DIM, ENC_PAD_IDX, DEC_PAD_IDX, ENC_DROPOUT).to(device)\n",
    "preds = model(batch_src, batch_trg)\n",
    "\n",
    "print(batch_src.size())\n",
    "print(batch_trg.size())\n",
    "print(preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([49, 128])\n"
    }
   ],
   "source": [
    "DEC_SOS_IDX = TRG.vocab.stoi[TRG.init_token]\n",
    "DEC_EOS_IDX = TRG.vocab.stoi[TRG.eos_token]\n",
    "pred_indexes = model.translate(batch_src, DEC_SOS_IDX)\n",
    "\n",
    "print(pred_indexes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(True, device='cuda:0')"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Check if data are mixed across different samples in a batch.\n",
    "model.eval()\n",
    "preds_012 = model(batch_src[:, 0:3], batch_trg[:, 0:3])\n",
    "preds_123 = model(batch_src[:, 1:4], batch_trg[:, 1:4])\n",
    "(preds_012[:, 1:] == preds_123[:, :2]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 9,039,621 trainable parameters\n"
    }
   ],
   "source": [
    "# This is important...\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name and param.dim() > 1:\n",
    "            nn.init.xavier_uniform_(param.data)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=HID_DIM, nhead=ENC_HEADS, dim_feedforward=ENC_PF_DIM, dropout=ENC_DROPOUT).to(device)\n",
    "# encoder = nn.TransformerEncoder(encoder_layer, num_layers=ENC_LAYERS).to(device)\n",
    "# decoder_layer = nn.TransformerDecoderLayer(d_model=HID_DIM, nhead=DEC_HEADS, dim_feedforward=DEC_PF_DIM, dropout=DEC_DROPOUT).to(device)\n",
    "# decoder = nn.TransformerDecoder(decoder_layer, num_layers=DEC_LAYERS).to(device)\n",
    "# transformer = nn.Transformer(d_model=HID_DIM, custom_encoder=encoder, custom_decoder=decoder).to(device)\n",
    "\n",
    "transformer = nn.Transformer(d_model=HID_DIM, nhead=ENC_HEADS, num_encoder_layers=ENC_LAYERS, num_decoder_layers=DEC_LAYERS, dim_feedforward=ENC_PF_DIM, dropout=ENC_DROPOUT).to(device)\n",
    "model = Seq2Seq(transformer, SRC_IN_DIM, TRG_IN_DIM, ENC_PAD_IDX, DEC_PAD_IDX, ENC_DROPOUT).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0184, -0.0200, -0.0165,  0.0215, -0.0038,  0.0216, -0.0175,  0.0079],\n        [-0.0094,  0.0131, -0.0214, -0.0269,  0.0110, -0.0021, -0.0092,  0.0062],\n        [ 0.0064,  0.0035, -0.0241,  0.0014, -0.0258,  0.0052,  0.0032,  0.0126]],\n       device='cuda:0', grad_fn=<SliceBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0137, -0.0169, -0.0053,  0.0119, -0.0081,  0.0172, -0.0046,  0.0161],\n        [-0.0128,  0.0117, -0.0254,  0.0119, -0.0251,  0.0178, -0.0033,  0.0144],\n        [ 0.0173, -0.0076, -0.0297, -0.0124, -0.0151,  0.0155,  0.0272, -0.0047]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "# Initialize Embeddings \n",
    "ENC_UNK_IDX = SRC.vocab.stoi[SRC.unk_token]\n",
    "DEC_UNK_IDX = TRG.vocab.stoi[TRG.unk_token]\n",
    "\n",
    "model.src_tok_emb.weight.data[ENC_UNK_IDX].zero_()\n",
    "model.src_tok_emb.weight.data[ENC_PAD_IDX].zero_()\n",
    "model.trg_tok_emb.weight.data[DEC_UNK_IDX].zero_()\n",
    "model.trg_tok_emb.weight.data[DEC_PAD_IDX].zero_()\n",
    "\n",
    "print(model.src_tok_emb.weight[:5, :8])\n",
    "print(model.trg_tok_emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=DEC_PAD_IDX, reduction='mean')\n",
    "# The `lr` is important...\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, loss_func, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        # Forward pass\n",
    "        batch_src, batch_src_lens = batch.src\n",
    "        batch_trg, batch_trg_lens = batch.trg\n",
    "        # preds: (batch, trg_step-1, trg_out_dim)\n",
    "        preds = model(batch_src, batch_trg)\n",
    "        \n",
    "        # Calculate loss\n",
    "        preds_flattened = preds.view(-1, preds.size(-1))\n",
    "        batch_trg_flattened = batch_trg[1:].flatten()\n",
    "        loss = loss_func(preds_flattened, batch_trg_flattened)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "def eval_epoch(model, iterator, loss_func):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Forward pass\n",
    "            batch_src, batch_src_lens = batch.src\n",
    "            batch_trg, batch_trg_lens = batch.trg\n",
    "            # preds: (batch, trg_step-1, trg_out_dim)\n",
    "            preds = model(batch_src, batch_trg)\n",
    "            \n",
    "            # Calculate loss\n",
    "            preds_flattened = preds.view(-1, preds.size(-1))\n",
    "            batch_trg_flattened = batch_trg[1:].flatten()\n",
    "            loss = loss_func(preds_flattened, batch_trg_flattened)\n",
    "            \n",
    "            # Accumulate loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Epoch Time: 0m 13s\n\tTrain Loss: 4.023 | Train PPL:  55.855\n\t Val. Loss: 2.689 |  Val. PPL:  14.717\nEpoch: 02 | Epoch Time: 0m 13s\n\tTrain Loss: 2.536 | Train PPL:  12.626\n\t Val. Loss: 2.112 |  Val. PPL:   8.262\nEpoch: 03 | Epoch Time: 0m 13s\n\tTrain Loss: 2.030 | Train PPL:   7.615\n\t Val. Loss: 1.862 |  Val. PPL:   6.439\nEpoch: 04 | Epoch Time: 0m 13s\n\tTrain Loss: 1.717 | Train PPL:   5.570\n\t Val. Loss: 1.707 |  Val. PPL:   5.514\nEpoch: 05 | Epoch Time: 0m 13s\n\tTrain Loss: 1.489 | Train PPL:   4.434\n\t Val. Loss: 1.633 |  Val. PPL:   5.120\nEpoch: 06 | Epoch Time: 0m 13s\n\tTrain Loss: 1.310 | Train PPL:   3.708\n\t Val. Loss: 1.600 |  Val. PPL:   4.955\nEpoch: 07 | Epoch Time: 0m 14s\n\tTrain Loss: 1.166 | Train PPL:   3.209\n\t Val. Loss: 1.597 |  Val. PPL:   4.936\nEpoch: 08 | Epoch Time: 0m 13s\n\tTrain Loss: 1.043 | Train PPL:   2.839\n\t Val. Loss: 1.592 |  Val. PPL:   4.914\nEpoch: 09 | Epoch Time: 0m 14s\n\tTrain Loss: 0.938 | Train PPL:   2.554\n\t Val. Loss: 1.622 |  Val. PPL:   5.065\nEpoch: 10 | Epoch Time: 0m 13s\n\tTrain Loss: 0.846 | Train PPL:   2.331\n\t Val. Loss: 1.656 |  Val. PPL:   5.240\n"
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, loss_func, CLIP)\n",
    "    valid_loss = eval_epoch(model, valid_iterator, loss_func)\n",
    "    epoch_secs = time.time() - t0\n",
    "\n",
    "    epoch_mins, epoch_secs = int(epoch_secs // 60), int(epoch_secs % 60)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"models/tut7-model.pt\")\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Val. Loss: 1.592 |  Val. PPL:   4.914\nTest Loss: 1.646 |  Test PPL:   5.187\n"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/tut7-model.pt\", map_location=device))\n",
    "\n",
    "valid_loss = eval_epoch(model, valid_iterator, loss_func)\n",
    "test_loss = eval_epoch(model, test_iterator, loss_func)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test PPL: {np.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Embeddings\n",
    "* The Embeddings of `<unk>` and `<pad>` tokens\n",
    "    * Because the `padding_idx` has been passed to `nn.Embedding`, so the `<pad>` embedding will remain zeros throughout training.  \n",
    "    * While the `<unk>` embedding will be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.0145, -0.0028, -0.0133, -0.0288, -0.0227,  0.0068,  0.0158, -0.0114],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0326, -0.0211, -0.0186,  0.0078, -0.0133,  0.0187, -0.0087, -0.0062],\n        [ 0.0179,  0.0209, -0.0283, -0.0350,  0.0054,  0.0049, -0.0063, -0.0033],\n        [ 0.0119,  0.0186, -0.0199, -0.0004, -0.0388,  0.0182,  0.0030, -0.0090]],\n       device='cuda:0', grad_fn=<SliceBackward>)\ntensor([[-0.0110,  0.0086,  0.0037, -0.0252, -0.0145, -0.0162,  0.0091, -0.0139],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0040, -0.0098,  0.0058,  0.0036, -0.0020,  0.0027,  0.0047,  0.0114],\n        [-0.0127,  0.0116, -0.0252,  0.0118, -0.0249,  0.0176, -0.0033,  0.0143],\n        [ 0.0193, -0.0043, -0.0161,  0.0036, -0.0015,  0.0110,  0.0005, -0.0074]],\n       device='cuda:0', grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "print(model.src_tok_emb.weight[:5, :8])\n",
    "print(model.trg_tok_emb.weight[:5, :8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_tokens):\n",
    "    \"\"\"\n",
    "    Single sentence translation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src_indexes = [SRC.vocab.stoi[tok] for tok in src_tokens]\n",
    "    src = torch.tensor(src_indexes, dtype=torch.long, device=device).unsqueeze(-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # pred_indexes: (batch, trg_step-1)\n",
    "        pred_indexes = model.translate(src, DEC_SOS_IDX)\n",
    "\n",
    "    trans_tokens = []\n",
    "    for idx in pred_indexes.flatten():\n",
    "        tok = TRG.vocab.itos[idx.item()]\n",
    "        trans_tokens.append(tok)\n",
    "        if tok == TRG.eos_token:\n",
    "            break\n",
    "    return trans_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<sos> zwei junge weiße männer sind im freien in der nähe vieler büsche . <eos>\ntwo young white men are outside near many bushes . <eos>\ntwo young , white males are outside near many bushes .\n"
    }
   ],
   "source": [
    "ex_idx = 0\n",
    "src_tokens = train_data[ex_idx].src\n",
    "trg_tokens = train_data[ex_idx].trg\n",
    "src_tokens = [SRC.init_token] + src_tokens + [SRC.eos_token]\n",
    "trans_tokens = translate(model, src_tokens)\n",
    "\n",
    "print(\" \".join(src_tokens))\n",
    "print(\" \".join(trans_tokens))\n",
    "print(\" \".join(trg_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<sos> mehrere männer mit schutzhelmen bedienen ein antriebsradsystem . <eos>\nseveral men in hard hats are operating a <unk> . <eos>\nseveral men in hard hats are operating a giant pulley system .\n"
    }
   ],
   "source": [
    "ex_idx = 1\n",
    "src_tokens = train_data[ex_idx].src\n",
    "trg_tokens = train_data[ex_idx].trg\n",
    "src_tokens = [SRC.init_token] + src_tokens + [SRC.eos_token]\n",
    "trans_tokens = translate(model, src_tokens)\n",
    "\n",
    "print(\" \".join(src_tokens))\n",
    "print(\" \".join(trans_tokens))\n",
    "print(\" \".join(trg_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<sos> ein kleines mädchen klettert in ein spielhaus aus holz . <eos>\na little girl climbs into a wooden playhouse . <eos>\na little girl climbing into a wooden playhouse .\n"
    }
   ],
   "source": [
    "ex_idx = 2\n",
    "src_tokens = train_data[ex_idx].src\n",
    "trg_tokens = train_data[ex_idx].trg\n",
    "src_tokens = [SRC.init_token] + src_tokens + [SRC.eos_token]\n",
    "trans_tokens = translate(model, src_tokens)\n",
    "\n",
    "print(\" \".join(src_tokens))\n",
    "print(\" \".join(trans_tokens))\n",
    "print(\" \".join(trg_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<sos> ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster . <eos>\na man in a blue shirt stands on a ladder cleaning a window . <eos>\na man in a blue shirt is standing on a ladder cleaning a window .\n"
    }
   ],
   "source": [
    "ex_idx = 3\n",
    "src_tokens = train_data[ex_idx].src\n",
    "trg_tokens = train_data[ex_idx].trg\n",
    "src_tokens = [SRC.init_token] + src_tokens + [SRC.eos_token]\n",
    "trans_tokens = translate(model, src_tokens)\n",
    "\n",
    "print(\" \".join(src_tokens))\n",
    "print(\" \".join(trans_tokens))\n",
    "print(\" \".join(trg_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "*BLEU* (Bilingual Evaluation Understudy) is a metric measuring the quality of translation.  \n",
    "BLEU looks at the overlap in the predicted and actual target sequences in terms of their *n-grams*.  \n",
    "BLEU gives a number between 0 and 1 for each sequence, and a higher BLEU suggests better translation quality.  \n",
    "BLEU being 1 means a perfect overlap, i.e., a perfect translation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calc_bleu(data):\n",
    "    trg_data = []\n",
    "    trans_data = []\n",
    "    for ex in data:\n",
    "        src_tokens = ex.src\n",
    "        trg_tokens = ex.trg\n",
    "        src_tokens = [SRC.init_token] + src_tokens + [SRC.eos_token]\n",
    "        trans_tokens = translate(model, src_tokens)\n",
    "\n",
    "        # The groudtruth may contain multiple actual sentences (right translations). \n",
    "        trg_data.append([trg_tokens])\n",
    "        trans_data.append(trans_tokens[:-1])  # Cut off the <eos> token\n",
    "\n",
    "    return bleu_score(trans_data, trg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BLEU score is: 36.00\n"
    }
   ],
   "source": [
    "bleu = calc_bleu(test_data)\n",
    "\n",
    "print(f'BLEU score is: {bleu*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}